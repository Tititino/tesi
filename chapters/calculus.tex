\chapter{The focused calculus}\label{chapter:calculus}
% Before describing the calculus we must give some preliminary definitions

% Explaination on constraints why we use them ecc ecc ecc
In this chapter we will define a focused one sided constraint calculus for full linear logic.
This calculus is a hybrid between the one defined in \cite{HarlandPym} and the triadic calculus of \cite{Focusing}.
Before all, we give the definition of a linear logic formula.
\begin{define}[Linear logic formula]
	\label{def:ll formula}
	A propositional linear logic formula is defined as follows:
	\begin{center}
		\begin{tabular}{l}
			$\left. 
				\begin{array}{wc{.5cm}wc{.2cm}wc{1.5cm}wc{.2cm}wc{.5cm}}
					\phi & ::=  & \phi \llten \phi & \mid & 1 \\
					     & \mid & \phi \llpar \phi & \mid & \bot \\
					     & \mid & \phi \lolli \phi &      &
				\end{array} \; \right \} \text{(Multiplicatives and their identities)} $ \\
			$\left.
				\begin{array}{wc{.5cm}wc{.2cm}wc{1.5cm}wc{.2cm}wc{.5cm}}
					& \mid & \phi \llplus \phi & \mid & 0 \\
					& \mid & \phi \llwith \phi & \mid & \bot 
				\end{array} \; \right \} \text{(Additives and their identities)} $ \\
			$ \left.
				\begin{array}{wc{.5cm}wc{.2cm}wc{1.5cm}wc{.2cm}wc{.5cm}}
				     	& \mid & \llbang{\phi}    & \mid & \llwn{\phi} 
			     	\end{array} \right. \text{(Exponentials)} $ \\
			$ \left.
				\begin{array}{wc{.5cm}wc{.2cm}wc{1.5cm}wc{.2cm}wc{.5cm}}
					& \mid & \llnot{\phi} & & 
			     	\end{array} \right. \text{(Negation)} $ \\
			$ \left.
				\begin{array}{wc{.5cm}wc{.2cm}wc{1.5cm}wc{.2cm}wc{.5cm}}
					& \mid & \alpha & & 
			     	\end{array} \right. \text{(Atom)} $ \\
		\end{tabular}
	\end{center}
	We will use $\phi$ to denote formulae, $\alpha$ to denote atoms, and $\Delta$ or $\Gamma$ to denote multisets of formulae.
\end{define}

\section{Normalization}\label{sec:normalization}
Since in linear logic negation is symmetric and an involution, it is usual to work only with formulae in negated normal form.
\begin{define}[Negated Normal Form -- NNF]
	\label{def:nnf}
	A formula is in NNF if all its linear implications ($\displaylolli$) are expanded to pars ($\displaypar$) using the tautology
	$$ a \lolli b \Leftrightarrow \llnot{a} \llpar b $$
	and negation is pushed down to atoms.
	Alternatively a formula in NNF is defined as follows:
	\begin{center}
		\begin{tabular}{ccccc}
			$\phi$ & $ ::=  $  & $ \phi \llten \phi  $ & $ \mid $ & $ 1    $ \\
			       & $ \mid $  & $ \phi \llpar \phi  $ & $ \mid $ & $ \bot $ \\
			       & $ \mid $  & $ \phi \llplus \phi $ & $ \mid $ & $ 0    $ \\
			       & $ \mid $  & $ \phi \llwith \phi $ & $ \mid $ & $ \bot $ \\
			       & $ \mid $  & $ \llbang{\phi}     $ & $ \mid $ & $ \llwn{\phi} $ \\
			       & $ \mid $  & $ \llnot{\alpha}    $ & $ \mid $ & $ \alpha $
		\end{tabular}
	\end{center}
	Accordingly, a sequent is in NNF iff all of its formulae are in NNF.
\end{define}
A generic formula is normalized by applying recursively the DeMorgan rules for linear logic until NNF is reached.
Normalization of judgments instead takes a two-sided judgment of the form
$$ \Delta \vdash \Gamma $$
and transforms it into a one-sided judgment
$$ \vdash \Delta' $$
where the right side is composed of the normalization of $\Gamma$ and $\llnot{\Delta}$.

Normalization has some implementation-wise advantages, but for now it is only important because it shrinks the size of the complete calculus by roughly a half since only the right rules remain, thus making the calculus easier to represent (see. Figure \ref{fig:hp calculus} and \ref{fig:calculus}) and making the proof of Theorem \ref{thm:soundness} significantly shorter.

\section{Focusing}
Focusing is a technique described by Andreoli in his seminal paper \cite{Focusing}.
In it he recognizes two alternating phases in a proof: a deterministic phase -- called asynchronous phase; and a non-deterministic phase -- called synchronous phase or focused phase.
In the asynchronous phase the applicable rules are all invertible, making their order of application irrelevant.
For this reason these rules are called asynchronous rules, whereas the non-invertible ones are called synchronous.
In this system to each formula is assigned a positive or negative polarity based on its top-level connective:
\begin{itemize}
	\item connectives with a synchronous right rule are defined to have a positive polarity, these are:
		$$ \displayten, \displayplus, \displaybang, \displayone$$
	\item whereas connectives with an asynchronous right rule have a negative polarity, these are:
		$$ \displaypar, \displaywith, \displaywn, \displaytop, \displaybot$$
\end{itemize}
For atoms polarities may be assigned with some arbitrarily complex mechanisms.
We will follow \cite{LiangMiller} and simply assign atoms with a negative polarity and negated atoms with a positive one.
Since we work in one sided linear logic and connectives have only right rules, positive connectives may also be called synchronous, and negative connectives asynchronous.
\begin{define}\label{def:focusing predicates}
	Based on the definitions above we get the following predicates:
	\begin{itemize}
		\item ``$\phi \; \mathrm{atom}$'' is true whenever $\phi$ is an atom;
		\item ``$\phi \; \mathrm{asy}$'' is true whenever $\phi$ has as an asynchronous top-level connective;
		\item ``$\phi \; \mathrm{negative}$'' is true whenever $\phi$ is either an atom or an asynchronous connective, so
			$$ \phi \; \mathrm{negative} = \phi \; \mathrm{atom} \vee \phi \; \mathrm{asy} $$
	\end{itemize}
\end{define}

\begin{figure}[h!]
	\centering
	\input{figures/triadic-calculus}
	\caption{J.-M. Andreoli's triadic calculus.\label{fig:triadic}}
\end{figure}
Figure \ref{fig:triadic} shows the focused calculus described by Anderoli in \cite{Focusing}.
This calculus is also called triadic calculus, since its judgments have three members: a set of unrestricted formulae; a multiset of linear formulae put to the side; and either a single formula or another multiset of linear formulae depending on the current phase.
An arrow pointing up symbolizes the asynchronous phase, and an arrow pointing down the focusing phase.
Phase switching happens using the decide rules (\derRule[A]{\displaydecide[1]}, \derRule[A]{\displaydecide[2]}), which non-deterministically choose a formula to focus on.

\section{Constraints}
During proof search constraints are generated as the formulae get broken up.
These are created in a particular way, as to guarantee that if an assignment exists that satisfies them, then the formulae have been used linearly.
\begin{define}[Variables, expressions]
	\label{def:bool expr}
	A boolean variable is simply a symbol to which one can associate a value of true or false.
	A boolean expression, in our case, is just a conjunction of possibly negated boolean variables as follows
	$$ e ::= x \wedge e \mid \overline{x} \wedge e \mid x $$
	We will call $e$ such a conjunction and $x$ the single boolean variables.
	Given a boolean expression $e$ we write
	$$ \vars{e} = \{ x_i \mid x_i \in e \} $$
	as the set of the variables appearing in the expression.
\end{define}

\begin{define}[Annotated formula]
	\label{def:annotated}
	Given a formula $\phi$ defined as in Definition \ref{def:nnf} and a boolean expression $e$ defined as in Definition \ref{def:bool expr}, an \textit{annotated formula} is simply a pair
	$$ \af{\phi}{e} $$
	that associates the formula to the expression.
	We denote 
	\begin{itemize}
		\item the operation of extracting the boolean expression associated to a given formula as
			$$ \expr{\af{\phi}{e}} = e $$
			and then extend this notation to sequents such that $ \expr{\Delta} $ is the set of all boolean expressions of $\Delta$
			$$ \expr{\Delta} = \{ \expr{\phi} \mid \phi \in \Delta \} $$
		\item the operation of extracting the set of variables appearing in the expression $e$ as
			$$\vars{\af{\phi}{e}} = \vars{e} $$
			and then extend this notation to sequents such that $\vars{\Delta}$ is the set of all the variables appearing in the boolean expressions of the annotated formulae of $\Delta$
			$$ \vars{\Delta} = \{ \vars{\expr{\phi}} \mid \phi \in \Delta \} $$
	\end{itemize}
\end{define}
It is important to note that only the topmost connective gets annotated, and not the sub-formulae.

The purpose of putting formulae and expressions together in the annotated formula is twofold:
\begin{itemize}
	\item the actions taken on the formula determine the constraints that will be generated, and these refer to the expressions associated to said formula;
	\item after the constraints are solved we can query the assignment of the variables and find out if the associated formula is used or not in a certain branch of a proof (see Definition \ref{def:evaluation}).
\end{itemize}
The above constraints may be only of two kinds: ``$\avail{e}$'' and ``$\used{e}$''.
\begin{define}[Constraints]
	\label{def:constraints}
	Given an annotated formula $\af{\phi}{e}$ as in Definition \ref{def:annotated}, a constraint $\lambda$ is either
	\begin{itemize}
		\item ``$\used{e}$'', which states that the formula $\phi$ gets consumed in this branch of the proof;
		\item ``$\avail{e}$'', which states that the formula $\phi$ does not get consumed in this branch of the proof, and thus is available to be used in another branch.
	\end{itemize}
	We then extend these constraints to sequents, such that
	\begin{align*}
		\used{\Delta} &= \{ \used{e} \mid e \in \mathrm{exp}(\Delta) \} \\
		\avail{\Delta} &= \{ \avail{e} \mid e \in \mathrm{exp}(\Delta) \}
	\end{align*}
	We denote
	\begin{itemize}
		\item $\mathrm{exp}(\lambda) = e$ where $\lambda$ is either ``$\used{e}$'' or ``$\avail{e}$'';
		\item $\mathrm{vars}(\lambda) = \mathrm{vars}(\mathrm{exp}(\lambda))$, and extend this notation to a set of constraints $\Lambda$
			$$ \mathrm{vars}(\Lambda) = \bigcup_{\lambda \in \Lambda} \mathrm{vars}(\lambda) $$
	\end{itemize}
\end{define}
\begin{define}[Assignment]
	\label{def:assignment}
	An assignment is a function
	$$ \mathV : \{ \dots, x_i, x_j, \dots \} \rightarrow \{ \top, \bot \} $$
	that associates the members of a set of variables to either true or false.
	Given a set of variables $X$, we say that the assignment $\mathV$ \textit{covers} $X$ if no variable in $X$ is left undefined under $\mathV$, or
	$$ X \subseteq \Dom{\mathV} $$
\end{define}
\begin{define}[Evaluation]
	\label{def:evaluation}
	Given a boolean expression $e$ and an assignment $\mathV$, such that $\mathV$ covers $\vars{e}$, we write
		$$ e[\mathV] = e[\dots, x_i \subst \mathV(x_i), x_j \subst \mathV(x_j), \dots] $$
	as the value of the expression $e$ substituting its variables using assignment $\mathV$.
	We extend this notation, such that
	\begin{itemize}
		\item given a constraint $\lambda$ and an assignment $\mathV$ that covers $\vars{\lambda}$
			$$ 
			\begin{cases} 
				\used{e}[\mathV] = \top & \text{if } e[\mathV] = \top \\
				\used{e}[\mathV] = \bot & \text{if } e[\mathV] = \bot \\
				\avail{e}[\mathV] = \top & \text{if } e[\mathV] = \bot \\
				\avail{e}[\mathV] = \bot & \text{if } e[\mathV] = \top \\
			\end{cases}
			$$
		\item given an annotated sequent $\Delta$ and an assignment $\mathV$ that covers $\vars{\Delta}$
			$$ \Delta[\mathV] = \{ \phi \mid \af{\phi}{e} \in \Delta , e[\mathV] = \top \} $$
	\end{itemize}
\end{define}
\begin{define}
	\label{def:coincidence}
	Given two assignments $\mathV'$ and $\mathV''$ and a set of variables $X$, we say that $\mathV'$ and $\mathV''$ coincide for $X$ -- written $\mathV' \sim_X \mathV''$ -- when they both map all the variables in $X$ to the same values, or
	$$ \mathV' \sim_X \mathV'' \Leftrightarrow \forall x \in X \mid \mathV'(x) = \mathV''(x) $$
\end{define}
\begin{define}[Satisfaiability]
	\label{def:sat}
	Given a set of constraints $\Lambda$ and an assignment function V that covers $\mathrm{vars}(\Lambda)$, we say that $\mathV$ satisfies $\Lambda$ iff every constraint in $\Lambda$ is true under $\mathV$, or
	$$ \sat{\Lambda}{\mathV} \Leftrightarrow \bigwedge_{\lambda \in \Lambda} \lambda[\mathV] = \top $$
\end{define}
We now expand the concept of triadic judgment from \cite{Focusing} (Figure \ref{fig:triadic}) by adding explicit constraints.
\begin{define}
	Given any judgment in our calculus, it can be in either two forms:
	\begin{itemize}
		\item focused or in the synchronous phase, written:
			$$\focus{\Psi}{\Delta}{\phi} \separator \constr{\Lambda}{\mathV}$$
		\item in the asynchronous phase, written:
			$$\async{\Psi}{\Delta}{\Gamma} \separator \constr{\Lambda}{\mathV}$$
	\end{itemize}
	Where 
	\begin{itemize}
		\item $\Psi$ is the same as the triadic calculus of \cite{Focusing}: a set of unrestricted non annotated formulae, or all formulae that can be freely discarded or duplicated.
		\item $\Delta$ and $\Gamma$ are multisets of linear (annotated) formulae, these are respectively the formulae ``put to the side'' and the formulae which are being ``worked on'' during a certain moment of the asynchronous phase;
		\item $\Lambda$ and V are the constraints and the assignment as defined in Definition \ref{def:sat}.
			By adding these members we make the flow of constraints through the proof tree explicit, leaving no ambiguity to where the constraints should be checked.
			This approach to constraints differs from the one in \cite{HarlandPym}.
			The choice of letters is mainly a mnemonic one, constraints $\Lambda$ ``go-up'' the proof tree and solutions V ``come down'' from the leaves.
	\end{itemize}
\end{define}

\begin{define}[Splitting]
	\label{def:split}
	Given a sequent of annotated formulae $\Delta$ and a set of variables $X$ such that $|\Delta| = |X|$, we define the operation of splitting it as a function
	$$ \spliton{\Delta}{X} \mapsto (\Delta_L, \Delta_R) $$
	where
	\begin{align*}
		\Delta_L &= \{ \af{\phi_i}{x_i \wedge e_i} \mid i \in \{1, \dots, n\}\} \\
		\Delta_R &= \{ \af{\phi_i}{\varNot{x_i} \wedge e_i} \mid i \in \{1, \dots, n\}\}
	\end{align*}
	with $n$ the cardinality of $\Delta$, and $\phi_i$ (resp. $e_i$) the formula (resp. the expression) of the $i$-eth annotated formula in $\Delta$ using an arbitrary order.
	The same holds for $x_i$ and $X$.
	With a slight abuse of notation we will write $\Delta_L^X$ and $\Delta_R^X$ to mean respectively the left projection and the right projection of the pair $(\Delta_L, \Delta_R)$.
\end{define}
As a small example for clarity, given the sequent
\begin{align*}
	\Delta &= \af{a \llten b}{x_1}, \af{\llnot{c}}{x_2} \\
	X      &= \{ x_3, x_4 \} 
\end{align*}
this is split into
\begin{align*}
	\Delta_L^X &= \af{a \llten b}{x_3 \varAnd x_1}, \af{\llnot{c}}{x_4 \varAnd x_2} \\
	\Delta_R^X &= \af{a \llten b}{\varNot{x_3} \varAnd x_1}, \af{\llnot{c}}{\varNot{x_4} \varAnd x_2} 
\end{align*}



% TODO: expand
Figure \ref{fig:hp calculus} shows the calculus of \cite{HarlandPym} using our notation of explicit constraints, Figure \ref{fig:calculus} shows our focused calculus instead.
Pym and Harland in \cite{HarlandPym} describe a number different ways of accumulating and checking constraints, roughly comparable to breadth first search, depth first search and a third intermediate method.
Figure \ref{fig:hp calculus} and \ref{fig:calculus} represent what \cite{HarlandPym} calls a ``lazy'' strategy, corresponding to a DFS.
In both the calculi it is easy to see that most of the rules simply make sure the constraints remain consistent, and only the axioms (\derRule{\displayid[1]}, \derRule{\displayid[2]}, \derRule[PH]{A}, $\dots$) and the tensor (\derRule{\displayten}, \derRule[PH]{\displayten}) actually introduce new variables or significant constraints.
\begin{figure}[p]
	\centering
	\input{figures/hp-calculus}
	\caption{The one sided version of the calculus from \cite{HarlandPym} with explicit constraint propagation.\label{fig:hp calculus}}
\end{figure}
\begin{figure}[p]
	\begin{subfigure}{\textwidth}
		\centering
		\input{figures/asy-calculus}
		\caption{Asynchronous rules.\label{fig:asy calculus}}
	\end{subfigure}
\end{figure}
\begin{figure}[p]
	\ContinuedFloat
	\begin{subfigure}{\textwidth}
		\centering
		\input{figures/sync-calculus}
		\caption{Synchronous rules.\label{fig:sync calculus}}
	\end{subfigure}
\end{figure}
\begin{figure}[ht]
	\ContinuedFloat
	\begin{subfigure}{\textwidth}
		\centering
		\input{figures/id-dec-calculus}
		\caption{Identity and decide rules.\label{fig:id dec calculus}}
	\end{subfigure}
	\caption{Focused constraint calculus for Linear Logic. \label{fig:calculus}}
\end{figure}
\begin{define}[New variables]
	\label{def:new}
	In Figure \ref{fig:hp calculus} and \ref{fig:calculus} we write
	$$ \new{x} $$
	to state that the variable name $x$ has not yet occurred in any expression of the proof tree.
	One can think of a counter threaded through the proof tree, which is incremented for each new variable:
	$$
		\AXC{$\phi\;\text{not atom}$}
		\AXC{$\new{x_i}$}
		\AXC{$\vdash^{i + 1, n} \Psi : \Delta \Uparrow \af{\phi}{x_i} \separator \constr{\used{x_i}, \Lambda}{\mathV} $}
		\LeftLabel{\derRule{\displaydecide[2]}}
		\TIC{$\vdash^{i, n} \phi, \Psi : \Delta \Uparrow . \separator \constr{\Lambda}{\mathV} $}
		\DP
	$$
	here $i$ is the current value of the counter, and $n$ is the value of the counter exiting the proof, such that:
	$$
		\AXC{$x\;\text{atom}$}
		\AXC{$\sat{\used{e_1}, \used{e_2}, \avail{\Delta}}{\mathV}$}
		\LeftLabel{\derRule{\displayid[1]}}
		\BIC{$\vdash^{i, i} \Psi : \af{\alpha}{e_1}, \Delta \Uparrow \af{\llnot{\alpha}}{e_2} \separator \constr{\Lambda}{\mathV} $}
		\DP
	$$
	Given this, we then define 
	$$\new{X}$$
	as a set of variable names such that the members have not yet occurred in the proof and are all distinct:
		$$ \left (\forall x_i \in X \mid \new{x_i}\right ) \wedge \left (\forall x_i, x_j \in X \mid i \neq j \Rightarrow x_i \neq x_j \right ) $$
	Then the counter is updated as follows:
	$$
	\AXC{$ \new{X} $}
	\AX$\vdash^{i + |X|,n} \Psi : \Delta_L^X \Downarrow \af{\phi_1}{e} \fCenter \separator \constr{\used{e}, \Lambda}{\mathV'}$
	\noLine
	\UI$\vdash^{n,n'} \Psi : \Delta_R^X \Downarrow \af{\phi_2}{e} \fCenter \separator \constr{\used{e}, \Lambda}{\mathV''}$
	\AXC{$\mathV' \sim_{\mathrm{vars}(\Delta), X} \mathV''$}
	\LeftLabel{\derRule{\displayten}}
	\TIC{$\vdash^{i, n'} \Psi : \Delta \Downarrow \af{\phi_1 \llten \phi_2}{e} \separator \constr{\Lambda}{\mathV''}$} 
	\DP
	$$
\end{define}
\begin{define}
	Given a judgment
	$$ \vdash \Delta $$
	the corresponding judgment in the calculus of Figure \ref{fig:calculus} is
	$$ \async{.}{.}{\hat{\Delta}}\separator \constr{\used{X}}{\mathV} $$
	with $\new{X}$, and
	$$ \hat{\Delta} = \{ \af{\phi_i}{x_i} \mid \phi_i \in \Delta, x_i \in X\} $$
	for some arbitrary ordering of $\Delta$ and $X$.
\end{define}

\begin{lemma}
	\label{lemma:cap}
	For all sequents $\Delta$ and assignments $\mathV$ which cover $\vars{\Delta} \cup X$:
	$$ \Delta_L^X[\mathV] \cap \Delta_R^X[\mathV] = \varnothing $$
\end{lemma}
\begin{proof}
	This is a simple consequence of the fact that if $\phi \in \Delta_L^X[\mathV]$ there is a annotated formula $\af{\phi}{e} \in \Delta$ such that 
	$$ e[\mathV] = \top $$
	Since $e$ is defined as a conjunction on boolean variables, all the variables in it must evaluate to true.
	It is straightforward to see that if the variable added by the split in $\Delta_L^X$ is $x_i$, and the corresponding one in $\Delta_R^X$ is $\varNot{x_i}$, then when $x_i$ is true in the assignment V, $\llnot{x_i}$ is false.
	Hence $\phi \not \in \Delta_R^X[\mathV]$.
	The same can be done to show that if $\phi \in \Delta_R^X[\mathV]$ then $\phi \not \in \Delta_L^X[\mathV]$.
\end{proof}

\begin{lemma}
	\label{lemma:cup}
	For all sequents $\Delta$ and assignments $\mathV$ which cover $\vars{\Delta} \cup X$,
	$$ \Delta[\mathV] = \Delta_L^X[\mathV] \cup \Delta_R^X[\mathV] $$
\end{lemma}
\begin{proof}
	The simpler side is $\Delta_L^X[\mathV] \cup \Delta_R^X[\mathV] \subseteq \Delta[\mathV]$, since it  holds by the definition of the split (Definition \ref{def:split}).
	For the other side, suppose there was a formula $\phi$ such that $\phi \in \Delta[\mathV]$ and $\phi \not \in \Delta_L^X[\mathV] \cup \Delta_R^X[\mathV]$.
	This means that for some variable $x_i$ 
	\begin{align*}
		\af{\phi}{e} \in \Delta &\Rightarrow e[\mathV] = \top \\
		\af{\phi}{x_i \varAnd e} \not \in \Delta_L^X &\Rightarrow x_i \varAnd e[\mathV] = \bot \\
		\af{\phi}{\varNot{x_i} \varAnd e} \not \in \Delta_R^X &\Rightarrow \varNot{x_i} \varAnd e[\mathV] = \bot
	\end{align*}
	But either $x_i$ or $\varNot{x_i}$ must be true in a certain assignment, thus either $x_i \varAnd e$ or $\varNot{x_i} \varAnd e$ must be true, contradicting the hypothesis.
\end{proof}

% TODO: write this better
\begin{teor}[Soundness]\label{thm:soundness}
	For any judgment in our constraint calculus:
	\begin{itemize}
		\item if $ \async{\Psi}{\Delta}{\Gamma} \separator \constr{\Lambda}{\mathV} $, then $ \async[A]{\Psi}{\Delta}{\Gamma}$;
		\item if $ \focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\Lambda}{\mathV} $, then $ \focus[A]{\Psi}{\Delta}{\phi}$.
	\end{itemize}
	Where $A$ is the triadic calculus of \cite{Focusing} (Figure \ref{fig:triadic}).
\end{teor}
\begin{proof}
	We will proceed by mutual induction on the structure of the given derivation.
	The four base cases are proved essentially in the same way, with the exception of top ($\displaytop$):
	\begin{itemize}
		\indCase{\displayid[1]} Given the rule \derRule{\displayid[1]}
			$$
			\AXC{$\isNegLit{\alpha}$}
			\AXC{$ \sat{\used{e_1}, \used{e_2}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[I_1]$}
			\BIC{$\focus{\Psi}{\af{\alpha}{e_1}}{\af{\llnot{\alpha}}{e_2}, \Delta} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			looking at the constraints we get that
			\begin{align*}
				\Delta[\mathV] &= \varnothing \tag{Because of $\avail{\Delta}$} \\
				\af{\alpha}{e_1}[\mathV] &= \alpha \tag{Because of $\used{e_1}$} \\
				\af{\llnot{\alpha}}{e_2}[\mathV] &= \llnot{\alpha} \tag{Because of $\used{e_2}$}
			\end{align*}
			so we can rewrite this as
			$$
			\AXC{$\isNegLit{\alpha}$}
			\LeftLabel{$[I_1]$}
			\UIC{$\focus[A]{\Psi}{\alpha}{\llnot{\alpha}}$}
			\DP
			$$
		\indCase{\displayid[2]} Given the rule \derRule{\displayid[2]}
			$$
			\AXC{$\isNegLit{\alpha}$}
			\AXC{$ \sat{\used{e}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[I_2]$}
			\BIC{$\focus{\Psi, \alpha}{\Delta}{\af{\llnot{\alpha}}{e}} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			proceeding as above we get
			\begin{align*}
				\Delta[\mathV] &= \varnothing \\
				\af{\llnot{\alpha}}{e}[\mathV] &= \llnot{\alpha}
			\end{align*}
			thus
			$$
			\AXC{$\isNegLit{\alpha}$}
			\LeftLabel{$[I_2]$}
			\UIC{$\focus[A]{\alpha, \Psi}{.}{\llnot{\alpha}}$}
			\DP
			$$
		\indCase{\displayone} Given the rule \derRule{\displayone}
			$$
			\AXC{$\sat{ \used{e}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[1]$}
			\UIC{$\focus{\Psi}{\Delta}{\af{\llone}{e}} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			proceeding as above we get
			\begin{align*}
				\Delta[\mathV] &= \varnothing \\
				\af{\llone}{e}[\mathV] &= \llone
			\end{align*}
			thus
			$$
			\AXC{}
			\LeftLabel{$[\llone]$}
			\UIC{$\focus[A]{\Psi}{.}{\llone}$}
			\DP
			$$
		\indCase{\displaytop} Given the rule \derRule{\displaytop} we have that
			$$ 
			\AXC{}
			\UIC{$\async{\Psi}{\Delta}{\af{\top}{-}, \Gamma} \separator \constr{-}{-}$} 
			\DP
			$$
			Thus we can choose whichever assignment $\mathV$ that covers $\vars{\Delta, \Gamma}$, and obtain
			$$ 
			\AXC{}
			\UIC{$\async[A]{\Psi}{\Delta[\mathV]}{\top, \Gamma[\mathV]}$}
			\DP
			$$
	\end{itemize}
	The induction then follows like this:
	\begin{itemize}
		\indCase{\displayten} Given the rule \derRule{\displayten}, we apply the inductive hypothesis, and from the premises 
			\begin{align*}
				&\focus{\Psi}{\Delta_L^X}{\af{\phi_1}{e}} \separator \constr{\used{e}, \Lambda}{\mathV'} \\
				&\focus{\Psi}{\Delta_R^X}{\af{\phi_2}{e}} \separator \constr{\used{e}, \Lambda}{\mathV''} 
			\end{align*}
			we extract the triadic proofs
			\begin{align*}
				&\focus[A]{\Psi}{\Delta_L^X[\mathV']}{\phi_1} \\
				&\focus[A]{\Psi}{\Delta_R^X[\mathV'']}{\phi_2} 
			\end{align*}
			Since $\mathV' \sim_{\vars{\Delta}, X} \mathV''$, this can be rewritten as
			\begin{align*}
				&\focus[A]{\Psi}{\Delta_L^X[\mathV'']}{\phi_1} \\
				&\focus[A]{\Psi}{\Delta_R^X[\mathV'']}{\phi_2} 
			\end{align*}
			Furthermore because of Lemma \ref{lemma:cap} we have that $\Delta_L^X[\mathV'']$ and $\Delta_R^X[\mathV'']$ are disjoint, so the contexts of the two branches are separated.
			Hence we can apply \derRule[A]{\displayten}, and obtain
			$$ \focus[A]{\Psi}{\Delta_L^X[\mathV''], \Delta_R^X[\mathV'']}{\phi_1 \llten \phi_2} $$
			which, because of Lemma \ref{lemma:cup}, correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llten \phi_2}{e}} \separator \constr{\Lambda}{\mathV''} $$
			should be mapped to.
		\indCase{\displaywith} Given the rule \derRule{\displaywith}, we apply the inductive hypothesis, and from the premises
			\begin{align*}
				&\async{\Psi}{\Delta}{\af{\phi_1}{e}, \Gamma} \separator \constr{\used{e}, \Lambda}{\mathV'} \\
				&\async{\Psi}{\Delta}{\af{\phi_2}{e}, \Gamma} \separator \constr{\used{e}, \Lambda}{\mathV''} 
			\end{align*}
			we extract the triadic proofs
			\begin{align*}
			 	&\async{\Psi}{\Delta[\mathV']}{\phi_1, \Gamma[\mathV']}\\
			 	&\async{\Psi}{\Delta[\mathV'']}{\phi_2, \Gamma[\mathV'']} 
			\end{align*}
			Now, since $\mathV' \sim_{\vars{\Delta, \Gamma}} \mathV''$, we can write
			\begin{align*}
				&\async{\Psi}{\Delta[\mathV'']}{\phi_1, \Gamma[\mathV'']} \\
				&\async{\Psi}{\Delta[\mathV'']}{\phi_2, \Gamma[\mathV'']} 
			\end{align*}
			this way we can apply \derRule[A]{\displaywith} to obtain
			$$ \async[A]{\Psi}{\Delta[\mathV'']}{\phi_1 \llwith \phi_2, \Gamma[\mathV'']} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\phi_1 \llwith \phi_2}{e}, \Gamma} \separator \constr{\Lambda}{\mathV''} $$
			should be mapped to.
		\indCase{\displaypar} Given the rule \derRule{\displaypar}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi_1}{e}, \af{\phi_2}{e}, \Gamma} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof 
			$$\async[A]{\Psi}{\Delta[\mathV]}{\phi_1, \phi_2, \Gamma[\mathV]} $$
			Then we apply \derRule[A]{\displaypar}, and obtain
			$$\async[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llpar \phi_2, \Gamma[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\phi_1 \llpar \phi_2}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
			% Furthermore, since the assignment $\mathV$ is the same between premise and conclusion, then $\Delta[\mathV]$ and $\Gamma[\mathV]$ stay the same, and since the assignment respects the constraint $\used{e}$, then $\phi_1 \llpar \phi_2$ must be present.
		\indCase{\displayplus[L]} Given the rule \derRule{\displayplus[L]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi_1}{e}} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1} $$
			Then we apply \derRule[A]{\displayplus[L]}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llplus \phi_2} $$
			which correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be.
		\indCase{\displayplus[R]} Given the rule \derRule{\displayplus[R]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi_2}{e}} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_2} $$
			Then we apply \derRule[A]{\displayplus[R]}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llplus \phi_2} $$
			which correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be.
		\indCase{\displaybang} Given the rule \derRule{\displaybang}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \avail{\Delta}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{.}{\phi} $$
			since $\Delta[\mathV] = \varnothing$ under assignment $\mathV$.
			We then apply \derRule[A]{\displaybang} and obtain
			$$ \focus[A]{\Psi}{.}{\;\llbang{\phi}} $$
			which correctly corresponds to what the conclusion 
			$$ \focus{\Psi}{\Delta}{\af{\llbang{\phi}}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to
		\indCase{\displaywn} Given the rule \derRule{\displaywn}, we apply the inductive hypothesis, and from the premise
			$$ \async{\phi, \Psi}{\Delta}{\Gamma} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\phi, \Psi}{\Delta[\mathV]}{\Gamma[\mathV]}$$
			Then we apply \derRule[A]{\displaywn} and obtain
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\,\llwn{\phi}, \Gamma[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\llwn{\phi}}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaybot} Given the rule \derRule{\displaybot}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\Gamma} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\Gamma[\mathV]} $$
			Then we apply \derRule[A]{\displaybot} and obtain
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\llbot, \Gamma[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\llbot}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaydecide[1]} Given the rule \derRule{\displaydecide[1]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaydecide[1]} and obtain
			$$ \async[A]{\Psi}{\phi, \Delta[\mathV]}{.} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\af{\phi}{e}, \Delta}{.} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaydecide[2]} Given the rule \derRule{\displaydecide[2]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi}{x}} \separator \constr{\used{x}, \Lambda}{\mathV} $$
			where $\new{x}$, we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaydecide[2]} and obtain
			$$ \async[A]{\phi, \Psi}{ \Delta[\mathV]}{.} $$
			which correctly corresponds to what the conclusion
			$$ \async{\phi, \Psi}{\Delta}{.} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaytodelta} Given the rule \derRule{\displaytodelta}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\af{\phi}{e}, \Delta}{\Gamma} \separator \constr{\Lambda}{\mathV} $$
			when we extract the triadic proof, two cases arise:
			\begin{itemize}
				\item $\phi$ disappears under assignment $\mathV$, and we get
					$$ \async[A]{\Psi}{\Delta[\mathV]}{\Gamma[\mathV]} $$
					which correctly corresponds to what the conclusion
					$$ \async{\Psi}{\Delta}{\af{\phi}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
					should be mapped to, since if $\phi$ disappears from the premise, it will also disappear from the conclusion.
				\item $\phi$ remains under assignment $\mathV$, and we get
					$$ \async[A]{\Psi}{\phi, \Delta[\mathV]}{\Gamma[\mathV]} $$
					Then we apply \derRule[A]{\displaytodelta} and obtain
					$$ \async[A]{\Psi}{\Delta[\mathV]}{\phi, \Gamma[\mathV]} $$
					which correctly corresponds to what the conclusion
					$$ \async{\Psi}{\Delta}{\af{\phi}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
					should be mapped to.
			\end{itemize}
		\indCase{\displaytoasy} Given the rule \derRule{\displaytoasy}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaytoasy}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			which correctly corresponds to what the conclusion 
			$$ \focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
	\end{itemize}
\end{proof}
In Appendix \ref{appendix:example} we show an example derivation using our calculus of Figure \ref{fig:calculus}.


