\chapter{The focused calculus}\label{chapter:calculus}
% Before describing the calculus we must give some preliminary definitions

% Explaination on constraints why we use them ecc ecc ecc
In this chapter we will define the focused one sided constraint calculus for full linear logic.
Before all though, we give we definition of a linear logic formula.
\begin{define}[Linear logic formula]
	\label{def:ll formula}
	A linear logic formula is a formula having the ???
	\begin{center}
		\begin{tblr}{ colspec = {cccccccccr}
			, cells = { mode = math } 
			% , vborder{1-4} = { leftspace = 0pt, rightspace = 0pt } 
			}
			\phi & ::=  & 1              &\mid& \phi \llten \phi  &\mid& \bot &\mid& \phi \llpar \phi  & \text{(Multiplicatives and their constants)} \\
			& \mid & 0              &\mid& \phi \llplus \phi &\mid& \top &\mid& \phi \llwith \phi & \text{(Additives and their constants)} \\
			& \mid & \llbang{\phi}  &\mid& \llwn{\phi}       &    &      &    &                   & \text{(Exponentials)} \\
			& \mid & \llnot{\alpha} &\mid& \alpha	      &    &      &    &                   & \text{(Where $\alpha$ is an atom)}
		\end{tblr}
	\end{center}
	We will use $\phi$ for formulae and $\alpha$ for atoms.
	A sequent, written $\Delta$ or $\Phi$, is a multiset of formulae.
\end{define}

\section{Normalization}\label{sec:normalization}
Since in linear logic negation is symmetric and an involution, it is usual to work only with formulae in negated normal form.
\begin{define}[Negated Normal Form -- NNF]
	\label{def:nnf}
	A formula is in NNF if all its linear implications ($\lolli$) are expanded to pars ($\llpar$) using the tautology
	$$ a \lolli b \Leftrightarrow \llnot{a} \llpar b$$
	and negation is pushed down to atoms.	% terms?
	Intuitively, a sequent is in NNF if all its formulae are in NNF.
\end{define}
A generic formula is then normalized applying recursively the DeMorgan rules for linear logic, until NNF is reached.
The process of normalization takes a two-sided judgment, of the form
$$ \Delta \vdash \Gamma $$
and transforms it into a one-sided judgment
$$ \vdash \Delta' $$
where the right side is composed of the normalization of $\Gamma$ and $\llnot{\Delta}$.

This choice has some implementation-wise advantages, but for now we will only care about the fact that it shrinks the size of the complete calculus by roughly half, since we only have to deal with the right rules of the connectives.

\section{Focusing}
Focusing is a technique described by Andreoli in his seminal paper \cite{Focusing}.
There he recognizes two alternating phases in a proof: a deterministic phase, where the order of rule application does not matter; and a non deterministic phase, where several choices may be available.
These two phases are respectively called asynchronous and synchronous phase.
All terms are assigned a polarity: 
\begin{itemize}
	\item connectives with a synchronous right rule are defined to have a positive polarity
		$$ \displayten, \displayplus, \displaybang, \displayone$$
	\item whereas connectives with a asynchronous right rule have a negative one
		$$ \displaypar, \displaywith, \displaywn, \displaytop, \displaybot$$
\end{itemize}
Atoms also have polarities, which may be assigned with some arbitrarily complex mechanisms.
We will follow \cite{LiangMiller} and simply assign atoms with a negative polarity and negated atoms with a positive one.
Since we will work in one sided linear logic and connectives have only right rules, positive connectives are also called synchronous, and negative connectives asynchronous.
\begin{define}
	Based on the definitions above we define the following predicates:
	\begin{itemize}
		\item ``$\phi \; \mathrm{atom}$'' is true whenever $\phi$ is an atom;
		\item ``$\phi \; \mathrm{asy}$'' is true whenever $\phi$ is an asynchronous connective;
		\item ``$\phi \; \mathrm{negative}$'' is true whenever $\phi$ is either an atom or an asynchronous connective, so
			$$ \phi \; \mathrm{negative} = \phi \; \mathrm{atom} \vee \phi \; \mathrm{asy} $$
	\end{itemize}
\end{define}

\section{Constraints}
The calculus uses constraints to manage the resources.
\begin{define}[Variables, expressions]
	\label{def:bool-expr}
	A boolean variable is simply a symbol to which one can associate a value of true or false.
	A boolean expression, in our case, is just a conjunction of possibly negated boolean variables as 
	\begin{center}
	\begin{tblr}{ colspec = {cccccr}, cells = { mode = math } }
		x & ::=  & x_i &\mid& \overline{x_i} & \text{(Variable)}\\
		e & ::=  & x \wedge e    &\mid& x & \text{(Expression)} \\
	\end{tblr}
	\end{center}
	We will call $e$ such a conjunction and $x$ the single boolean variables.
	Given a boolean expression $e$ we write
	$$ \mathrm{vars}(e) = \{ x_i \mid x_i \in e \} $$
\end{define}
\begin{define}[New variables]
	\label{def:new}
	Sometimes we will write 
	$$ \new{x}, \new{X} $$
	These respectively mean that:
	\begin{itemize}
		\item the variable name $x$ has not yet occurred in any expression of the proof tree, i.e. does not appear in any constraint of the father, or of its (the father) siblings and their sub-trees;
		\item each variable name $x_i, x_j \in X$ has not yet occurred in the proof and each variable in $X$ is distinct:
			$$ \forall i \mid \new{x_i} \wedge \forall i, j \mid i \neq j \Rightarrow x_i \neq x_j $$
	\end{itemize}
\end{define}

\begin{define}[Annotated formula]
	\label{def:annotated}
	Given a formula $\phi$ defined as in Figure \ref{ctives} and a boolean expression $e$ defined as in Definition \ref{def:bool-expr}, an \textit{annotated formula} is simply a term 
	$$ \af{\phi}{e} $$
	that associates the formula to the expression.
	We denote 
	\begin{itemize}
		\item $ \mathrm{exp}(\af{\phi}{e}) = e $
			as the operation of extracting the boolean expression associated to a given formula; and then extend this notation to sequents such that $ \mathrm{exp}(\Delta) $ is the set of all boolean expressions of $\Delta$.
		\item $\mathrm{vars}(\af{\phi}{e}) = \mathrm{vars}(e) $
			as the operation of extracting the set of variables appearing in the expression $e$; and then extend this notation to sequents such that $ \mathrm{vars}(\Delta)$ is the set of all the variables appearing in the boolean expressions of the annotated formulae of $\Delta$.
	\end{itemize}
\end{define}
It is important to note that only the topmost connective gets annotated, and not the sub-formulae.

The purpose of putting formulae and expressions together in the annotated formula is twofold:
\begin{itemize}
	\item the actions taken on the formula determine the constraints that will be generated, and these depend on the variables associated to said formula;
	\item after the constraints are solved we can query the assignment of the variables and find out if the associated formula is used or not in a certain branch of a proof.
\end{itemize}
These constraints may be only of two kinds: ``$\avail{e}$'' and ``$\used{e}$''.
\begin{define}[Constraints]
	\label{def:constraints}
	Given an annotated formula $\af{\phi}{e}$ as in Definition \ref{def:annotated}, a constraint $\lambda$ may be of two kinds
	\begin{itemize}
		\item ``$\used{e}$'' states that the formula $\phi$ gets consumed in this branch of the proof.
			This corresponds to saying the expression $e$ is true or
			$$ x_i \wedge \dots \wedge x_j \leftrightarrow \top $$
		\item ``$\avail{e}$'' states that the formula $\phi$ does not get consumed in this branch of the proof, and thus is available to be used in another branch.
			This corresponds to saying the expression $e$ is not true or
			$$ x_i \wedge \dots \wedge x_j \leftrightarrow \bot $$
	\end{itemize}
	We then extend these predicates to sequents
	\begin{align*}
		\used{\Delta} &= \{ \used{e} \mid e \in \mathrm{exp}(\Delta) \} \\
		\avail{\Delta} &= \{ \avail{e} \mid e \in \mathrm{exp}(\Delta) \}
	\end{align*}
	We denote
	\begin{itemize}
		\item $\mathrm{exp}(\lambda) = e$ where $\lambda$ is either ``$\used{e}$'' or ``$\avail{e}$'';
		\item $\mathrm{vars}(\lambda) = \mathrm{vars}(\mathrm{exp}(\lambda))$, and extend this notation to a set of constraints $\Lambda$
			$$ \mathrm{vars}(\Lambda) = \bigcup_{\lambda \in \Lambda} \mathrm{vars}(\lambda) $$
	\end{itemize}
\end{define}
\begin{define}[Evaluation]
	Given a boolean expression $e$ and a function V mapping variables to their values 
	$$ \mathV : \{ x_1, x_2, \dots, x_n \} \rightarrow \{ \top, \bot \} $$
	with $\mathrm{vars}(e) \subseteq \mathrm{Dom}(\mathV)$; we write
		$$ e[\mathV] = e[\dots, x_i \subst \mathV(x_i), x_j \subst \mathV(x_j), \dots] $$
	as the value of the expression $e$ substituting with the assignment V.
	We extend this notation, such that
	\begin{itemize}
		\item given a constraint $\lambda$ and an assignment V, such that $\mathrm{vars}(\lambda) \subseteq \mathrm{Dom}(\mathV)$
			$$ 
			\begin{cases} 
				\used{e}[\mathV] = \top & \text{if } e[\mathV] = \top \\
				\used{e}[\mathV] = \bot & \text{if } e[\mathV] = \bot \\
				\avail{e}[\mathV] = \top & \text{if } e[\mathV] = \bot \\
				\avail{e}[\mathV] = \bot & \text{if } e[\mathV] = \top \\
			\end{cases}
			$$
		\item given an annotated sequent $\Delta$ and an assignment V, such that $\mathrm{vars}(\Delta) \subseteq \mathrm{Dom}(\mathV)$
			$$ \Delta[\mathV] = \{ \phi \mid \af{\phi}{e} \in \Delta , e[\mathV] = \top \} $$
	\end{itemize}
\end{define}
A branch is considered correct if its constraints are satisfiable.
Otherwise the branch of the proof fails.
\begin{define}[Covering]
	Given an assignment $\mathV$ and a set of variables $X$, we say that $\mathV$ \textit{covers} $X$ iff
	$$ \forall x \in X \mid x \in \mathrm{Dom}(\mathV) $$
\end{define}
\begin{define}[Satisfaiability of constraints]
	\label{def:sat}
	Given a set of constraints $\Lambda$ and an assignment function V that covers $\mathrm{vars}(\Lambda)$, we say that $\mathV$ satisfies $\Lambda$ iff every constraint in $\Lambda$ is true under $\mathV$, or
	$$ \sat{\Lambda}{\mathV} \Leftrightarrow \bigwedge_{\lambda \in \Lambda} \lambda[\mathV] = \top $$
\end{define}

We now expand the concept of triadic sequent of \cite{Focusing} by adding constraints.
% TODO: expand
\begin{define}[Members of the sequent]
	Given any sequent it can be in either two forms:
	\begin{itemize}
		\item focused or in the synchronous phase, written:
			$$\focus{\Psi}{\Delta}{\phi} \separator \constr{\Lambda}{\mathV}$$
		\item in the asynchronous phase, written:
			$$\async{\Psi}{\Delta}{\Phi} \separator \constr{\Lambda}{\mathV}$$
	\end{itemize}
	Where 
	\begin{itemize}
		\item $\Psi$ is a set of unrestricted non annotated formulae, or all formulae that can be freely discarded or duplicated.
		\item $\Delta$ and $\Phi$ are multi-sets of linear (annotated) formulae, these are respectively the formulas ``put to the side'' and the formulae which are being ``worked on'' during a certain moment of the asynchronous phase;
		\item $\Lambda$ and V are the constraints and the assignment as defined in Definition \ref{def:sat}.
			By adding these members we make the flow of constraints through the proof tree explicit, leaving no ambiguity to where the constraints should be checked.
			This approach to constraints differs from the one in \cite{HarlandPym}, which prioritizes generality.
			The choice of letters is mainly a mnemonic one, constraints $\Lambda$ ``go-up'' the proof tree and solutions V ``come down'' from the leaves.
	\end{itemize}
\end{define}

\begin{define}[Splitting]
	\label{def:split}
	Given a sequent of annotated formulae $\Delta$ and a set of variables $X$ such that $|\Delta| = |X|$ we define the operation of splitting it as a function
	$$ \mathrm{split}(\Delta, X) \mapsto (\Delta_L, \Delta_R) $$
	where
	\begin{align*}
		\Delta_L &= \{ \af{\phi_i}{x_i \wedge e_i} \mid i \in \{1, \dots, n\}\} \\
		\Delta_R &= \{ \af{\phi_i}{\varNot{x_i} \wedge e_i} \mid i \in \{1, \dots, n\}\}
	\end{align*}
	with $n$ the cardinality of $\Delta$, and $\phi_i$ (resp. $e_i$) the formula (resp. the expression) of the $i$-eth annotated formula in $\Delta$ using an arbitrary order.
	The same holds for $x_i$ and $X$.

	\noindent With a slight abuse of notation we will write $\Delta_L^X$ and $\Delta_R^X$ respectively as the left projection and the right projection of the pair $(\Delta_L, \Delta_R)$.
\end{define}
As a small example for clarity, given the sequent
\begin{align*}
	\Delta &= \af{a \llten b}{x_1}, \af{\llnot{c}}{x_2} \\
	X      &= \{ x_3, x_4 \} 
\end{align*}
this is split into
\begin{align*}
	\Delta_L^X &= \af{a \llten b}{x_3 \varAnd x_1}, \af{\llnot{c}}{x_4 \varAnd x_2} \\
	\Delta_R^X &= \af{a \llten b}{\varNot{x_3} \varAnd x_1}, \af{\llnot{c}}{\varNot{x_4} \varAnd x_2} 
\end{align*}

\begin{define}
	Given two assignments $\mathV'$ and $\mathV''$ and a set of variables $X$, we say that $\mathV'$ and $\mathV''$ coincide for $X$, written
	$$ \mathV' \sim_X \mathV'' \Leftrightarrow \forall x \in X \mid \mathV'(x) = \mathV''(x) $$
\end{define}
% example of splitting and variables

% One simple but important detail that will be useful later when explaining the Prolog implementation is noting that the variables in common between two branches with the same root are always introduced before the two branches diverge.
% Or -- put differently -- all new variables introduced in any point of a path from the root of the proof to a leaf may appear only in the subtrees.	% sistemo
Figure \ref{fig:hp calculus} shows the calculus of \cite{HarlandPym} using our notation of explicit constraints.
What is actually represented is roughly what they define as the ``lazy'' strategy.
Figure \ref{fig:calculus} instead presents our focused constraint calculus.

\begin{lemma}
	\label{lemma:cap}
	For all sequents $\Delta$ and assignments $\mathV$ which cover $\mathrm{vars}(\Delta)$:
	$$ \Delta_L^X[\mathV] \cap \Delta_R^X[\mathV] = \varnothing $$
\end{lemma}
\begin{proof}
	This is a simple consequence of the fact that if $\phi \in \Delta_L^X[\mathV]$ there is a annotated formula $\af{\phi}{e} \in \Delta$ such that 
	$$ e[\mathV] = \top $$
	Since $e$ is defined as a conjunction on boolean variables, all the variables in it must evaluate to true.
	It is straightforward to see that if the variable added by the split in $\Delta_L^X$ is $x_i$, and the corresponding one in $\Delta_R^X$ is $\varNot{x_i}$, then when $x_i$ is true in the assignment V, $\llnot{x_i}$ is false.
	Hence $\phi \not \in \Delta_R^X[\mathV]$.
	The same can be done to show that if $\phi \in \Delta_R^X[\mathV]$ then $\phi \not \in \Delta_L^X[\mathV]$.
\end{proof}
\begin{lemma}
	\label{lemma:cup}
	For all sequents $\Delta$ and assignments $\mathV$ which cover $\vars{\Delta}$,
	$$ \Delta[\mathV] = \Delta_L^X[\mathV] \cup \Delta_R^X[\mathV] $$
\end{lemma}
\begin{proof}
	The simpler side is $\Delta_L^X[\mathV] \cup \Delta_R^X[\mathV] \subseteq \Delta[\mathV]$, since it  holds by the definition of the split (Definition \ref{def:split}).
	For the other side, suppose there was a formula $\phi$ such that $\phi \in \Delta[\mathV]$ and $\phi \not \in \Delta_L^X[\mathV] \cup \Delta_R^X[\mathV]$.
	This means that for some variable $x_i$ 
	\begin{align*}
		\af{\phi}{e} \in \Delta &\Rightarrow e[\mathV] = \top \\
		\af{\phi}{x_i \varAnd e} \not \in \Delta_L^X &\Rightarrow x_i \varAnd e[\mathV] = \bot \\
		\af{\phi}{\varNot{x_i} \varAnd e} \not \in \Delta_R^X &\Rightarrow \varNot{x_i} \varAnd e[\mathV] = \bot
	\end{align*}
	But either $x_i$ or $\varNot{x_i}$ must be true in a certain assignment, thus either $x_i \varAnd e$ or $\varNot{x_i} \varAnd e$ must be true, contradicting the hypothesis.
\end{proof}
\begin{lemma}
	Given any sequent $\Delta$ and any assignment $\mathV$ which covers $\vars{\Delta}$, splitting induces a partition on the sequent $\Delta[\mathV]$.
\end{lemma}
\begin{proof}
	See Lemma \ref{lemma:cap} and \ref{lemma:cup}.
\end{proof}

\begin{teor}[Soundness]\label{thm:soundness}
	For some assignment $\mathV$:
	\begin{align*}
		\async{\Psi}{\Delta}{\Phi} \separator \constr{\Lambda}{\mathV} &\Rightarrow \async[A]{\Psi}{\Delta[\mathV]}{\Phi[\mathV]} \\
		\focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\Lambda}{\mathV} &\Rightarrow \focus[A]{\Psi}{\Delta[\mathV]}{\phi}
	\end{align*}
	where $A$ is the triadic calculus of \cite{Focusing} defined in Figure \ref{fig:triadic}.
\end{teor}
\begin{proof}
	The three base cases are proved essentially in the same way:
	\begin{itemize}
		\indCase{\displayid[1]} Given the rule \derRule{\displayid[1]}
			$$
			\AXC{$\isNegLit{\alpha}$}
			\AXC{$ \sat{\used{e_1}, \used{e_2}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[I_1]$}
			\BIC{$\focus{\Psi}{\af{\alpha}{e_1}}{\af{\llnot{\alpha}}{e_2}, \Delta} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			looking at the constraints we get that
			\begin{align*}
				\Delta[\mathV] &= \varnothing \tag{Because of $\avail{\Delta}$} \\
				\af{\alpha}{e_1}[\mathV] &= \alpha \tag{Because of $\used{e_1}$} \\
				\af{\llnot{\alpha}}{e_2}[\mathV] &= \llnot{\alpha} \tag{Because of $\used{e_2}$}
			\end{align*}
			so we can rewrite this as
			$$
			\AXC{$\isNegLit{\alpha}$}
			\LeftLabel{$[I_1]$}
			\UIC{$\focus[A]{\Psi}{\alpha}{\llnot{\alpha}}$}
			\DP
			$$
		\indCase{\displayid[2]} Given the rule \derRule{\displayid[2]}
			$$
			\AXC{$\isNegLit{\alpha}$}
			\AXC{$ \sat{\used{e}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[I_2]$}
			\BIC{$\focus{\Psi, \alpha}{\Delta}{\af{\llnot{\alpha}}{e}} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			proceeding as above we get
			\begin{align*}
				\Delta[\mathV] &= \varnothing \\
				\af{\llnot{\alpha}}{e}[\mathV] &= \llnot{\alpha}
			\end{align*}
			thus
			$$
			\AXC{$\isNegLit{\alpha}$}
			\LeftLabel{$[I_2]$}
			\UIC{$\focus[A]{\alpha, \Psi}{.}{\llnot{\alpha}}$}
			\DP
			$$
		\indCase{\displayone} Given the rule \derRule{\displayone}
			$$
			\AXC{$\sat{ \used{e}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[1]$}
			\UIC{$\focus{\Psi}{\Delta}{\af{\llone}{e}} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			proceeding as above we get
			\begin{align*}
				\Delta[\mathV] &= \varnothing \\
				\af{\llone}{e}[\mathV] &= \llone
			\end{align*}
			thus
			$$
			\AXC{}
			\LeftLabel{$[\llone]$}
			\UIC{$\focus[A]{\Psi}{.}{\llone}$}
			\DP
			$$
	\end{itemize}
	The induction then follows like this:
	\begin{itemize}
		\indCase{\displayten} Given the rule \derRule{\displayten}, we apply the inductive hypothesis, and from the premises 
			\begin{align*}
				&\focus{\Psi}{\Delta_L^X}{\af{\phi_1}{e}} \separator \constr{\used{e}, \Lambda}{\mathV'} \\
				&\focus{\Psi}{\Delta_R^X}{\af{\phi_2}{e}} \separator \constr{\used{e}, \Lambda}{\mathV''} 
			\end{align*}
			we extract the triadic proofs
			\begin{align*}
				&\focus[A]{\Psi}{\Delta_L^X[\mathV']}{\phi_1} \\
				&\focus[A]{\Psi}{\Delta_R^X[\mathV'']}{\phi_2} 
			\end{align*}
			Since $\mathV' \sim_{\vars{\Delta}, X} \mathV''$, this can be rewritten as
			\begin{align*}
				&\focus[A]{\Psi}{\Delta_L^X[\mathV'']}{\phi_1} \\
				&\focus[A]{\Psi}{\Delta_R^X[\mathV'']}{\phi_2} 
			\end{align*}
			Furthermore because of Lemma \ref{lemma:cap} we have that $\Delta_L^X[\mathV'']$ and $\Delta_R^X[\mathV'']$ are disjoint, so the contexts of the two branches are separated.
			Hence we can apply \derRule[A]{\displayten}, and obtain
			$$ \focus[A]{\Psi}{\Delta_L^X[\mathV''], \Delta_R^X[\mathV'']}{\phi_1 \llten \phi_2} $$
			which, because of Lemma \ref{lemma:cup}, correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llten \phi_2}{e}} \separator \constr{\Lambda}{\mathV''} $$
			should be mapped to.
		\indCase{\displaytop} Missing
		\indCase{\displaywith} Given the rule \derRule{\displaywith}, we apply the inductive hypothesis, and from the premises
			\begin{align*}
				&\async{\Psi}{\Delta}{\af{\phi_2}{e}, \Phi} \separator \constr{\used{e}, \Lambda}{\mathV'} \\
				&\async{\Psi}{\Delta}{\af{\phi_1}{e}, \Phi} \separator \constr{\used{e}, \Lambda}{\mathV''} 
			\end{align*}
			we extract the triadic proofs
			\begin{align*}
			 	&\async{\Psi}{\Delta[\mathV']}{\phi_2, \Phi[\mathV']} \\
			 	&\async{\Psi}{\Delta[\mathV'']}{\phi_1, \Phi[\mathV'']}
			\end{align*}
			Now since $\mathV' \sim_{\vars{\Delta, \Phi}} \mathV''$, we can write
			\begin{align*}
				&\async{\Psi}{\Delta[\mathV'']}{\phi_2, \Phi[\mathV'']} \\
				&\async{\Psi}{\Delta[\mathV'']}{\phi_1, \Phi[\mathV'']}
			\end{align*}
			this way we can apply \derRule[A]{\displaywith}, to obtain
			$$ \async[A]{\Psi}{\Delta[\mathV'']}{\phi_1 \llwith \phi_2, \Phi[\mathV'']} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\phi_1 \llwith \phi_2}{e}, \Phi} \separator \constr{\Lambda}{\mathV''} $$
			should be mapped to.
%			which after erasing the unused formulae using the assignment becomes
%			\LeftLabel{$[\llwith]$}
%			\BIC{$\async{\Psi}{\Delta[\mathV'']}{\phi_1 \llwith \phi_2, \Phi[\mathV'']}$}
%			or equivalently 
%			\AXC{$\async{\Psi}{\Delta[\mathV']}{\phi_2, \Phi[\mathV']}$}
%			\AXC{$\async{\Psi}{\Delta[\mathV'']}{\phi_1, \Phi[\mathV'']}$}
%			\LeftLabel{$[\llwith]$}
%			\BIC{$\async{\Psi}{\Delta[\mathV']}{\phi_1 \llwith \phi_2, \Phi[\mathV']}$}
%			\DP
%			since  $\Lambda \subseteq \used{e}, \Lambda$ and Lemma \ref{lemma:subsume}.
		\indCase{\displaypar} Given the rule \derRule{\displaypar}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi_1}{e}, \af{\phi_2}{e}, \Phi} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof 
			$$\async[A]{\Psi}{\Delta[\mathV]}{\phi_1, \phi_2, \Phi[\mathV]} $$
			Then we apply \derRule[A]{\displaypar}, and obtain
			$$\async[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llpar \phi_2, \Phi[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\phi_1 \llpar \phi_2}{e}, \Phi} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
			% Furthermore, since the assignment $\mathV$ is the same between premise and conclusion, then $\Delta[\mathV]$ and $\Phi[\mathV]$ stay the same, and since the assignment respects the constraint $\used{e}$, then $\phi_1 \llpar \phi_2$ must be present.
		\indCase{\displayplus[L]} Given the rule \derRule{\displayplus[L]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi_1}{e}} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1} $$
			Then we apply \derRule[A]{\displayplus[L]}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llplus \phi_2} $$
			which correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be.
		\indCase{\displayplus[R]} Given the rule \derRule{\displayplus[R]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi_2}{e}} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_2} $$
			Then we apply \derRule[A]{\displayplus[R]}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llplus \phi_2} $$
			which correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be.
		\indCase{\displaybang} Given the rule \derRule{\displaybang}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \avail{\Delta}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{.}{\phi} $$
			since $\Delta[\mathV] = \varnothing$ under assignment $\mathV$ by Lemma \ref{lemma:empty}.
			Then we apply \derRule[A]{\displaybang} and obtain
			$$ \focus[A]{\Psi}{.}{\llbang{\phi}} $$
			which correctly corresponds to what the conclusion 
			$$ \focus{\Psi}{\Delta}{\af{\llbang{\phi}}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to
		\indCase{\displaywn} Given the rule \derRule{\displaywn}, we apply the inductive hypothesis, and from the premise
			$$ \async{\phi, \Psi}{\Delta}{\Phi} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\phi, \Psi}{\Delta[\mathV]}{\Phi[\mathV]}$$
			Then we apply \derRule[A]{\displaywn} and obtain
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\,\llwn{\phi}, \Phi[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\llwn{\phi}}{e}, \Phi} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaybot} Given the rule \derRule{\displaybot}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\Phi} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\Phi[\mathV]} $$
			Then we apply \derRule[A]{\displaybot} and obtain
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\llbot, \Phi[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\llbot}{e}, \Phi} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaydecide[1]} Given the rule \derRule{\displaydecide[1]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaydecide[1]} and obtain
			$$ \async[A]{\Psi}{\phi, \Delta[\mathV]}{.} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\af{\phi}{e}, \Delta}{.} \separator \constr{\Lambda}{\mathV} $$
		\indCase{\displaydecide[2]} Given the rule \derRule{\displaydecide[2]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi}{x}} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			where $\new{x}$, we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaydecide[2]} and obtain
			$$ \async[A]{\phi, \Psi}{ \Delta[\mathV]}{.} $$
			which correctly corresponds to what the conclusion
			$$ \async{\phi, \Psi}{\Delta}{.} \separator \constr{\Lambda}{\mathV} $$
		\indCase{\displaytodelta} Given the rule \derRule{\displaytodelta}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\af{\phi}{e}, \Delta}{\Phi} \separator \constr{\Lambda}{\mathV} $$
			when we extract the triadic proof, two cases arise:
			\begin{itemize}
				\item $\phi$ disappears under assignment $\mathV$, and we get
					$$ \async[A]{\Psi}{\Delta[\mathV]}{\Phi[\mathV]} $$
					which correctly corresponds to what the conclusion
					$$ \async{\Psi}{\Delta}{\af{\phi}{e}, \Phi} \separator \constr{\Lambda}{\mathV} $$
					should be mapped to, since if $\phi$ disappears from the premise, it will also disappear from the conclusion.
				\item $\phi$ remains under assignment $\mathV$, and we get
					$$ \async[A]{\Psi}{\phi, \Delta[\mathV]}{\Phi[\mathV]} $$
					Then we apply \derRule[A]{\displaytodelta} and obtain
					$$ \async[A]{\Psi}{\Delta[\mathV]}{\phi, \Phi[\mathV]} $$
					which correctly corresponds to what the conclusion
					$$ \async{\Psi}{\Delta}{\af{\phi}{e}, \Phi} \separator \constr{\Lambda}{\mathV} $$
					should be mapped to.
			\end{itemize}
		\indCase{\displaytoasy} Given the rule \derRule{\displaytoasy}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaytoasy}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			which correctly corresponds to what the conclusion 
			$$ \focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
	\end{itemize}
\end{proof}


