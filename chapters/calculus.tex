\chapter{The focused calculus}\label{chapter:calculus}
% Before describing the calculus we must give some preliminary definitions

% Explaination on constraints why we use them ecc ecc ecc
In this chapter we will define the focused one sided constraint calculus for full linear logic.
This calculus is a hybrid between the one defined in \cite{HarlandPym} and the triadic calculus in \cite{Focusing}: it uses focusing for its performance wise advantages; and constraints to manage splitting efficiently.

Before all, we give we definition of a linear logic formula.
\begin{define}[Linear logic formula]
	\label{def:ll formula}
	A linear logic formula is a term defined as follows 
	\begin{center}
		\begin{tblr}{ colspec = {Q[l,t]}
			, cells = { mode = math } 
			% , vborder{1-4} = { leftspace = 0pt, rightspace = 0pt } 
	    		, rows = {abovesep=0pt, belowsep=0pt}
			}
			\left. 
				\begin{tblr}{ colspec = {ccccc}, column{1,5} = {.5cm}, column{3} = {1.5cm}, column{2,4} = {0.2cm}}
					\phi & ::=  & \phi \llten \phi & \mid & 1 \\
					     & \mid & \phi \llpar \phi & \mid & \bot \\
					     & \mid & \phi \lolli \phi &      &
				\end{tblr} \; \right \} \text{(Multiplicatives and their identities)} \\
			\left.
				\begin{tblr}{ colspec = {ccccc}, column{1,5} = {.5cm}, column{3} = {1.5cm}, column{2,4} = {0.2cm}}
					& \mid & \phi \llplus \phi & \mid & 0 \\
					& \mid & \phi \llwith \phi & \mid & \bot 
				\end{tblr} \; \right \} \text{(Additives and their identities)} \\
			\left.
				\begin{tblr}{ colspec = {ccccc}, column{1,5} = {.5cm}, column{3} = {1.5cm}, column{2,4} = {0.2cm}}
				     	& \mid & \llbang{\phi}    & \mid & \llwn{\phi} 
			     	\end{tblr} \right. \text{(Exponentials)} \\
			\left.
				\begin{tblr}{ colspec = {ccccc}, column{1,5} = {.5cm}, column{3} = {1.5cm}, column{2,4} = {0.2cm}}
					& \mid & \llnot{\phi} & & 
			     	\end{tblr} \right. \text{(Negation)} \\
			\left.
				\begin{tblr}{ colspec = {ccccc}, column{1,5} = {.5cm}, column{3} = {1.5cm}, column{2,4} = {0.2cm}}
					& \mid & \alpha & & 
			     	\end{tblr} \right. \text{(Atom)} \\
		\end{tblr}
	\end{center}
	We will use $\phi$ to denote formulae and $\alpha$ to denote atoms.
	A sequent, written $\Delta$ or $\Gamma$, is a multiset of formulae.
\end{define}

\section{Normalization}\label{sec:normalization}
Since in linear logic negation is symmetric and an involution, it is usual to work only with formulae in negated normal form.
\begin{define}[Negated Normal Form -- NNF]
	\label{def:nnf}
	A formula is in NNF if all its linear implications ($\displaylolli$) are expanded to pars ($\displaypar$) using the tautology
	$$ a \lolli b \Leftrightarrow \llnot{a} \llpar b $$
	and negation is pushed down to atoms.
	Alternatively a formula in NNF is defined as follows:
	\begin{center}
		\begin{tblr}{ colspec = {ccccc}
			, cells = { mode = math } 
			% , vborder{1-4} = { leftspace = 0pt, rightspace = 0pt } 
	    		, rows = {abovesep=0pt, belowsep=0pt}
			}
			\phi & ::=  & \phi \llten \phi  & \mid & 1 \\
			     & \mid & \phi \llpar \phi  & \mid & \bot \\
			     & \mid & \phi \llplus \phi & \mid & 0 \\
			     & \mid & \phi \llwith \phi & \mid & \bot \\
			     & \mid & \llbang{\phi}     & \mid & \llwn{\phi} \\
			     & \mid & \llnot{\alpha}    & \mid & \alpha 
		\end{tblr}
	\end{center}
	A sequent is in NNF iff all its formulae are in NNF.
\end{define}
A generic formula is normalized by applying recursively the DeMorgan rules for linear logic, until NNF is reached.
Normalization of judgments instead takes a two-sided judgment of the form
$$ \Delta \vdash \Gamma $$
and transforms it into a one-sided judgment
$$ \vdash \Delta' $$
where the right side is composed of the normalization of $\Gamma$ and $\llnot{\Delta}$.

Normalization has some implementation-wise advantages, but for now it is only important because it shrinks the size of the complete calculus by roughly half, since we only have to deal with the right rules of the connectives.

\section{Focusing}
Focusing is a technique described by Andreoli in his seminal paper \cite{Focusing}.
In it he recognizes two alternating phases in a proof: a deterministic phase; and a non-deterministic phase, where several choices may be available.
These two phases are called respectively asynchronous and synchronous or focusing phase.
In the asynchronous phase the applicable rules are invertible, thus their order does not matter. 
For this reason these rules are called asynchronous rules, wheareas the non-invertible ones are called synchronous.
In this system to each formula is assigned a positive or neagative polarity based on its top-level connective:
\begin{itemize}
	\item connectives with a synchronous right rule are defined to have a positive polarity, these are:
		$$ \displayten, \displayplus, \displaybang, \displayone$$
	\item whereas connectives with an asynchronous right rule have a negative polarity, these are:
		$$ \displaypar, \displaywith, \displaywn, \displaytop, \displaybot$$
\end{itemize}
For atoms polarities may be assigned with some arbitrarily complex mechanisms.
We will follow \cite{LiangMiller} and simply assign atoms with a negative polarity and negated atoms with a positive one.
Since we work in one sided linear logic and connectives have only right rules, positive connectives may also be called synchronous, and negative connectives asynchronous.
\begin{define}\label{def:focusing predicates}
	Based on the definitions above we get the following predicates:
	\begin{itemize}
		\item ``$\phi \; \mathrm{atom}$'' is true whenever $\phi$ is an atom;
		\item ``$\phi \; \mathrm{asy}$'' is true whenever $\phi$ has as an asynchronous top-level connective;
		\item ``$\phi \; \mathrm{negative}$'' is true whenever $\phi$ is either an atom or an asynchronous connective, so
			$$ \phi \; \mathrm{negative} = \phi \; \mathrm{atom} \vee \phi \; \mathrm{asy} $$
	\end{itemize}
\end{define}

\begin{figure}[h!]
	\centering
	\input{figures/triadic-calculus}
	\caption{J.-M. Andreoli's triadic calculus.\label{fig:triadic}}
\end{figure}
Figure \ref{fig:triadic} shows the triadic calculus of \cite{Focusing}.
This has judgments with three members: a set of unrestricted formulae; a multiset of linear formulae put to the side; and either a single formula or another multiset of linear formulae.
An arrow pointing up symbolizes the asynchronous phase, and an arrow pointing down the focusing phase.
Phase switching happens using the decide rules (\derRule[A]{\displaydecide[1]}, \derRule[A]{\displaydecide[2]}), which non-deterministically choose a formula to focus on.

\section{Constraints}
During proof search the calculus generates constraints as the formulae get broken up.
The purpose of these is to ensure that if the constraints generated are satisfiable, then the formulae have been used linearly.
\begin{define}[Variables, expressions]
	\label{def:bool expr}
	A boolean variable is simply a symbol to which one can associate a value of true or false.
	A boolean expression, in our case, is just a conjunction of possibly negated boolean variables as 
	\begin{center}
	\begin{tblr}{ colspec = {cccccr}, cells = { mode = math } }
		x & ::=  & x_i &\mid& \overline{x_i} & \text{(Variable)}\\
		e & ::=  & x \wedge e    &\mid& x & \text{(Expression)} \\
	\end{tblr}
	\end{center}
	We will call $e$ such a conjunction and $x$ the single boolean variables.
	Given a boolean expression $e$ we write
	$$ \mathrm{vars}(e) = \{ x_i \mid x_i \in e \} $$
\end{define}
\begin{define}[New variables]
	\label{def:new}
	Sometimes we will write 
	$$ \new{x}, \new{X} $$
	These respectively mean that:
	\begin{itemize}
		\item the variable name $x$ has not yet occurred in any expression of the proof tree.
			One can think of a global counter which is incremented for each new variable.
		\item each variable name $x_i, x_j \in X$ has not yet occurred in the proof and each variable in $X$ is distinct:
			$$ \left (\forall x_i \in X \mid \new{x_i}\right ) \wedge \left (\forall x_i, x_j \in X \mid i \neq j \Rightarrow x_i \neq x_j \right ) $$
	\end{itemize}
\end{define}

\begin{define}[Annotated formula]
	\label{def:annotated}
	Given a formula $\phi$ defined as in Definition \ref{def:ll formula} and a boolean expression $e$ defined as in Definition \ref{def:bool expr}, an \textit{annotated formula} is simply a pair
	$$ \af{\phi}{e} $$
	that associates the formula to the expression.
	We denote 
	\begin{itemize}
		\item the operation of extracting the boolean expression associated to a given formula as
			$$ \expr{\af{\phi}{e}} = e $$
			and then extend this notation to sequents such that $ \expr{\Delta} $ is the set of all boolean expressions of $\Delta$
			$$ \expr{\Delta} = \{ \expr{\phi} \mid \phi \in \Delta \} $$
		\item the operation of extracting the set of variables appearing in the expression $e$ as
			$$\vars{\af{\phi}{e}} = \vars{e} $$
			and then extend this notation to sequents such that $\vars{\Delta}$ is the set of all the variables appearing in the boolean expressions of the annotated formulae of $\Delta$
			$$ \vars{\Delta} = \{ \vars{\expr{\phi}} \mid \phi \in \Delta \} $$
	\end{itemize}
\end{define}
It is important to note that only the topmost connective gets annotated, and not the sub-formulae.

The purpose of putting formulae and expressions together in the annotated formula is twofold:
\begin{itemize}
	\item the actions taken on the formula determine the constraints that will be generated, and these refer to the expressions associated to said formula;
	\item after the constraints are solved we can query the assignment of the variables and find out if the associated formula is used or not in a certain branch of a proof (see Definition \ref{def:evaluation}).
\end{itemize}
The above constraints may be only of two kinds: ``$\avail{e}$'' and ``$\used{e}$''.
\begin{define}[Constraints]
	\label{def:constraints}
	Given an annotated formula $\af{\phi}{e}$ as in Definition \ref{def:annotated}, a constraint $\lambda$ is either
	\begin{itemize}
		\item ``$\used{e}$'', which states that the formula $\phi$ gets consumed in this branch of the proof.
			This corresponds to saying the expression $e$ is true or
			$$ x_i \wedge \dots \wedge x_j \leftrightarrow \top $$
		\item ``$\avail{e}$'', which states that the formula $\phi$ does not get consumed in this branch of the proof, and thus is available to be used in another branch.
			This corresponds to saying the expression $e$ is not true or
			$$ x_i \wedge \dots \wedge x_j \leftrightarrow \bot $$
	\end{itemize}
	We then extend to sequents, such that
	\begin{align*}
		\used{\Delta} &= \{ \used{e} \mid e \in \mathrm{exp}(\Delta) \} \\
		\avail{\Delta} &= \{ \avail{e} \mid e \in \mathrm{exp}(\Delta) \}
	\end{align*}
	We denote
	\begin{itemize}
		\item $\mathrm{exp}(\lambda) = e$ where $\lambda$ is either ``$\used{e}$'' or ``$\avail{e}$'';
		\item $\mathrm{vars}(\lambda) = \mathrm{vars}(\mathrm{exp}(\lambda))$, and extend this notation to a set of constraints $\Lambda$
			$$ \mathrm{vars}(\Lambda) = \bigcup_{\lambda \in \Lambda} \mathrm{vars}(\lambda) $$
	\end{itemize}
\end{define}
\begin{define}[Assignment]
	\label{def:assignment}
	An assignment is a function
	$$ \mathV : \{ \dots, x_i, x_j, \dots \} \rightarrow \{ \top, \bot \} $$
	that associates the members of a set of variables to either true or false.
	Given a set of variables $X$, we say that the assignment $\mathV$ \textit{covers} $X$ if no variable in $X$ is left undefined under $\mathV$, or
	$$ X \subseteq \Dom{\mathV} $$
\end{define}
\begin{define}[Evaluation]
	\label{def:evaluation}
	Given a boolean expression $e$ and an assignment $\mathV$, such that $\mathV$ covers $\vars{e}$, we write
		$$ e[\mathV] = e[\dots, x_i \subst \mathV(x_i), x_j \subst \mathV(x_j), \dots] $$
	as the value of the expression $e$ substituting its variables using assignment $\mathV$.
	We extend this notation, such that
	\begin{itemize}
		\item given a constraint $\lambda$ and an assignment $\mathV$ that covers $\vars{\lambda}$
			$$ 
			\begin{cases} 
				\used{e}[\mathV] = \top & \text{if } e[\mathV] = \top \\
				\used{e}[\mathV] = \bot & \text{if } e[\mathV] = \bot \\
				\avail{e}[\mathV] = \top & \text{if } e[\mathV] = \bot \\
				\avail{e}[\mathV] = \bot & \text{if } e[\mathV] = \top \\
			\end{cases}
			$$
		\item given an annotated sequent $\Delta$ and an assignment $\mathV$ that covers $\vars{\Delta}$
			$$ \Delta[\mathV] = \{ \phi \mid \af{\phi}{e} \in \Delta , e[\mathV] = \top \} $$
	\end{itemize}
\end{define}
\begin{define}
	\label{def:coincidence}
	Given two assignments $\mathV'$ and $\mathV''$ and a set of variables $X$, we say that $\mathV'$ and $\mathV''$ coincide for $X$ -- written $\mathV' \sim_X \mathV''$ -- when they both map all the variables in $X$ to the same values, or
	$$ \mathV' \sim_X \mathV'' \Leftrightarrow \forall x \in X \mid \mathV'(x) = \mathV''(x) $$
\end{define}

A judgment is considered provable if its constraints are satisfiable.
\begin{define}[Satisfaiability]
	\label{def:sat}
	Given a set of constraints $\Lambda$ and an assignment function V that covers $\mathrm{vars}(\Lambda)$, we say that $\mathV$ satisfies $\Lambda$ iff every constraint in $\Lambda$ is true under $\mathV$, or
	$$ \sat{\Lambda}{\mathV} \Leftrightarrow \bigwedge_{\lambda \in \Lambda} \lambda[\mathV] = \top $$
\end{define}
We now expand the concept of triadic judgment from \cite{Focusing} (Figure \ref{fig:triadic}) by adding explicit constraints.
\begin{define}
	Given any judgment in our calculus, it can be in either two forms:
	\begin{itemize}
		\item focused or in the synchronous phase, written:
			$$\focus{\Psi}{\Delta}{\phi} \separator \constr{\Lambda}{\mathV}$$
		\item in the asynchronous phase, written:
			$$\async{\Psi}{\Delta}{\Gamma} \separator \constr{\Lambda}{\mathV}$$
	\end{itemize}
	Where 
	\begin{itemize}
		\item $\Psi$ is the same as the triadic calculus of \cite{Focusing}: a set of unrestricted non annotated formulae, or all formulae that can be freely discarded or duplicated.
		\item $\Delta$ and $\Gamma$ are multi-sets of linear (annotated) formulae, these are respectively the formulas ``put to the side'' and the formulae which are being ``worked on'' during a certain moment of the asynchronous phase;
		\item $\Lambda$ and V are the constraints and the assignment as defined in Definition \ref{def:sat}.
			By adding these members we make the flow of constraints through the proof tree explicit, leaving no ambiguity to where the constraints should be checked.
			This approach to constraints differs from the one in \cite{HarlandPym}, which prioritizes generality.
			The choice of letters is mainly a mnemonic one, constraints $\Lambda$ ``go-up'' the proof tree and solutions V ``come down'' from the leaves.
	\end{itemize}
\end{define}

\begin{define}[Splitting]
	\label{def:split}
	Given a sequent of annotated formulae $\Delta$ and a set of variables $X$ such that $|\Delta| = |X|$, we define the operation of splitting it as a function
	$$ \spliton{\Delta}{X} \mapsto (\Delta_L, \Delta_R) $$
	where
	\begin{align*}
		\Delta_L &= \{ \af{\phi_i}{x_i \wedge e_i} \mid i \in \{1, \dots, n\}\} \\
		\Delta_R &= \{ \af{\phi_i}{\varNot{x_i} \wedge e_i} \mid i \in \{1, \dots, n\}\}
	\end{align*}
	with $n$ the cardinality of $\Delta$, and $\phi_i$ (resp. $e_i$) the formula (resp. the expression) of the $i$-eth annotated formula in $\Delta$ using an arbitrary order.
	The same holds for $x_i$ and $X$.
	With a slight abuse of notation we will write $\Delta_L^X$ and $\Delta_R^X$ to mean respectively the left projection and the right projection of the pair $(\Delta_L, \Delta_R)$.
\end{define}
As a small example for clarity, given the sequent
\begin{align*}
	\Delta &= \af{a \llten b}{x_1}, \af{\llnot{c}}{x_2} \\
	X      &= \{ x_3, x_4 \} 
\end{align*}
this is split into
\begin{align*}
	\Delta_L^X &= \af{a \llten b}{x_3 \varAnd x_1}, \af{\llnot{c}}{x_4 \varAnd x_2} \\
	\Delta_R^X &= \af{a \llten b}{\varNot{x_3} \varAnd x_1}, \af{\llnot{c}}{\varNot{x_4} \varAnd x_2} 
\end{align*}

% TODO: expand
Figure \ref{fig:hp calculus} shows the calculus of \cite{HarlandPym} using our notation of explicit constraints.
In their paper they describe different ways of accumulating and checking constraints, roughly comparable to breadth first search, depth first search and a third intermediate method.
In Figure \ref{fig:calculus} and in Figure \ref{fig:hp calculus} we represent what the paper calls a ``lazy'' strategy, corresponding to a DFS.
\begin{figure}[h!]
	\centering
	\input{figures/hp-calculus}
	\caption{The one sided version of the calculus from \cite{HarlandPym} with explicit constraint propagation.\label{fig:hp calculus}}
\end{figure}
\begin{lemma}
	\label{lemma:cap}
	For all sequents $\Delta$ and assignments $\mathV$ which cover $\vars{\Delta} \cup X$:
	$$ \Delta_L^X[\mathV] \cap \Delta_R^X[\mathV] = \varnothing $$
\end{lemma}
\begin{proof}
	This is a simple consequence of the fact that if $\phi \in \Delta_L^X[\mathV]$ there is a annotated formula $\af{\phi}{e} \in \Delta$ such that 
	$$ e[\mathV] = \top $$
	Since $e$ is defined as a conjunction on boolean variables, all the variables in it must evaluate to true.
	It is straightforward to see that if the variable added by the split in $\Delta_L^X$ is $x_i$, and the corresponding one in $\Delta_R^X$ is $\varNot{x_i}$, then when $x_i$ is true in the assignment V, $\llnot{x_i}$ is false.
	Hence $\phi \not \in \Delta_R^X[\mathV]$.
	The same can be done to show that if $\phi \in \Delta_R^X[\mathV]$ then $\phi \not \in \Delta_L^X[\mathV]$.
\end{proof}
\begin{figure}[H]
	\begin{subfigure}{\textwidth}
		\centering
		\input{figures/asy-calculus}
		\caption{Asynchronous rules.\label{fig:asy calculus}}
	\end{subfigure}
\end{figure}
\begin{figure}[H]
	\ContinuedFloat
	\begin{subfigure}{\textwidth}
		\centering
		\input{figures/sync-calculus}
		\caption{Synchronous rules.\label{fig:sync calculus}}
	\end{subfigure}
\end{figure}
\begin{figure}[H]
	\ContinuedFloat
	\begin{subfigure}{\textwidth}
		\centering
		\input{figures/id-dec-calculus}
		\caption{Identity and decide rules.\label{fig:id dec calculus}}
	\end{subfigure}
	\caption{Focused constraint calculus for Linear Logic. \label{fig:calculus}}
\end{figure}

\begin{lemma}
	\label{lemma:cup}
	For all sequents $\Delta$ and assignments $\mathV$ which cover $\vars{\Delta} \cup X$,
	$$ \Delta[\mathV] = \Delta_L^X[\mathV] \cup \Delta_R^X[\mathV] $$
\end{lemma}
\begin{proof}
	The simpler side is $\Delta_L^X[\mathV] \cup \Delta_R^X[\mathV] \subseteq \Delta[\mathV]$, since it  holds by the definition of the split (Definition \ref{def:split}).
	For the other side, suppose there was a formula $\phi$ such that $\phi \in \Delta[\mathV]$ and $\phi \not \in \Delta_L^X[\mathV] \cup \Delta_R^X[\mathV]$.
	This means that for some variable $x_i$ 
	\begin{align*}
		\af{\phi}{e} \in \Delta &\Rightarrow e[\mathV] = \top \\
		\af{\phi}{x_i \varAnd e} \not \in \Delta_L^X &\Rightarrow x_i \varAnd e[\mathV] = \bot \\
		\af{\phi}{\varNot{x_i} \varAnd e} \not \in \Delta_R^X &\Rightarrow \varNot{x_i} \varAnd e[\mathV] = \bot
	\end{align*}
	But either $x_i$ or $\varNot{x_i}$ must be true in a certain assignment, thus either $x_i \varAnd e$ or $\varNot{x_i} \varAnd e$ must be true, contradicting the hypothesis.
\end{proof}

% TODO: write this better
\begin{teor}[Soundness]\label{thm:soundness}
	For any judgment in our constraint calculus:
	\begin{itemize}
		\item if $ \async{\Psi}{\Delta}{\Gamma} \separator \constr{\Lambda}{\mathV} $, then $ \vDash\; \llwn{\Psi}, \Delta, \Gamma $
		\item if $ \focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\Lambda}{\mathV} $, then $ \vDash\; \llwn{\Psi}, \Delta, \phi $
	\end{itemize}
\end{teor}
\begin{proof}
	As a soundness proof we give a translation from our constraint calculus to $A$: the triadic calculus of \cite{Focusing} defined in Figure \ref{fig:triadic}.
	By mutual induction on the structure of the given derivation we will show that
	\begin{align*}
		\async{\Psi}{\Delta}{\Gamma} \separator \constr{\Lambda}{\mathV} &\Rightarrow \async[A]{\Psi}{\Delta[\mathV]}{\Gamma[\mathV]} \\
		\focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\Lambda}{\mathV} &\Rightarrow \focus[A]{\Psi}{\Delta[\mathV]}{\phi}
	\end{align*}
	The four base cases are proved essentially in the same way, with the exception of top ($\displaytop$):
	\begin{itemize}
		\indCase{\displayid[1]} Given the rule \derRule{\displayid[1]}
			$$
			\AXC{$\isNegLit{\alpha}$}
			\AXC{$ \sat{\used{e_1}, \used{e_2}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[I_1]$}
			\BIC{$\focus{\Psi}{\af{\alpha}{e_1}}{\af{\llnot{\alpha}}{e_2}, \Delta} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			looking at the constraints we get that
			\begin{align*}
				\Delta[\mathV] &= \varnothing \tag{Because of $\avail{\Delta}$} \\
				\af{\alpha}{e_1}[\mathV] &= \alpha \tag{Because of $\used{e_1}$} \\
				\af{\llnot{\alpha}}{e_2}[\mathV] &= \llnot{\alpha} \tag{Because of $\used{e_2}$}
			\end{align*}
			so we can rewrite this as
			$$
			\AXC{$\isNegLit{\alpha}$}
			\LeftLabel{$[I_1]$}
			\UIC{$\focus[A]{\Psi}{\alpha}{\llnot{\alpha}}$}
			\DP
			$$
		\indCase{\displayid[2]} Given the rule \derRule{\displayid[2]}
			$$
			\AXC{$\isNegLit{\alpha}$}
			\AXC{$ \sat{\used{e}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[I_2]$}
			\BIC{$\focus{\Psi, \alpha}{\Delta}{\af{\llnot{\alpha}}{e}} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			proceeding as above we get
			\begin{align*}
				\Delta[\mathV] &= \varnothing \\
				\af{\llnot{\alpha}}{e}[\mathV] &= \llnot{\alpha}
			\end{align*}
			thus
			$$
			\AXC{$\isNegLit{\alpha}$}
			\LeftLabel{$[I_2]$}
			\UIC{$\focus[A]{\alpha, \Psi}{.}{\llnot{\alpha}}$}
			\DP
			$$
		\indCase{\displayone} Given the rule \derRule{\displayone}
			$$
			\AXC{$\sat{ \used{e}, \avail{\Delta}, \Lambda}{\mathV}$}
			\LeftLabel{$[1]$}
			\UIC{$\focus{\Psi}{\Delta}{\af{\llone}{e}} \separator \constr{\Lambda}{\mathV}$}
			\DP
			$$
			proceeding as above we get
			\begin{align*}
				\Delta[\mathV] &= \varnothing \\
				\af{\llone}{e}[\mathV] &= \llone
			\end{align*}
			thus
			$$
			\AXC{}
			\LeftLabel{$[\llone]$}
			\UIC{$\focus[A]{\Psi}{.}{\llone}$}
			\DP
			$$
		\indCase{\displaytop} Given the rule \derRule{\displaytop} we have that
			$$ 
			\AXC{}
			\UIC{$\async{\Psi}{\Delta}{\af{\top}{-}, \Gamma} \separator \constr{-}{-}$} 
			\DP
			$$
			Thus we can choose whichever assignment $\mathV$ that covers $\vars{\Delta, \Gamma}$, and obtain
			$$ 
			\AXC{}
			\UIC{$\async[A]{\Psi}{\Delta[\mathV]}{\top, \Gamma[\mathV]}$}
			\DP
			$$
	\end{itemize}
	The induction then follows like this:
	\begin{itemize}
		\indCase{\displayten} Given the rule \derRule{\displayten}, we apply the inductive hypothesis, and from the premises 
			\begin{align*}
				&\focus{\Psi}{\Delta_L^X}{\af{\phi_1}{e}} \separator \constr{\used{e}, \Lambda}{\mathV'} \\
				&\focus{\Psi}{\Delta_R^X}{\af{\phi_2}{e}} \separator \constr{\used{e}, \Lambda}{\mathV''} 
			\end{align*}
			we extract the triadic proofs
			\begin{align*}
				&\focus[A]{\Psi}{\Delta_L^X[\mathV']}{\phi_1} \\
				&\focus[A]{\Psi}{\Delta_R^X[\mathV'']}{\phi_2} 
			\end{align*}
			Since $\mathV' \sim_{\vars{\Delta}, X} \mathV''$, this can be rewritten as
			\begin{align*}
				&\focus[A]{\Psi}{\Delta_L^X[\mathV'']}{\phi_1} \\
				&\focus[A]{\Psi}{\Delta_R^X[\mathV'']}{\phi_2} 
			\end{align*}
			Furthermore because of Lemma \ref{lemma:cap} we have that $\Delta_L^X[\mathV'']$ and $\Delta_R^X[\mathV'']$ are disjoint, so the contexts of the two branches are separated.
			Hence we can apply \derRule[A]{\displayten}, and obtain
			$$ \focus[A]{\Psi}{\Delta_L^X[\mathV''], \Delta_R^X[\mathV'']}{\phi_1 \llten \phi_2} $$
			which, because of Lemma \ref{lemma:cup}, correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llten \phi_2}{e}} \separator \constr{\Lambda}{\mathV''} $$
			should be mapped to.
		\indCase{\displaywith} Given the rule \derRule{\displaywith}, we apply the inductive hypothesis, and from the premises
			\begin{align*}
				&\async{\Psi}{\Delta}{\af{\phi_1}{e}, \Gamma} \separator \constr{\used{e}, \Lambda}{\mathV'} \\
				&\async{\Psi}{\Delta}{\af{\phi_2}{e}, \Gamma} \separator \constr{\used{e}, \Lambda}{\mathV''} 
			\end{align*}
			we extract the triadic proofs
			\begin{align*}
			 	&\async{\Psi}{\Delta[\mathV']}{\phi_1, \Gamma[\mathV']}\\
			 	&\async{\Psi}{\Delta[\mathV'']}{\phi_2, \Gamma[\mathV'']} 
			\end{align*}
			Now, since $\mathV' \sim_{\vars{\Delta, \Gamma}} \mathV''$, we can write
			\begin{align*}
				&\async{\Psi}{\Delta[\mathV'']}{\phi_1, \Gamma[\mathV'']} \\
				&\async{\Psi}{\Delta[\mathV'']}{\phi_2, \Gamma[\mathV'']} 
			\end{align*}
			this way we can apply \derRule[A]{\displaywith} to obtain
			$$ \async[A]{\Psi}{\Delta[\mathV'']}{\phi_1 \llwith \phi_2, \Gamma[\mathV'']} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\phi_1 \llwith \phi_2}{e}, \Gamma} \separator \constr{\Lambda}{\mathV''} $$
			should be mapped to.
		\indCase{\displaypar} Given the rule \derRule{\displaypar}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi_1}{e}, \af{\phi_2}{e}, \Gamma} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof 
			$$\async[A]{\Psi}{\Delta[\mathV]}{\phi_1, \phi_2, \Gamma[\mathV]} $$
			Then we apply \derRule[A]{\displaypar}, and obtain
			$$\async[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llpar \phi_2, \Gamma[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\phi_1 \llpar \phi_2}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
			% Furthermore, since the assignment $\mathV$ is the same between premise and conclusion, then $\Delta[\mathV]$ and $\Gamma[\mathV]$ stay the same, and since the assignment respects the constraint $\used{e}$, then $\phi_1 \llpar \phi_2$ must be present.
		\indCase{\displayplus[L]} Given the rule \derRule{\displayplus[L]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi_1}{e}} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1} $$
			Then we apply \derRule[A]{\displayplus[L]}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llplus \phi_2} $$
			which correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be.
		\indCase{\displayplus[R]} Given the rule \derRule{\displayplus[R]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi_2}{e}} \separator \constr{\used{e}, \Lambda}{\mathV}$$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_2} $$
			Then we apply \derRule[A]{\displayplus[R]}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi_1 \llplus \phi_2} $$
			which correctly corresponds to what the conclusion
			$$ \focus{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be.
		\indCase{\displaybang} Given the rule \derRule{\displaybang}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \avail{\Delta}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{.}{\phi} $$
			since $\Delta[\mathV] = \varnothing$ under assignment $\mathV$.
			We then apply \derRule[A]{\displaybang} and obtain
			$$ \focus[A]{\Psi}{.}{\;\llbang{\phi}} $$
			which correctly corresponds to what the conclusion 
			$$ \focus{\Psi}{\Delta}{\af{\llbang{\phi}}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to
		\indCase{\displaywn} Given the rule \derRule{\displaywn}, we apply the inductive hypothesis, and from the premise
			$$ \async{\phi, \Psi}{\Delta}{\Gamma} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\phi, \Psi}{\Delta[\mathV]}{\Gamma[\mathV]}$$
			Then we apply \derRule[A]{\displaywn} and obtain
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\,\llwn{\phi}, \Gamma[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\llwn{\phi}}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaybot} Given the rule \derRule{\displaybot}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\Gamma} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\Gamma[\mathV]} $$
			Then we apply \derRule[A]{\displaybot} and obtain
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\llbot, \Gamma[\mathV]} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\Delta}{\af{\llbot}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaydecide[1]} Given the rule \derRule{\displaydecide[1]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaydecide[1]} and obtain
			$$ \async[A]{\Psi}{\phi, \Delta[\mathV]}{.} $$
			which correctly corresponds to what the conclusion
			$$ \async{\Psi}{\af{\phi}{e}, \Delta}{.} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaydecide[2]} Given the rule \derRule{\displaydecide[2]}, we apply the inductive hypothesis, and from the premise
			$$ \focus{\Psi}{\Delta}{\af{\phi}{x}} \separator \constr{\used{x}, \Lambda}{\mathV} $$
			where $\new{x}$, we extract the triadic proof
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaydecide[2]} and obtain
			$$ \async[A]{\phi, \Psi}{ \Delta[\mathV]}{.} $$
			which correctly corresponds to what the conclusion
			$$ \async{\phi, \Psi}{\Delta}{.} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
		\indCase{\displaytodelta} Given the rule \derRule{\displaytodelta}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\af{\phi}{e}, \Delta}{\Gamma} \separator \constr{\Lambda}{\mathV} $$
			when we extract the triadic proof, two cases arise:
			\begin{itemize}
				\item $\phi$ disappears under assignment $\mathV$, and we get
					$$ \async[A]{\Psi}{\Delta[\mathV]}{\Gamma[\mathV]} $$
					which correctly corresponds to what the conclusion
					$$ \async{\Psi}{\Delta}{\af{\phi}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
					should be mapped to, since if $\phi$ disappears from the premise, it will also disappear from the conclusion.
				\item $\phi$ remains under assignment $\mathV$, and we get
					$$ \async[A]{\Psi}{\phi, \Delta[\mathV]}{\Gamma[\mathV]} $$
					Then we apply \derRule[A]{\displaytodelta} and obtain
					$$ \async[A]{\Psi}{\Delta[\mathV]}{\phi, \Gamma[\mathV]} $$
					which correctly corresponds to what the conclusion
					$$ \async{\Psi}{\Delta}{\af{\phi}{e}, \Gamma} \separator \constr{\Lambda}{\mathV} $$
					should be mapped to.
			\end{itemize}
		\indCase{\displaytoasy} Given the rule \derRule{\displaytoasy}, we apply the inductive hypothesis, and from the premise
			$$ \async{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\used{e}, \Lambda}{\mathV} $$
			we extract the triadic proof
			$$ \async[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			Then we apply \derRule[A]{\displaytoasy}, and obtain
			$$ \focus[A]{\Psi}{\Delta[\mathV]}{\phi} $$
			which correctly corresponds to what the conclusion 
			$$ \focus{\Psi}{\Delta}{\af{\phi}{e}} \separator \constr{\Lambda}{\mathV} $$
			should be mapped to.
	\end{itemize}
\end{proof}
In Appendix \ref{appendix:example} we show an example derivation using this calculus.


