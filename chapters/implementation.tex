\chapter{Implementation}\label{chapter:implementation}
We now describe the main implementation details of the prover.
When explaining the code we will use some common names for variables, these are
\begin{itemize}
	\item \texttt{A} is a set of unrestricted atoms, which purpose will be explained in Section \ref{sec:identity};
	\item \texttt{U} is a set of unrestricted formulae;
	\item \texttt{F}, \texttt{F1}, ..., are formulae, and \texttt{Fs} and \texttt{D} are a lists of them;
	\item \texttt{S} is the queue of currently usable unrestricted formulae, which purpose will be explained in Section \ref{sec:decide};
	\item \texttt{In} is a list of constraints.
\end{itemize}

\section{Why Prolog}
Prolog as a language and as an environment has been historically tied to automated theorem proving for its ability to express its algorithms naturally.
% In particular we chose SWI-Prolog because it offers a comprehensive and mature free Prolog environment.
One particularly convenient characteristic of Prolog is its automatic management of backtracking, in most other languages we would have had to use exceptions to walk down the stack, or a queue of unfinished computations, which would have made the code much less readable.

Most Prolog implementations also support CLP or constraint logic programming.
This is implemented through a series of libraries, these libraries allow to express constraint in the body of the clauses referencing some attribute of the variables;
in our case we will use \CLPB \cite{clpb}, which provides tools to deal with boolean constraints.
By boolean constraint in this context we mean an expression made of Prolog variables and the constants 1 and 0, respectively true and false.
The allowed operators in these expressions are the usual ones one would expect; in our case we will use exclusively conjunction, negation and equality, respectively
\begin{minted}{prolog}
X * Y.
~ X.
X =:= Y.
\end{minted}

Usually constraints will be accumulated in a list, for this reason \CLPB{} provides the functor \texttt{*(L)} to express the conjunction of its members.
A constraint may be checked using the predicate \texttt{sat/1}, which succeeds iff it is satisfiable.
Since we only deal with conjunctions we define the helper predicate \texttt{check/1}
\begin{minted}{prolog}
check(L) :-
  sat(*(L)).
\end{minted}
The predicate \texttt{sat/1} may work a bit unexpectedly, to clarify its possible outcomes we give a little example:
\begin{minted}{prolog}
?- check([X * Y =:= 1, X =:= 0]).
false.
?- check([X * Y =:= 0]).
sat(1#X*Y).
?- check([X * Y =:= 0, X =:= 1]).
X = 1,
Y = 0.
\end{minted}
Here we can see three different cases:
\begin{itemize}
	\item the first case is unsatisfiable, so the predicate fails;
	\item the second case is not instantiated enough and so the constraint just gets rewritten;
	\item the third case shows how one constraint (\texttt{X =:= 1}) can force a variable to be unified to a value.
\end{itemize}
We use the library with the flag \texttt{clpb\_monotonic} set to \texttt{true}.
This makes the algorithm many orders of magnitude faster but mandates that all variables be wrapped by the functor \texttt{v/1}.
This small explanation sums up the extent of the library we use in our prover.

\section{Formula transformations}
Before beginning the proof a sequent passes through a number of transformations.
These transformations both preprocess the sequent to a more convenient form, and also add information about the sub-formulae.

As a first transformation the sequent gets normalized into a sequent in negated normal form (NNF) as defined in Definition \ref{def:nnf}.
Normalization is a common technique -- used in all the provers we compare with.
The process has just two steps
\begin{enumerate}
	\item the left sequent is negated and appended to the right sequent, implemented by the predicate \texttt{negate\_premises/3};
	\item the predicate \texttt{nnf/2}, which encodes the DeMorgan rules, is mapped recursively over the new sequent
\end{enumerate}
This is possible since classic linear logic is symmetric and negation is an involution.

The purpose of normalization is to cut away a great deal of possible rules applicable to the sequent, sacrificing some of the structure of the sequent.
In fact the number of rules one needs to implement, and thus the number of different choices the prover may have in a certain moment, is more than halved as said in Section \ref{sec:normalization}.

As a second transformation, to each formula we assign its why-not height, a measure borrowed from APLL's implementation.
\begin{define}[Why-not height]
	\label{def:why-not height}
	Why-not height is the maximum number of nested ``why-not''s in a formula, or
	$$ \mathrm{wnh}(\phi) = 
	\begin{cases}	
		0 & \text{if }\phi \in \{\llbot, \lltop, \llone, \llzero\} \\
		\max(\mathrm{wnh}(\phi_1), \mathrm{wnh}(\phi_2)) & \text{if } \phi \in \{ \phi_1 \llten \phi_2, \phi_1 \llpar \phi_2, \phi_1 \llplus \phi_2, \phi_1 \llwith \phi_2 \} \\
		\mathrm{wnh}(\phi_1) & \text{if }\phi \in \{ \llnot{\phi_1}, \llbang{\phi_1}\} \\
		1 + \mathrm{wnh}(\phi_1) & \text{if }\phi \in \{ \llwn{\phi_1} \} 
	\end{cases}
	$$
\end{define}
The purpose of this attribute is to guide the prover to the reasonably simpler choice -- the one with the least nested exponentials -- at different times during proof search.
This happens in three ways: 
\begin{itemize}
	\item When a rule generates two branches ($\llten$, $\llwith$), the branch associated to the formula with the least why-not height is tried first.
		This will presumably make the prover choose the simpler branch of the two, making it so we can fail early if the branch turns out to be false.
		An example of this is Section \ref{sec:asy phase}.
	\item In the case of plus ($\llplus$), the branch branch associated to the least why-not height is tried first; this makes it so we can continue if the branch turns out to be true, ignoring the other harder branch.
	\item During the $D_2$ rule, the exponentials are tried in order of ascending why-not height.
		This process is further explained in Section \ref{sec:decide}.
\end{itemize}

After this transformation formulae are attribute trees with at each node the why-not.
For example the formula \texttt{*(a, ?(b))} becomes \texttt{*(a-0, ?(b-0)-1)-1}, or
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\node (root1) {\texttt{*}}
			child { node {\texttt{a}} }
			child { 
				node {\texttt{?}}
				child { node {\texttt{b}} }
			};

		\node (arrow) at ([xshift=1.25cm] root1-2.west) {$\overset{\mathrm{wnh}}{\Rightarrow}$};
		\node (root2) at ([xshift=4cm] root1.west) {\texttt{*-1}}
			child { node {\texttt{a-0}} }
			child { 
				node {\texttt{?-1}}
				child { node {\texttt{b-0}} }
			};

	\end{tikzpicture}
\end{figure}

As a third and final transformation, each formula gets annotated as in Definition \ref{def:annotated}:
given a sequent $\Delta$ we obtain
$$ \hat{\Delta} = \{ \af{\phi}{x} \mid \new{x}, \phi \in \Delta \} $$
To be clear, a variable is only assigned to the ``top-level'' formula, and sub-formulae are left unchanged.

The process of annotation is implemented by the predicate \texttt{annotate/3}.
This also returns the initial constraints, with set each variable to ``used''.
Thus stating that in the proof each and every formula must be used.
% continuo?

\section{Helper predicates}\label{sec:helper}
We now define some helper predicates to work with the constraints.
What we defined as $\avail{\Delta}$ in Definition \ref{def:constraints} corresponds to the predicate \texttt{set\_to\_zero/2}.

The other helper predicate implements the split function defined in Definition \ref{def:split}.
\begin{minted}{prolog}
%! split_ctx(+[AFs], -[AFs], -[AFs], -[Cns], -[Cns]) is det.
split_ctx(Afs, Pos, Neg, PCns, NCns) :-
  maplist([ af(F, N, E)
          , af(F, VarPos, Y)
          , af(F, VarNeg, Z)
          , v(Y) =:= v(X) * v(E)
          , v(Z) =:= (~ v(X)) * v(E)
          ]>>(
  	     gensym(x, V),
  	     atomic_list_concat([N, V], '.', VarPos),
  	     atomic_list_concat([N, V], '.~', VarNeg)
  ), Afs, Pos, Neg, PCns, NCns).
\end{minted}
It is again a map over the list of formulae, that generates the new formulae and the constraints accordingly.
Three new Prolog variables are introduced: \texttt{X}, \texttt{Y} and \texttt{Z}.
\texttt{X} is the new variable, the annotated formulae refer to the variable \texttt{Y} and \texttt{Z}, and constraints are added so that
\begin{align*}
	y &= x \varAnd e \\
	y &= \varNot{x} \varAnd e
\end{align*}
Compare this to the original definition of Definition \ref{def:split}, one can see that the two are basically identical other than the fact that here the name of the variable (the atom) and its value are treated separately.
In the implementation the concept of variable is split in two: the name of the variable -- represented by a Prolog atom, and the value of the variable -- represented by a Prolog variable.
This is needed since, after checking the constraints, the SAT-solver unifies the variable to its value if it finds a satisfiable solution; the purpose of the atom is to associate the variable value to its name if the final proof.

\section{Focusing}\label{sec:focusing impl}
\subsection{Asynchronous and focusing phase}\label{sec:asy phase}
During the asynchronous phase we have a list of formulae which are being worked on and a list of formulae which are put to the side; with the former being called \texttt{Fs} and the latter \texttt{D}.
At each step we analyze the first element of the list \texttt{Fs}, and we keep decomposing the members of the list until we cannot anymore.
This process can be seen for example in the predicate for with ($\llwith$)
\begin{minted}[linenos]{prolog}
async(A, U, D, [F|Fs], S, M, In) :-
  F = af(((F1-H1) & (F2-H2))-_), N, E), !,
  ( H2 > H1	
  -> async(A, U, D, [af((F1-H1), N, E)|Fs], S, M, [v(E) =:= 1|In]), 
     async(A, U, D, [af((F2-H2), N, E)|Fs], S, M, [v(E) =:= 1|In]) 
  ;  async(A, U, D, [af((F2-H2), N, E)|Fs], S, M, [v(E) =:= 1|In]),
     async(A, U, D, [af((F1-H1), N, E)|Fs], S, M, [v(E) =:= 1|In])
  ).
\end{minted}
Here we can see both the choice being made based on the why-not height of the two sub-formulae (Definition \ref{def:why-not height}), and how the with is broken down.
Compare this with the $\llwith$ rule in Figure \ref{fig:calculus}.
The cut at line 2 is essential: it signifies that when an asynchronous connective is encountered the only thing to do is to break it apart.

If a formula cannot be further be broken apart -- i.e. it is either an atom, a negated atom, or it has a top-level synchronous connective -- then it is put to the side in \texttt{D}.
This can be seen in the rule \texttt{to\_delta} which implements the rule $R\!\Uparrow$
\begin{minted}{prolog}
async(A, U, D, [F|Fs], S, M, In, _) :-
  F = af((F1-_), _, _),
  \+ (is_asy(F1)), !,
  async(A, U, [F|D], Fs, S, M, In, _).
\end{minted}
This process goes as long as \texttt{Fs} has still formulae inside.

When \texttt{Fs} is empty the phase switches, and the focusing process begins: we choose a formula -- called \texttt{decide} -- from either \texttt{D} or \texttt{U} and we break it down until either an asynchronous connective or a negated atom is left. 
This is represented by the rules $D_1$ and $D_2$ that will be discussed further ahead in Section \ref{sec:decide}.

\subsection{Identity rules}\label{sec:identity}
This process of alternating asynchronous and synchronous phases in classic focusing goes on until only a positive literal (in our case a negated atom) in \texttt{Fs} is left; and the corresponding negative literal (in our case just an atom) in either \texttt{U} or \texttt{D}.
When this happens the axioms -- rules $I_1$ or $I_2$ -- are applied to close the branch of the proofs.
In our case when we are focusing and we have a positive literal in \texttt{Fs}, we check if the corresponding negative literal exists in \texttt{D}.
If this is true, then the variables of all the other formulae in \texttt{D} are set to zero using the predicate \texttt{set\_to\_zero/2} defined in Section \ref{sec:helper}, and the constraints are checked.
This is encoded in the clause 
\begin{minted}{prolog}
focus(A, U, D, F, _, _, In) :-
  F = af(((~ T)-_), _, E1),
  is_term(T),
  select(af((T-_), _, E2), D, D1),
  set_to_zero(D1, Dz),
  append([v(E1) =:= 1, v(E2) =:= 1|Dz], In, Cns),
  check(Cns).
\end{minted}
A slightly different thing happens if instead a correspondence is found in \texttt{A} instead of \texttt{D}.
Here \texttt{A} is a special set containing just unrestricted atoms.
This is a small modification to APLL's approach based on the fact that once negative literals are put in a sequent they can never leave it, and is due to the fact that since \texttt{U} may be sorted many times, we try to keep the number of formulae in it small.

Some care is to be given to explaining how the constraints propagate.
In fact, in contrast to Figure \ref{fig:calculus} the implementation does not have explicit propagation of the solution of the constraints.
This is because Prolog's unification implicitly propagates a solution from one branch to another.

\subsection{Decide rules}\label{sec:decide}
For the decide rules, particularly for $D_2$, we use a modified version of APLL's algorithm defined in Section \ref{sec:apll}.
The method consists of not using directly the set $\Psi$ in the $D_2$ rule, but instead a queue of ordered unrestricted formulae which can be refilled only a certain number of times per-branch.
This can be seen in the definition of the rule \texttt{decide\_2} for the \texttt{async/8} predicate
\begin{minted}{prolog}
async(A, U, D, [], [H|T], M, In) :-
  \+ U = [],
  gensym(x, X),
  focus(A, U, D, af(H, X, E), T, M, [v(E) =:= 1|In]).
async(A, U, D, [], [_|T], M, In) :-
  \+ U = [],
  async(A, U, D, [], T, M, In).
async(A, U, D, [], [], M, In) :-
  \+ U = [],
  refill(U, M, S, M1),
  early_stop(A, U, D, S, M1, In).
\end{minted}
Here the fifth argument is the queue and the sixth is the bound.
Two cases arise:
\begin{itemize}
	\item if \texttt{S = []} and \texttt{M > 0} then the sequent of unrestricted formulae \texttt{U} is taken and it is sorted based on why-not height.	% cambio persona
		This can be seen in the predicate \texttt{refill/4}
		\begin{minted}{prolog}
refill(U, M, S, M1) :-
  \+ M = 0,
  \+ U = [], 
  sort(2, @=<, U, S), 
  M1 is M - 1.
		\end{minted}
		Line 4 is a sort in increasing order on the second attribute (the why-not height), keeping duplicates.
		This new list of unrestricted formulae becomes the new \texttt{S} and \texttt{M} is decreased.
		Otherwise if if \texttt{M} is 0 (line 2) the branch fails.
	\item if \texttt{S} is not empty, then the first formula in the queue is extracted and added to the working set.	% sistemo
		If the branch fails the formula gets discarded and the next one in the queue is tried.	% sistemo
\end{itemize}
In particular, if the queue \texttt{S} is refilled, we do not directly call \texttt{async/8}, but instead call the predicate \texttt{early\_stop/7} (line 11), defined as:
\begin{minted}{prolog}
early_stop(_, _, _, [], _, _) :-
  false.
early_stop(A, U, D, [H|T], M, In) :-
  gensym(x, X),
  focus(A, U, D, af(H, X, E), T, M, [v(E) =:= 1|In]).
early_stop(A, U, D, [_|T], M, In) :-
  early_stop(A, U, D, T, M, In).
\end{minted}
This is due the simple fact that if the branch was not provable and we instead called directly \texttt{async/8} at line 11, we would try to refill the branch \texttt{M} times.
What \texttt{early\_stop/7} does is fail if the queue has just been refilled and it turns out the branch was not provable.

All the rules $D_1$, $I_1$ are defined before the unrestricted counterparts, so that they are tried first.

\section{Building the tree}
In the listings above we omitted one parameter from the calls to \texttt{focus/8} and \texttt{async/8}, which purpose is to build the proof tree.
For example in
\begin{minted}{prolog}
async(A, U, D, [F|Fs], S, M, In, node(par, A, U, D, [F|Fs], [Tree])) :- 
  F = af(((F1 / F2)-_), N, E), !,
  Fs1 = [af(F1, N, E), af(F2, N, E)|Fs],
  async(A, U, D, Fs1, S, M, [v(E) =:= 1|In], Tree).
\end{minted}
we can see clearly the structure of one node of the tree: a label, the context and an -- optionally empty -- list of sub-trees.
A leaf is just a node with an empty list of sub-trees.

This tree can be used in the end to reconstruct the actual proof tree by visiting it and -- for each formula of each node -- querying whether its variable is set to one, deleting it otherwise (this process is the same used in the proof for Theorem \ref{thm:soundness}).
A classic proof tree without the focusing infrastructure may be built by removing all the nodes regarding the phases (i.e. $R\!\Downarrow$ $R\!\Uparrow$ and decide rules) and by appending the sequents together as explained in \cite{Focusing}. 
A more sophisticated algorithm may even cancel out unwanted unrestricted formulae, that otherwise remain lingering in the sequent.

