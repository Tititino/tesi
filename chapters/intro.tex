\chapter{Intro}\label{chapter:intro}
In 2001 Pym and Harland publish a paper \cite{HarlandPym} where they propose a new way to tackle the problem of splitting sequents during linear logic proof search using boolean expressions.
These are associated to each formula, and constraints on the expressions are used to enforce linearity.
This way the complexity shifts from choosing the right set of formulas to prove a certain branch, to solving for boolean assignment -- a problem for which there are much more sophisticated algorithms.

In \S\ref{chapter:calculus} we define a focused and one-sided version to the calculus described in \cite{HarlandPym}, and we give a proof of its soundness consisting of a forgetful functor to the triadic calculus of \cite{Focusing}.
In \S\ref{chapter:implementation} we discuss a Prolog implementation of the calculus of the previous chapter.
In \S\ref{chapter:state of the art} we quickly describe the main implementation details of two other backwards provers for full linear logic: llprover and APLL.
Finally in \S\ref{chapter:testing} we describe the framework built to test and compare our prover with the others from the previous section, and then we show the results of these benchmarks.

% \section{Sequent calculus}
% We will often talk about sequents, a seuquent of the form
% $$ \Delta_1, \dots, \Delta_n \vdash \Gamma_1, \dots, \Gamma_m $$
% is another way of writing
% $$ \Delta_1 \wedge \dots \wedge \Delta_n \Rightarrow \Gamma_1 \vee \dots \vee \Gamma_m $$
% In sequent calculus we define some rules to manipulate these sequents, these rules are for example 
% $$
% \AXC{$\Gamma, \phi \vdash \Delta$}
% \AXC{$\Gamma, \psi \vdash \Delta$}
% \BIC{$\Gamma, \phi \vee \psi \vdash \Delta$}
% \DP
% $$
% means that if $\Gamma, \phi \vdash \Delta$ and $\Gamma, \psi \vdash \Delta$ hold, then $\Gamma, \phi \vee \psi \vdash \Delta$ holds.
% This for example is the classic rule for $\vee$.
% 
% When trying to build a proof bottom up, we utilize these rules inversed to try to arrive at what are called axioms or leafs, rules with no premises.
% $$
% \AXC{}
% \UIC{$\phi \vdash \phi$}
% \DP
% $$
% 
% Gentzen introduced the sequent calculus LK for classical logic, this had -- other that the usual rules -- three so called structural rules.
% These rules were used to manipulate the sequent itself, and are
% \begin{itemize}
% 	\item weakening: we can always ``weaken'' the sequent by adding a proposition without changing its truth,
% 		$$
% 		\AXC{$\Gamma \vdash \Delta$}
% 		\UIC{$\Gamma, \phi \vdash \Delta$}
% 		\DP
% 		$$
% 	\item contraction: we can always ``contract'' two copies of the same proposition into one without changing the truth of the sequent,
% 		$$
% 		\AXC{$\Gamma, \phi, \phi \vdash \Delta$}
% 		\UIC{$\Gamma, \phi \vdash \Delta$}
% 		\DP
% 		$$
% 	\item exchange: we can change position of two propositions in a sequent freely without changing its truth
% 		$$
% 		\AXC{$\Gamma, \phi, \psi \vdash \Delta$}
% 		\UIC{$\Gamma, \psi, \phi \vdash \Delta$}
% 		\DP
% 		$$
% \end{itemize}
% and their symmetric right rules.
% These structural rules will be important in the next section where we will introduce linear logic.

\section{Linear logic}
Linear logic is a logic proposed by Jean-Yves Girard in his seminal paper of 1987 \cite{LinearLogic}.
The distinctive trait of this logic is that its formulae cannot be copied (weakening) or discarded (contraction), but instead they are consumed.
Under these rules a certain sequent it true if and only if all of its formulae get consumed exactly once.
For this reason this logic is sometimes called a logic of resources, in the same way classical logic is a logic of truths and intuitionistic logic is a logic of proofs.
% Questo particolare utilizzo delle formule permette di avere una logica che mantiene la simmetria delle logica classica, e il costruttivismo delle logica intuizionista.

In linear logic each connective of classical logic is doubled.
To better see this let's analyze classic conjunction, this can be defined as 
$$
\begin{array}{cc}
\AXC{$\Delta \vdash \phi_2, \Gamma$}
\AXC{$\Delta \vdash \phi_1, \Gamma$}
\BIC{$\Delta \vdash \phi_1 \wedge \phi_2, \Gamma$}
\DP
	&
\AXC{$\Delta' \vdash \phi_1, \Gamma'$}
\AXC{$\Delta'' \vdash \phi_2, \Gamma''$}
\BIC{$\Delta', \Delta'' \vdash \phi_1 \wedge \phi_2, \Gamma', \Gamma''$}
\DP
\end{array}
$$
These two rule are equivalent only if the use of weakening and contraction is permitted.
For this is the reason in linear logic all connectives have two versions: an additive one -- where the two branches keep the same context, and a multiplicative one -- where the context gets portioned between the two branches.
Obviously the constants $\top$ and $\bot$ also have two versions.
We have that
\begin{center}
	\begin{tblr}{ccc}
		\hline
		Class. & Add. & Mult. \\
		\hline
		\hline
		$\wedge$ & $\llwith$ & $\llten$ \\
		$\vee$ & $\llplus$ & $\llpar$ \\
		$\top$ & $\top$ & $1$ \\
		$\bot$ & $0$ & $\bot$ \\
	\end{tblr}
\end{center}
It is the multiplicative side which brings the most complexity.
In top-down proof search, the action of partitioning the context -- called splitting -- implies an exponential number of attempts to find which subset of the multi-set is right for a certain branch.

Linear logic defined as of right now, albeit having the added complexity of splitting, is nonetheless decidable: since formulae are finite and they cannot be copied, it is possible to explore all the possibilities.
To make linear logic as strong as classical logic two new connectives are added: $\llbang{\phi}$ and $\llwn{\phi}$ -- called respectively of-course and why-not.
% Linear logic defined as of right now is decidable, since formulas cannot grow in size we can explore each possibility.
% To be as strong as classical logic, there are two so called exponentials: bang or $!\phi$ and why not or $?\phi$.
% These have the purpose of localizing the uses of weakening and contraction:
These are called exponentials and their purpose is to localize uses of contraction and weakening.
For example, formulas marked with $!$ can be used any number of times. %, so the intuistic implication $a \rightarrow b$ is translated as $!(a \lolli b)$, and transitions in a petri net are represented by $!(resources_1 \lolli resources_2)$.

% Linear logic can be used to ensure that objects are used exactly once, thus allowing the system to safely deallocate an object after its use.
% The Haskell's compiler GHC has an experimental extensions to permit signatures with linear types.

% \subsection{Linear logic in practice}
% utilizzi logica lineare
%   * pi-calcolo
%   * risorse
%   * linear-haskell?
% A good example of linear logic may be chemical reactions % https://www.cs.cmu.edu/~crary/317-f22/lectures/20-linear.pdf
% Here we can see a reaction as an implication, if we have the reagents we can consume them to obtain the products.

% Petri nets can be encoded in linear logic, for example, 
% https://johnwickerson.github.io/talks/linearlogic.pdf

