\chapter{Introduction}\label{chapter:intro}
Sequent calculus is a formalism first introduced by G. Gentzen in 1934 \cite{Gentzen1935I, Gentzen1935II}.
In its classical interpretation, a sequent is just an implication between finite (possibly empty) sequences of formulae, such that
$$ \Delta \vdash \Gamma $$ 
is to be interpreted as
$$ \bigwedge_{\phi \in \Delta} \phi \Rightarrow \bigvee_{\gamma \in \Gamma} \gamma $$
and $\Delta$ is called the antecedent, and $\Gamma$ the succedent.

Sequents are manipulated using rules that have sequents as premises and as a conclusion.
These rules are used to describe connectives operationally: defining their behavior by how they are derived from smaller formulae.
A rule acting on the antecedent is called a left rule whereas a rule acting on the succedent is called a right rule.
A proof is then a tree where the root is the sequent to prove, and the inner nodes are the rules chained one to another by matching permises to conclusions.
This way the leaves are just rules without any premises, also called axioms.
In this context, a theorem prover -- or simply a prover -- is an algorithm that tries to build a proof of a given sequent to determine if it is true or not.
More precisely a bottom-up prover starts at the final sequent, and tries to build the proof tree by applying the rules of the calculus backwards, breaking down the formulae into their subformulae.

There are three special rules -- called structural rules -- often left implicit in proofs.
We give their left versions:
\begin{itemize}
	\item weakening: one may ``weaken'' the sequent by adding a proposition without changing the truth of the sequent
		$$
		\AXC{$\Gamma \vdash \Delta$}
		\UIC{$\Gamma, \phi \vdash \Delta$}
		\DP
		$$
	\item contraction: one may ``contract'' two copies of the same proposition into one without changing the truth of the sequent
		$$
		\AXC{$\Gamma, \phi, \phi \vdash \Delta$}
		\UIC{$\Gamma, \phi \vdash \Delta$}
		\DP
		$$
	\item exchange: one may switch position of two propositions in a sequent freely without changing its truth
		$$
		\AXC{$\Gamma, \phi, \psi \vdash \Delta$}
		\UIC{$\Gamma, \psi, \phi \vdash \Delta$}
		\DP
		$$
\end{itemize}
These do not directly specify the behavior of some connective, but rather show the ways in which a formula may be moved inside the sequent.
Logics missing at least one of these three rules are called substructural; it is in this class that lies linear logic.

Linear logic is a logic proposed by J.-Y. Girard in his seminal paper of 1987 \cite{LinearLogic}.
The distinctive trait of this logic is that in general its formulae cannot be copied or discarded, but instead are consumed.
Put differently, its sequent calculus lacks the structural rules of weakening and contraction\footnote{There are logics where even the exchange rule is missing, these are called non-commutative because the order of the formulae in the sequent matters (e.g. Lambek calculus).}.

Under these assumptions a certain sequent (here we are interpreting sequents as multisets) is provable if and only if all of its formulae get used exactly once; for this reason this logic is sometimes called a logic of resources, in the same way classical logic is a logic of truths and intuitionistic logic is a logic of proofs.
% Questo particolare utilizzo delle formule permette di avere una logica che mantiene la simmetria delle logica classica, e il costruttivismo delle logica intuizionista.

In linear logic each connective from classical logic has two interpretations: an additive one -- where the context is the same for all the premises, and a multiplicative one -- where the context is partitioned between the premises.
To better understand why, we analyze two rules for classic conjunction:
$$
\begin{array}{cc}
\AXC{$\Delta \vdash \phi_2, \Gamma$}
\AXC{$\Delta \vdash \phi_1, \Gamma$}
\BIC{$\Delta \vdash \phi_1 \wedge \phi_2, \Gamma$}
\DP
	&
\AXC{$\Delta' \vdash \phi_1, \Gamma'$}
\AXC{$\Delta'' \vdash \phi_2, \Gamma''$}
\BIC{$\Delta', \Delta'' \vdash \phi_1 \wedge \phi_2, \Gamma', \Gamma''$}
\DP
\end{array}
$$
The two rules above are equivalent only if the use of weakening and contraction is permitted; for this reason in linear logic the two interpretations are distinct: the left one is the additive one, and the right one the multiplicative one.
The same holds for the constants $\top$ and $\bot$, which also have two versions.
\begin{table}[h!]
	\centering
	\begin{tabular}{c|cc}
		\hline
		Class. & Add. & Mult. \\
		\hline
		\hline
		$\wedge$ & $\displaywith$  & $\displayten$ \\
		$\vee$   & $\displayplus$  & $\displaypar$ \\
		$\top$   & $\displaytop$   & $\displayone$ \\
		$\bot$   & $\displayzero$  & $\displaybot$ \\
	\end{tabular}
	\caption{Classical connectives and their corresponding linear ones.\label{table:classic to linear}}
\end{table}
Table \ref{table:classic to linear} shows the linear logic connectives corresponding to the classical ones.

Linear logic defined as of right now, albeit having the added complexity of splitting, is nonetheless decidable: since formulae are finite and they cannot be copied, it is possible to explore all the possibilities.
Undecidability is reintroduced by localizing the uses of contraction and weakening by means of two modalities --  ``of-course'', written $\llbang{\phi}$ and ``why-not'', written $\llwn{\phi}$ -- called exponentials.
% Linear logic defined as of right now is decidable, since formulas cannot grow in size we can explore each possibility.
% To be as strong as classical logic, there are two so called exponentials: bang or $!\phi$ and why not or $?\phi$.
% Since undecidability may lead to non-termination during proof search, usally some kind of bound is put (e.g. on the number of contractions per branch).
% This has the side effect of making the proover sometimes reject judgments which may be true, but would need too many contractions.

% Linear logic can be used to ensure that objects are used exactly once, thus allowing the system to safely deallocate an object after its use.
% The Haskell's compiler GHC has an experimental extensions to permit signatures with linear types.

% \subsection{Linear logic in practice}
% utilizzi logica lineare
%   * pi-calcolo
%   * risorse
%   * linear-haskell?
% A good example of linear logic may be chemical reactions % https://www.cs.cmu.edu/~crary/317-f22/lectures/20-linear.pdf
% Here we can see a reaction as an implication, if we have the reagents we can consume them to obtain the products.

% Petri nets can be encoded in linear logic, for example, 
% https://johnwickerson.github.io/talks/linearlogic.pdf
It is the multiplicative side which brings the most complexity during bottom-up proof search: the action of partitioning the sequent -- called splitting -- may imply an exponential number of attempts to find the correct subsets.
Some methods have been proposed to alleviate this, such as the input/output method \cite{IO}.

In 2001 D. Pym and J. Harland publish a paper \cite{HarlandPym} where they present a new way of tackling the problem of splitting by means of boolean constraints.
These constraints are generated during proof search from boolean expressions associated to the formulae, and are used to enforce linearity.
This way the complexity shifts from choosing the right set of formulas to prove a certain branch, to solving for boolean assignment -- a problem for which there are much more sophisticated algorithms.

This thesis is organized as follows:
\begin{itemize}
	\item in \S\ref{chapter:calculus} we define a focused and one-sided version to the calculus described in \cite{HarlandPym}, and we give a proof of its soundness consisting of a forgetful functor to the triadic calculus of \cite{Focusing};
	\item in \S\ref{chapter:implementation} we discuss a Prolog implementation of the calculus of the previous chapter;
	\item in \S\ref{chapter:related work} we quickly describe the main implementation details of two other bottom-up provers for full linear logic: llprover \cite{llprover} and APLL \cite{APLL};
	\item finally in \S\ref{chapter:testing} we present the framework built to test and compare our prover with the others from the previous section, and then we show the results of these benchmarks.
\end{itemize}
