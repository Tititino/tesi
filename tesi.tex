\documentclass[a4paper, 12pt, tesi, english]{report}

\usepackage[dbg]{tesi}

\begin{document}
\tableofcontents

\chapter{Intro}
In 2001 Pym and Harland publish a paper \cite{HarlandPym} where they propose a new way to tackle 
Given that SAT-solving has been at the center of reasearch for decades, and efficient algorithms have been developed for it, the paper proposes a new calculus for linear logic that associates to each formula a boolean variable, and enforces linearity by constraints on said variables.
This way the complexity shifts from choosing the right set of formulas to prove a certain branch, to solving for boolean assignment -- a problem for which there are much more sophisticated algorithmns.

We examine the efficiency of this method and we compare it to other provers for different substets of linear logic.

\section{Sequent calculus}
We will often talk about sequents, a seuquent of the form
$$ \Delta_1, \dots, \Delta_n \vdash \Gamma_1, \dots, \Gamma_m $$
is another way of writing
$$ \Delta_1 \wedge \dots \wedge \Delta_n \Rightarrow \Gamma_1 \vee \dots \vee \Gamma_m $$
In sequent calculus we define some rules to manipulate these sequents, these rules are for example 
$$
\AxiomC{$\Gamma, \phi \vdash \Delta$}
\AxiomC{$\Gamma, \psi \vdash \Delta$}
\BinaryInfC{$\Gamma, \phi \vee \psi \vdash \Delta$}
\DisplayProof
$$
means that if $\Gamma, \phi \vdash \Delta$ and $\Gamma, \psi \vdash \Delta$ hold, then $\Gamma, \phi \vee \psi \vdash \Delta$ holds.
This for example is the classic rule for $\vee$.

When trying to build a proof bottom up, we utilize these rules inversed to try to arrive at what are called axioms or leafs, rules with no premises.
$$
\AxiomC{}
\UnaryInfC{$\phi \vdash \phi$}
\DisplayProof
$$

Gentzen introduced the sequent calculus LK for classical logic, this had -- other that the usual rules -- three so called structural rules.
These rules were used to manipulate the sequent itself, and are
\begin{itemize}
	\item weakening: we can always ``weaken'' the sequent by adding a proposition without changing its truth,
		$$
		\AxiomC{$\Gamma \vdash \Delta$}
		\UnaryInfC{$\Gamma, \phi \vdash \Delta$}
		\DisplayProof
		$$
	\item contraction: we can always ``contract'' two copies of the same proposition into one without changing the truth of the sequent,
		$$
		\AxiomC{$\Gamma, \phi, \phi \vdash \Delta$}
		\UnaryInfC{$\Gamma, \phi \vdash \Delta$}
		\DisplayProof
		$$
	\item exchange: we can change position of two propositions in a sequent freely without changing its truth
		$$
		\AxiomC{$\Gamma, \phi, \psi \vdash \Delta$}
		\UnaryInfC{$\Gamma, \psi, \phi \vdash \Delta$}
		\DisplayProof
		$$
\end{itemize}
and their symmetric right rules.
These structural rules will be important in the next section where we will introduce linear logic.

\section{Linear logic}
Linear logic is a logic proposed by Jean-Yves Girard in his seminal paper of 1987 \cite{LinearLogic}.
It is a substructural logic, more precisely is the logic obtained by removing the rules for weakening and contraction:
This means that formula in a linear sequent must be used once and exactly once, since it cannot be duplicated nor discarded.
This property is called \textit{linearity}.

This peculiar treatment of formulae gives rise to two interpretations of the classic connectives, that would have been equivalent otherwise. 
So in linear logic connectives and their constants are doubled, there are multiplicative and additive versions of conjuction and disjunction:
\begin{itemize}
	\item multiplicative connectives -- namely $\llten$ (tensor) and $\llone$ (one), and $\llpar$ (par) and $\llbot$ (bottom) -- ``split'' the context in two
	\item additive connectives -- namely $\llwith$ (with) and $\lltop$ (top), and $\llplus$ (plus) and $\llzero$ (zero), deal with the same context
\end{itemize}

From this property of formulae sometimes linear logic is called a logic of resources.

Linear logic is also constructed with negation to be involutive, so there is a DeMorgan formula for each connective to push negation to the variables.

Linear logic defined as of right now is decidable, since formulas cannot grow in size we can explore each possibility.
To be as strong as classical logic, there are two so called exponentials: bang or $!\phi$ and why not or $?\phi$.
These have the purpose of localizing the uses of weakening and contraction:

For example, formulas marked with $!$ can be used any number of times, so the intuistic implication $a \rightarrow b$ is translated as $!(a \lolli b)$, and transitions in a petri net are represented by $!(resources_1 \lolli resources_2)$.

Linear logic can be used to ensure that objects are used exactly once, thus allowing the system to safely deallocate an object after its use.
The Haskell's compiler GHC has an experimental extensions to permit signatures with linear types.

% \subsection{Linear logic in practice}
% utilizzi logica lineare
%   * pi-calcolo
%   * risorse
%   * linear-haskell?
A good example of linear logic may be chemical reactions % https://www.cs.cmu.edu/~crary/317-f22/lectures/20-linear.pdf
Here we can see a reaction as an implication, if we have the reagents we can consume them to obtain the products.

Petri nets can be encoded in linear logic, for example, 
% https://johnwickerson.github.io/talks/linearlogic.pdf

\section{State of the art}
There are not many provers for classic linear logic.
llprover's website has a list of related works but these links are mostly dead and the provers cited lost.

\subsection{APLL}
APLL is the underlying prover of click\&collect. % cit
It provides 4 different searches -- forward and backwards for classic and intuitionistic linear logic. 
We will focus on the backwards algorithm for classic linear logic.

Before diving into the analysis of this prover, we give the definition of \textit{why-not height}
\begin{define}[Why-not height]
	\label{def:why-not-height}
	Why-not height is the maximum number of nested ``why-not''s in a formlua, or
	$$ \text{wnh}(\phi) = 
	\begin{cases}	
		0 & \text{se }\phi \in \{\llbot, \lltop, \llone, \llzero\} \\
		\max(\text{wnh}(\phi_1), \text{wnh}(\phi_2)) & \text{se } \phi \in \{ \phi_1 \llten \phi_2, \phi_1 \llpar \phi_2, \phi_1 \llplus \phi_2, \phi_1 \llwith \phi_2 \} \\
		\text{wnh}(\phi_1) & \text{se }\phi \in \{ \llnot{\phi_1}, \llbang{\phi_1}\} \\
		1 + \text{wnh}(\phi_1) & \text{se }\phi \in \{ \llwn{\phi_1} \} 
	\end{cases}
	$$
\end{define}
This measure is used in their particular way of dealing with uncontrained formulae, but is also used as a way to decide which branch to prove first for any normal operator.

The program is written in OCaML and implements a pretty standard focused proof search on normalized formulae as seen in \cite{LiangMiller}.
In this section we will illustrate two noteworthy charateristics of its implementation:
\begin{itemize}
	\item Sequent splitting when encountering a tensor is done by generating all the numbers up to $2^{|\Delta|}$ -- where $\Delta$ is the sequent -- and using the bit representation of those to create the two subsets.
		This can be seen in the function \texttt{split\_list}, which in turn calls \texttt{split\_list\_aux}
		\begin{lstlisting}[language=caml]
let rec split_list_aux (acc1, acc2) l k = match l with
  | [] -> acc1, acc2
  | hd :: tl -> 
      if k mod 2 = 0 then split_list_aux (acc1, hd :: acc2) tl (k / 2)
      else split_list_aux (hd :: acc1, acc2) tl (k / 2)
		\end{lstlisting}
		where the argument \texttt{k} is the number that determines the decomposition of the sequent.
		This function is called recursively when a tensor is encountered during proof search, starting at $ k = 2^{|\Delta|}$ and decreasing by one at each iteration
		\begin{lstlisting}[language=caml]
(* ... *)
| Tensor (g, h) ->
  let rec split_gamma k = 
    if k = -1 then None
    else
      let gamma1, gamma2 = split_list gamma k in
        try
	  (* ... *)
        with NoValue ->
          split_gamma (k - 1) 
  in
    let k = fast_exp_2 (List.length gamma) - 1 in
      (* ... *)
		\end{lstlisting}

		As we will see in ... %ref
		this implementation choice will result in a degradation of performance on formulae with a high number of multiplicatives.
	\item This prover does not use a simple limit to the number of applications of the contraction rule in a branch, instead an initially empty queue of unrestricted formulae (\texttt{select\_d2}) and a counter (\texttt{max\_d2}) are kept during the search.
		Two cases arise:
		\begin{itemize}
			\item if \texttt{select\_d2 = []} and \texttt{max\_d2 > 0} then the sequent of unrestricted formulae is taken, negative terms are filtered out and it is sorted based on why-not height.	% cambio persona
				\begin{lstlisting}[language=caml]
(* ... *)
if select_d2 = [] then begin
  (if max_d2 = 0 then (bl := true; raise NoValue));                    
  let select_d2' = 
    sort_whynot (List.filter (fun x -> not (is_neg x)) 
    (Set_formula.elements theta)) in
  if select_d2' = [] then None
  else
    apply_d2 select_d2' (max_d2 - 1) end
    (* ... *)
				\end{lstlisting}
				This new list of unrestricted formule becomes the new \texttt{select\_d2}.
				Otherwise if \texttt{select\_d2} is still empty after being refilled (line 7) or if \texttt{max\_d2} is 0 (line 3) the branch fails.
			\item if \texttt{select\_d2} is not empty, then the first formula in the queue is extracted and added to the working set.	% sistemo
				If the branch fails the formula gets discarded and the next one in the queue is tried.	% sistemo
		\end{itemize}
		The main purpose of this whole process is to avoid infinite loops that always contract on the same formula.
		Instead all the formulae are tried one by one, starting from the simplest (lower why-not height).

		The counter \texttt{max\_d2} is a local bound, since decreasing it in a branch does not affect other branches.
\end{itemize}

\subsection{\texttt{llprover}}
\texttt{llprover} is a linear logic prover by Naoyuki Tamura.
This prover has the following characteristics:
\begin{itemize}
	\item iterative deepening on exponential formulae
	\item 
\end{itemize}
% iterative deepening, focused
% \texttt{llprover}'s 

\section{Why Prolog}
Prolog as a language and as an environment has been historically tied to automated theorem proving for its ability to express these king of algorithmns naturally.
% In particular we chose SWI-Prolog because it offers a comprehensive and mature free Prolog environment.
One particularly conventient characteristic of Prolog is its automatic managing of backtracking, in most other languages we would have had to use exceptions to walk down the stack, or a queue of unfinished computations, which would have made the code much less readable.

Most Prolog implementations also support CLP or constraint logic programming.
This allows to have constraints referencing some attributes of variables in the body of clauses, in our case we use CLP($\mathcal{B}$) for boolean constraints and an handy interface to a sat-solver.
The library exposes operators to compose boolean formulas made of prolog variables
\begin{lstlisting}[language=prolog, numbers=none]
X = (X =:= 1),
Y = (X * X)
\end{lstlisting}
and to check the satisfaiability of said formulas
\begin{lstlisting}[language=prolog, numbers=none]
?- sat(X * Y).
X = Y, Y = 1.
\end{lstlisting}

\chapter{The focused calculus}
Before describing the calculus we must give some definitions

% Explaination on constraints why we use them ecc ecc ecc

\begin{define}[Annotated formula]
	Given a formula $\phi$ defined as in Figure \ref{fig:ll-connectives} and a boolean expression
	$e$ defined as in Figure \ref{fig:var-name}, an \textit{annotated formula} is simply a term 
	$$ \text{af}(\phi, e) $$
	that associates the formula to the expression.
	We denote 
	$$ \text{exp}(\af{\phi}{e}) = e $$
	and then extend this notation to sequents, such that $ \text{exp}(\Delta) $ is the set of all boolean expressions of $\Delta$.
\end{define}
As seen in Figure \ref{fig:ll-connectives} and \ref{fig:var-name} we will usually use $\phi$ to refer to formulas, $x$ to refer to boolean variables, and $e$ to refer to boolean expressions (conjuctions of variables).

The purpose of putting formulae and variables together in the annotated formula is twofold:
\begin{itemize}
	\item the actions taken on the formula determine the constraints that will be generated, and these depend on the variables associated to said formula;
	\item after the constraints are solved we can query the assignement of the variables and find out if the associated formula is used or not in a certain branch of a proof.
\end{itemize}
\begin{figure}[H]
	\centering
	\begin{tblr}{ colspec = {cccccccccr}
		    , cells = { mode = math } 
		    % , vborder{1-4} = { leftspace = 0pt, rightspace = 0pt } 
		    }
		\phi & ::=  & 1             &\mid& \phi \llten \phi  &\mid& \bot &\mid& \phi \llpar \phi  & \text{(Multiplicatives and their constants)} \\
		     & \mid & 0             &\mid& \phi \llplus \phi &\mid& \top &\mid& \phi \llwith \phi & \text{(Additives and their constants)} \\
		     & \mid & \llbang{\phi} &\mid& \llwn{\phi}       &    &      &    &                   & \text{(Exponentials)} \\
		     & \mid & \llnot{\phi}  &\mid& \text{name} 
	\end{tblr}
	\caption{Linear logic connectives}
	\label{fig:ll-connectives}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{tblr}{ colspec = {cccccr}, cells = { mode = math } }
		x & ::=  & x_i &\mid& \overline{x_i} & \text{(Variable)}\\
		e & ::=  & x \wedge e    &\mid& x & \text{(Expression)} \\
	\end{tblr}
	\caption{Definition of a boolean variable and expression}
	\label{fig:var-name}
\end{figure}

\begin{define}[Consuming formulae]
	\label{def:used}
	Given a boolean expression $e$ as in Figure \ref{fig:var-name}, we use the following notation
	\begin{itemize}
		\item ``$\used{e}$'' to state that the formula associated to the boolean expression $e$ gets consumed in this branch of the proof,
			this corresponds to saying the constraint $e$ is true;
		\item ``$\notUsed{e}$'' to state that the formula associated to the boolean variable $e$ does not get consumed in this branch of the proof,
			this corresponds to saying the constraint $e$ is not true.
	\end{itemize}
	We then extends these predicates to sequents
	\begin{align*}
		\used{\Delta} &= \{ \used{e_2} \mid e_2 \in \text{exp}(\Delta) \} \\
		\notUsed{\Delta} &= \{ \notUsed{e_2} \mid e_2 \in \text{exp}(\Delta) \}
	\end{align*}
\end{define}

\begin{define}[Members of the sequent]
	Given any sequent this can be in either two forms:
	\begin{itemize}
		\item focused or in the synchronous phase, written:
			$$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\phi}$$
		\item in the asynchronous phase, written:
			$$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\Phi}$$
	\end{itemize}
	These two have more or less the same members, which are
	\begin{itemize}
		\item the set $\Psi$ of unrestricted formulae, or all formulae that can be freely discarded or duplicated;
		\item the multisets $\Delta$ and $\Phi$ of linear (annotated) formulae, these are respectively the formulas ``put to the side'' and the formulae which are being ``worked on'' during a certain moment of the asynchronous phase;
		\item the set $\Lambda$ of constraints, which is to be interpreted as the conjunction of its members, and the set $V$ which represents a propagated solution.
			Furthermore if $\Lambda$ is satisfiable we write $\sat{\Lambda}{V}$, so:
				$$ \sat{\Lambda}{V} \;\Longleftrightarrow\; \bigwedge_{e \in \Lambda} e \text{ is \textit{sat} by } V $$
			$V$ is in and of itself a boolean expression as in Figure \ref{fig:var-name}, in fact it can be seen as the conjunction of the variables assigned to true and the negation of the variables assigned to false.
			As such it can be used as a constraint, stating that a certain solution must be respected in a new one.
			
			This approach to constraints helps to make the flow of the variables and solutions through the proof tree more explicit and clear and leaves no ambiguity to where the constraints should be checked.

			The choice of letters is mainly a mnemonic or visual one, constraints $\Lambda$ ``go-up'' the proof tree and solutions $V$ ``come down'' from the leaves.
	\end{itemize}
\end{define}

\begin{define}[Splitting a sequent]
	\label{def:split}
	Given a sequent of annotated formulae $\Delta$ we define the operation of splitting it as a function
	$$ \text{split}(\Delta) \mapsto (\Delta^L, \Delta^R) $$
	where, given a set $X$ of new variable names $x_i$ for each formula $\phi_i \in \Delta$
	\begin{align*}
		\Delta^L &= \{ \af{\phi_i}{x_i \wedge e_i} \mid i \in \{1, \dots, n\}\} \\
		\Delta^R &= \{ \af{\phi_i}{\varNot{x_i} \wedge e_i} \mid i \in \{1, \dots, n\}\} \\
	\end{align*}
	with $n$ the cardinality of $\Delta$, and $\phi_i$ and $e_i$ respectively the formula and the variable of the $i$-eth annotated formula in $\Delta$ using an arbitrary order.

	It is worth noting that the variables $x_i \in X$ used for $\Delta^L$ and $\Delta^R$ must be the same, this condition is necessary for the mechanism ensuring that a formula used on the left side of a tensor proof is not used on the right side and viceversa.

	With a slight abuse of notation we will write $\text{split}(\Delta)_L$ and $\text{split}(\Delta)_R$ respectively as the left projection and the right projection of the pair $(\Delta_L, \Delta_R)$.
\end{define}
As a small example for clarity, given the sequent
$$ \Delta = \af{a \llten b}{x_1}, \af{\llnot{c}}{x_2} $$
this is split into
\begin{align*}
	\text{split}(\Delta)_L &\mapsto \af{a \llten b}{x_3 \varAnd x_1}, \af{\llnot{c}}{x_4 \varAnd x_2} \\
	\text{split}(\Delta)_R &\mapsto \af{a \llten b}{\varNot{x_3} \varAnd x_1}, \af{\llnot{c}}{\varNot{x_4} \varAnd x_2} 
\end{align*}
% example of splitting and variables

% One simple but important detail that will be useful later when explaining the Prolog implementation is noting that the variables in common between two branches with the same root are always introduced before the two branches diverge.
% Or -- put differently -- all new variables introduced in any point of a path from the root of the proof to a leaf may appear only in the subtrees.	% sistemo
\begin{define}
	Lastly given a formula $\phi$ we define the following predicates
	\begin{itemize}
		\item $\isAsy{\phi}$ is a predicate that's true only when $\phi$ is an asynchronous formula, which are
			$$ \phi ::= \phi \llpar \phi \mid \phi \llwith \phi \mid \llwn{\phi} \mid  \lltop \mid \llbot $$
		\item $\isNegLit{\phi}$ is a predicate that's true only when $\phi$ is a negative literal, in our implementation negative literals are atoms, and positive literals are negated atoms.
	\end{itemize}
\end{define}

We are now ready to present the full calculus.
\begin{figure}[H]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{ colspec = { cc }, rows = {abovesep=10pt, belowsep=10pt}}
			\SetCell[c=2]{c} {\small
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1}{e}, \af{\phi_2}{e}, \Phi}$}
			\LeftLabel{$[\llpar]$}
			\UnaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1 \llpar \phi_2}{e}, \Phi}$}
			\DisplayProof} \\
			{\small
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\Phi}$}
			\LeftLabel{$[\llbot]$}
			\UnaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\llbot}{e}, \Phi}$}
			\DisplayProof}
			&
			{\small
			\AxiomC{}
			\LeftLabel{$[\lltop]$}
			\UnaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\lltop}{-}, \Phi}$}
			\DisplayProof
			}
			\\
			\SetCell[c=2]{c} {\small
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{V'}}{\Psi}{\Delta}{\af{\phi_2}{e}, \Phi}$}
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{V''}}{\Psi}{\Delta}{\af{\phi_1}{e}, \Phi}$}
			\LeftLabel{$[\llwith]$}
			\BinaryInfC{$\async{\constr{\Lambda}{V', V''}}{\Psi}{\Delta}{\af{\phi_1 \llwith \phi_2}{e}, \Phi}$}	% capisco cosa fanno qui i constraint
			\DisplayProof}
			\\
			\SetCell[c=2]{c} {\small
			\AxiomC{$\async{\constr{\Lambda}{V}}{\Psi, \phi}{\Delta}{\Phi}$}
			\LeftLabel{$[\,?\,]$}
			\UnaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\llwn{\phi}}{-}, \Phi}$}
			\DisplayProof} 
			\\
			\SetCell[c=2]{c} {\small
			\AxiomC{$\isNotAsy{\phi}$}
			\AxiomC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta, \af{\phi}{e}}{\Phi}$}
			\LeftLabel{$[R\!\Uparrow]$}
			\BinaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}, \Phi}$}
			\DisplayProof
			}
		\end{tblr}
		\caption{Asynchronous rules}
	\end{subfigure}

	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{colspec = { cc }, rows = {abovesep=10pt, belowsep=10pt}}
			\SetCell[c=2]{c} {\small
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{V'}}{\Psi}{\text{split}(\Delta)^L}{\text{af}(\phi_1, e)}$}
			\AxiomC{$\focus{\constr{V'}{V''}}{\Psi}{\text{split}(\Delta)^R}{\text{af}(\phi_2, e)}$}
			\LeftLabel{$[\llten]$}
			\BinaryInfC{$\focus{\constr{\Lambda}{V''}}{\Psi}{\Delta}{\text{af}(\phi_1 \llten \phi_2, e)}$}	% capisco il movimento dei constraint
			\DisplayProof}
			\\ 
			{\small
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1}{e}} $}
			\LeftLabel{$[\llplus_L]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}}$}
			\DisplayProof}
			&
			{\small
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi_2}{e}}$}
			\LeftLabel{$[\llplus_R]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}}$}
			\DisplayProof}
			\\
			{\small
			\AxiomC{$\sat{ \used{e_1}, \notUsed{\Delta}, \Lambda}{V}$}
			\LeftLabel{$[1]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\llone}{e_1}}$}
			\DisplayProof} 
			&
			{\small
			\AxiomC{$\focus{\constr{\used{e_1}, \notUsed{\Delta}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e_1}}$}
			\LeftLabel{$[\,!\,]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\llbang{\phi}}{e_1}}$}
			\DisplayProof
			}
			\\
			\SetCell[c=2]{c} {\small
			\AxiomC{$\isAsy{\phi} \vee \isNegLit{\phi}$}
			\AxiomC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\LeftLabel{$[R\!\Downarrow]$}
			\BinaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\DisplayProof
			}
		\end{tblr}
		\caption{Synchronous rules}
	\end{subfigure}

	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{ colspec = { cc }
			    , rows = {abovesep=10pt, belowsep=10pt}
			    , vborder{1-2} = { leftspace = -10pt, rightspace = -10pt } 
			    }
			{\small
			\AxiomC{$ \sat{\used{e_1}, \used{e_2}, \notUsed{\Delta}, \Lambda}{V}$}
			\LeftLabel{$[I_1]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta, \af{\phi}{e_2}}{\af{\llnot{\phi}}{e_1}}$}
			\DisplayProof}
			&
			{\small
			\AxiomC{$\isNotNegLit{\phi}$}
			\AxiomC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\LeftLabel{$[D_1]$}
			\BinaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta, \af{\phi}{e}}{.}$}
			\DisplayProof}
			\\
			{\small
			\AxiomC{$ \sat{\used{e_1}, \notUsed{\Delta}, \Lambda}{V}$}
			\LeftLabel{$[I_2]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi, \phi}{\Delta}{\af{\llnot{\phi}}{e_1}}$}
			\DisplayProof}
			&
			{\small
			\AxiomC{$\isNotNegLit{\phi}$}
			\AxiomC{$\new{e}$}
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}} $}
			\LeftLabel{$[D_2]$}
			\TrinaryInfC{$\async{\constr{\Lambda}{V}}{\Psi, \phi}{\Delta}{.}$}
			\DisplayProof}
		\end{tblr}
		\caption{Identity and decide rules}
	\end{subfigure}
	\caption{The complete focused constraint calculus}
	\label{fig:calculus}
\end{figure}

\section{Completeness proof}

\chapter{Implementation}

\section{Formula transformations}
Before beginning the proof a sequent passes through a number of transformations.
These transformations both preprocess the sequent to a more convenient form, and also add information about the subformulae.

As a first transformation the sequent gets normalized into a sequent in negated normal form (NNF).
NNF is the form where all negations are pushed to atoms and all linear implications ($\lolli$) are expanded into pars ($\llpar$) using the following tautology
$$ a \lolli b \Leftrightarrow \llnot{a} \llpar b $$
Normalization is a common technique -- used in all the provers we compare with.
The process is composed of just two steps
\begin{enumerate}
	\item the left sequent is negated and appended to the right sequent, implemented by the predicate \texttt{negate\_premises};
	\item the predicate \texttt{nnf}, which encodes the DeMorgan rules, is mapped recursively over the new sequent
\end{enumerate}
This is possible since classic linear logic is symmetric and negation is involutive.

The purpose of this process is cutting away a great deal of possible rules applicable to the sequent sacrificing some of the structure of the sequent.
In fact the rules we need to implement after normalization is more than halved, since we now need just the right rules, without the ones for negation and linear implication.

As a second transformation to each formula we assign its why-not height, as defined in Definition \ref{def:why-not-height}.
Why-not height is used during proof search to decide which branch to do first and which exponential to decide first, as in APLL.
This is a technique borrowed from APLL and its obvious purpose is to try first the branches with less exponentials: in the case this -- probably -- simpler branch fails, we do not have to try the other -- probably -- harder one.
Formulae are now attribute trees, with, at each node the why not height of the subformula.

As a third and final transformation each formula gets annotated, this means we associate a variable to each formula in the sequent
Given a sequent $\Delta$ we obtain
$$ \hat{\Delta} = \{ \af{\phi}{x} \mid \new{x}, \phi \in \Delta \} $$
To be clear, a variable is only assigned to the ``top-level'' formula, and subformulae are left unchanged.

In the implementation the concept of variable is split in two: the name of the variable -- represented by a Prolog atom, and the value of the variable -- represented by a Prolog variable.
This is needed since, after checking the constraints, the sat solver unifies the variable to its value if it finds a satisfaiable solution, so we need the atom to associate the variable value to its name if the final proof.
The process of annotation is implemented by the predicate \texttt{annotate}
\begin{lstlisting}[language=prolog]
%! annotate(+[Formulae], -[AFs], -[Constraints]) is det.
annotate(Fs, Afs, Cns) :-
    	maplist([F, af(F, X, Var), v(Var) =:= 1]>>(gensym(x, X)), Fs, Afs, Cns).
\end{lstlisting}
which is a simple map over the sequent.
The predicate returns both the annotated formulae, and the constraints on the variables of the annotated formulae: all of the variables must be set to one in a solution -- all the formulae must be used.

\section{Constraint propagation}
Some care is to be given to explaining how the constraints propagate.
In fact, in constrast to Figure \ref{fig:calculus} The implementation does not have explicit propagation of the solution to the constraints.
This is because Prolog's unification implicitly propagates a solution from one branch to another, since the variables used in the constraints are the same.
% continuo?

\section{Helper predicates}
We define some helper predicates to work with the constraints.

What we defined as $\notUsed{\Delta}$ in \ref{def:used} corresponds to the predicate \texttt{set\_to\_zero}
\begin{lstlisting}[language=prolog]
%! set_to_zero(+[AFs], -[Constr]) is det.
set_to_zero(Fs, Cns) :-
    maplist([af(_, _, E), v(E) =:= 0]>>true, Fs, Cns).
\end{lstlisting}

The other helper predicate is the predicate to split the context when a tensor is encountered.
This implements the split function defined in Definition \ref{def:split}
\begin{lstlisting}[language=prolog]
%! split_ctx(+[AFs], -[AFs], -[AFs], -[Cns], -[Cns]) is det.
split_ctx(Afs, Pos, Neg, PCns, NCns) :-
    	maplist([ af(F, Var, E)
	        , af(F, VarPos, Y)
		, af(F, VarNeg, Z)
		, v(Y) =:= v(X) * v(E)
		, v(Z) =:= (~ v(X)) * v(E)
		]>>(
        	gensym(x, Name),
        	atomic_list_concat([Var, Name], '.', VarPos),
        	atomic_list_concat([Var, Name], '.~', VarNeg)
    	), Afs, Pos, Neg, PCns, NCns).
\end{lstlisting}
It is again a map over the list of formulae, that generates the new formulae and the constraints accordingly.
Three new Prolog variables are introduced: \texttt{X}, \texttt{Y} and \texttt{Z}.
\texttt{X} is the new variable, the annotated formulae refere to the variable \texttt{Y} and \texttt{Z}, and constraints are added so that
\begin{align*}
	y &= x \varAnd e \\
	y &= \varNot{x} \varAnd e \\
\end{align*}
Compare this to the original definition of Definition \ref{def:split}, one can see that the two are basically identical other than the fact that here the name of the variable (the atom) and its value are treated separately.

\section{Focusing}
When explaining the code we will use some common names for variables, these are
\begin{itemize}
	\item \texttt{A} is a set of unrestricted atoms
	\item \texttt{U} is a set of unrestriched formulae
	\item \texttt{F}, \texttt{F1}, ..., are formulae, and \texttt{Fs} and \texttt{D} is a list of them
	\item \texttt{S} is the queue of currently usable unrestricted formulae
	\item 
\end{itemize}
During the asynchronous phase we have a list of formulae which are being worked on and a list of formulae which are put to the side.
The former is \texttt{F} and the latter is \texttt{D}.
At each step we analyze the first element of the list \texttt{F}, and we keep scomposing the memebers of the list untill we can't anymore.
We can see this in the predicate for $\llwith$
\begin{lstlisting}[language=prolog]
async(A, U, D, [F|Fs], S, M, In, _) :-
  F = af(((F1-H1) & (F2-H2))-_), N, E), !,
  ( H2 > H1	
  -> async(A, U, D, [af((F1-H1), N, E)|Fs], S, M, [v(E) =:= 1|In], _), 
     async(A, U, D, [af((F2-H2), N, E)|Fs], S, M, [v(E) =:= 1|In], _) 
  ;  async(A, U, D, [af((F2-H2), N, E)|Fs], S, M, [v(E) =:= 1|In], _),
     async(A, U, D, [af((F1-H1), N, E)|Fs], S, M, [v(E) =:= 1|In], _)
  ).
\end{lstlisting}
Here we can see both the choice being made based on the why-not height of the two subformulae, and how the with is scomposed.
Compare this with the $\llwith$ rule in Figure \ref{fig:calculus}.
The whole idea of focusing is to reduce the number of choice points by identifying transformations which can be applied in any order (the asynchronous rules).
This is represented by the cut at line 2, once we find and asynchronous connective it gets scomposed and we do not try other rules on it if the branch fails.

If a formula cannot be further be broken apart -- i.e. it is either an atom, a negated atom, or it has a toplevel synchronous connective -- then it is put to the side in \texttt{D}.
This is implemented by the rule $R\!\Uparrow$
\begin{lstlisting}[language=prolog]
async(A, U, D, [F|Fs], S, M, In, _) :-
        F = af((Op-_), _, _),
        not(is_asy(Op)), !,
        async(A, U, [F|D], Fs, S, M, In, _).
\end{lstlisting}
This process goes on untill \texttt{Fs} has still formulae inside.

When \texttt{Fs} is empty the phase switches, and the focusing process begins: we choose a formula -- called \texttt{decide} -- from either \texttt{D} or \texttt{U} and we scompose it untill either an asynchronous connective is left or a negated atom.
This is represented by the rules $D_1$ and $D_2$.

This process of alternating asynchronous and synchronous phases in classic focusing goes on untill we have a positive literal (in our case a negated atom) in \texttt{Fs} and the corresponding negative literal (in our case just an atom) in either \texttt{U} or \texttt{D}.
When this happens the axioms -- rules $I_1$ or $I_2$ -- are applied and close the branch.
In our case when we are focusing and we have a positive literal in \texttt{Fs}, we check if the corresponding negative literal exists in \texttt{D}.
If this is true, then the variables of all the other formulae in \texttt{D} are set to zero using the predicate \texttt{set\_to\_zero}, and the constraints are checked.

This is encoded in the clause 
\begin{lstlisting}[language=prolog]
focus(A, U, D, F, _, _, In, node(id_1, A, U, D, F, [])) :-
	F = af(((~ T)-_), _, E1),
    	is_term(T),
	select(af((T-_), _, E2), D, D1),
    	set_to_zero(D1, Dz),
    	append([v(E1) =:= 1, v(E2) =:= 1|Dz], In, Cns),
    	check(Cns).
\end{lstlisting}
A slightly different process happens if instead a correspondence is found in \texttt{A} instead of \texttt{D}.
\texttt{A} is a special set containing just unrestricted atoms.
This is a slight modification to APLL approach based on the fact that once negative literals are put in a sequent they can never leave it.
Since \texttt{U} may be sorted many times, we try to keep the number of formulae in it small.

This ordeal of the queue of unrestricted formulae 

\begin{lstlisting}
async(A, U, D, [], [H|T], M, In, node(decide_2, A, U, D, [], [Tree])) :-
	\+ U = [],
	gensym(x, X),
	focus(A, U, D, af(H, X, E), T, M, [v(E) =:= 1|In], Tree).
% else if it fails try with the next one
async(A, U, D, [], [_|T], M, In, Tree) :-
	\+ U = [],
	async(A, U, D, [], T, M, In, Tree).
% if the queue is empty the refill it, 
% but go in a special state called early_stop
async(A, U, D, [], [], M, In, Tree) :-
	\+ U = [],
	refill(U, M, S, M1),
	early_stop(A, U, D, S, M1, In, Tree).

early_stop(_, _, _, [], _, _, _) :-
	false.
early_stop(A, U, D, [H|T], M, In, Tree) :-
	gensym(x, X),
	focus(A, U, D, af(H, X, E), T, M, [v(E) =:= 1|In], Tree).
early_stop(A, U, D, [_|T], M, In, Tree) :-
	early_stop(A, U, D, T, M, In, Tree).

refill(U, M, S, M1) :-
	\+ M = 0,
	\+ U = [], 
	sort(2, @=<, U, S), % sort on why_not_height, keep duplicates (same heights)
	M1 is M - 1.
\end{lstlisting}

All the rules $D_1$, $I_1$ are put before the unrestricted counterparts, so that their are tried first.

\section{Building the tree}
In the listings above we omitted one parameter of the predicates, which purpose is to build the proof tree.
At each call of async and focus one node of the proof tree is built.
This contains the context of the call.
For example in
\begin{lstlisting}[language=prolog]
async(A, U, D, [F|Fs], S, M, In, node(par, A, U, D, [F|Fs], [Tree])) :- 
  F = af(((F1 / F2)-_), N, E), !,
  Fs1 = [af(F1, N, E), af(F2, N, E)|Fs],
  async(A, U, D, Fs1, S, M, [v(E) =:= 1|In], Tree).
\end{lstlisting}
we can see clearly the structure of the node: a label, the context and an -- optionally empty -- list of subtrees.
A leaf is just a node with an empty list of subtrees.

This tree can be used in the end to reconstruct the actual proof tree, by visiting it and -- for each formula -- querying whether its variable is set of one, and cancelling it otherwise.
A more sophisticated algorithm may even cancel out unwanted unrestricted formulae, that otherwise remain lingering in the sequent.

\chapter{Testing}
\section{Reproducibility}
The prover's tests and benchmarks are made using a jupyter notebook.
To ensure reproducibility we use nix.
Nix is a build system based on reproducile and declarative recipes, called \textit{derivations}.
For this project infrastructure we used nix flakes, which are an experimental feature of nix that makes derivations more hermetic and pure.

\section{Prefix format}
Since for benchmarking we will interface with a lot of different provers, each with its own syntax for expressions, we define a common format for expressions.
This format must have the important the important characteristics of being easy to parse and translate, and being fairly versatile.
For this purpose we define a prefix format for linear logic formulae inspired by the format used by \cite{TarauPaiva} for implicational formulae
\begin{table}[H]
	\centering
	\begin{tblr}{ colspec = {cc} }
		\hline
			formula & symbol \\
		\hline
		\hline
			$\phi_A \llten \phi_B$  & \texttt{*AB} \\
			$\phi_A \llpar \phi_B$  & \texttt{|AB} \\
			$\phi_A \llplus \phi_B$ & \texttt{+AB} \\
			$\phi_A \llwith \phi_B$ & \texttt{\&AB} \\
			$\phi_A \lolli \phi_B$  & \texttt{@AB} \\
			$\llnot{\phi_A}$        & \texttt{\^{}A} \\
			$\llwn{\phi_A}$         & \texttt{?A} \\
			$\llbang{\phi_A}$       & \texttt{!A} \\
	\end{tblr}
\end{table}
Furthermore each character not representing an operator is considered as a variable name.
Longer names can be specified by enclosing them in single apices as in \texttt{'varname'}.
We give an example using DeMorgan for the tensor:
$$ \llnot{(a \llten b)} \lolli \llnot{a} \llpar \llnot{b} $$
gets translated into
$$ \text{\texttt{@\^{}ab|\^{}a\^{}b}} $$

Since most one of the datasets we use is LLTP % cite
, which is written in TPTP's format % cite
, we define a parser for this using Haskell's parser generator happy. % cite

\section{File formats}
As a standard format to store json file we use json, for its ease of use in most modern programming languages.
A test cases i defined as a list of
\begin{lstlisting}
TestCase ::= {
		"id" = <Number>,
		"premises" = [ <PrefixFormula> { , <PrefixFormula> } ],
		"conclusions" = [ <PrefixFormulas> { , <PrefixFormula> } ]
             }
\end{lstlisting}
These are
\begin{description}
	\item[id] the sole purpose of the numeric identifier is to trace back the test case from the output;
	\item[premises] a list of premises as prefix formulae
	\item[conclusions] a list of conclusions as prefix formulae
\end{description}

Other fields may be present, for example we will use the following optional fields:
\begin{description}
	\item[thm] whether this test case is a tautology or not, may be null;
	\item[*, ...] the number of times a specific appears in the test case;
	\item[notes] human readable text about the test case, for example its infix representation;
	\item[size] an indicative number of the size of the formula;
	\item[atoms] the upper bound on the number of atoms.
\end{description}

On the other side outpust are dumped in a CSV file.

\section{Testing}
Finding a comprehensive test suite for linear logic, moreover one which is not made up mostly of translations of other theorems to linear logic, is not easy.
Most of the theorems we use for testing are llprover's %cite
tests, which are simple basic tautologies like the DeMorgan laws.

The problem with datasets with a high number of exponential formulae is the high number of timeouts and failures.
Failures happen when the bound is too small for a formula, and all the branches are exhausted before the timeout.

Nervertheless we use test our prover using some of the tests from LLTP, like Kleene's fromulae using call-by-name or call-by-value translations. % cite
% Out of theese our prover perfo....
\missingfigure{performance of our prover}

\section{Benchmarking}
We now show how our prover compares mainly to APLL, which is the only prover we found using the backwards method with for full classical linear logic.
The adapt the random formula generator from APLL to make it ablee to choose which connectives to use.
And we generate some datasets of 100 formulae in mll, all, mall and finally cll.

\subsection{Results}
Using random formulae we can clearly see that our prover outperforms APLL (and llprover) with formulae rich in multiplicatives.
As soon as exponentials come into play the differences smooth out.

Most of these outcomes see a persistent slght delay in the times of our prover in respect to APLL ones.
This is to be expected since OCaML and is to be ascibed to the interpreted nature of Prolog.

Since llprover uses incremental search, its times are one of the slowest.

All tests are run with a timeout of 600 seconds, and an indicative bound of 5.
Each prover treats the bound differently so sometimes a provers prematurely fail because of a small bound. 

We run the tests on a dataset of 100 random normalized multiplicative formulae.
These results show what we said before, APLL choice for splitting greatly hampers it's ability with formulae rich in multiplicatives, to the point that APLL consistently exhausted its time on almont a fourth of the formulae tried.
...
\begin{figure}[h!]
	\centering
	\includegraphics{./images/mll.pdf}
\end{figure}

The additive case is not that significant, since the only choice is the one for plus, the formulae remain manageable and no major differences can be seen.
\missingfigure{all}

We can see that in the multiplicative and additive case the difference begin to level, 
\begin{figure}[h!]
	\centering
	\includegraphics{./images/mall.pdf}
\end{figure}
Finally in classical linear logic our prover performs slightly worse than APLL.

\chapter{Conclusions}
We showed that -- given appropriate modifications -- Harland\&Pym's method is competitive to other provers, and in some cases (see multiplicatives) may be even faster.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
