\documentclass[a4paper, 12pt, tesi, english]{report}

\usepackage[dbg]{tesi}

\title{Titolo}

\begin{document}
\maketitle

\newpage

\tableofcontents

\chapter{Intro}
In 2001 Pym and Harland publish a paper \cite{HarlandPym} where they propose a new way to tackle the problem of splitting sequents during linear logic proof search using boolean constraints.
The paper proposes a new calculus for linear logic that associates to each formula a boolean variable, and enforces linearity by constraints on said variables.
This way the complexity shifts from choosing the right set of formulas to prove a certain branch, to solving for boolean assignment -- a problem for which there are much more sophisticated algorithmns.

We examine the efficiency of this method and we compare it to other provers for different substets of linear logic.

% \section{Sequent calculus}
% We will often talk about sequents, a seuquent of the form
% $$ \Delta_1, \dots, \Delta_n \vdash \Gamma_1, \dots, \Gamma_m $$
% is another way of writing
% $$ \Delta_1 \wedge \dots \wedge \Delta_n \Rightarrow \Gamma_1 \vee \dots \vee \Gamma_m $$
% In sequent calculus we define some rules to manipulate these sequents, these rules are for example 
% $$
% \AxiomC{$\Gamma, \phi \vdash \Delta$}
% \AxiomC{$\Gamma, \psi \vdash \Delta$}
% \BinaryInfC{$\Gamma, \phi \vee \psi \vdash \Delta$}
% \DisplayProof
% $$
% means that if $\Gamma, \phi \vdash \Delta$ and $\Gamma, \psi \vdash \Delta$ hold, then $\Gamma, \phi \vee \psi \vdash \Delta$ holds.
% This for example is the classic rule for $\vee$.
% 
% When trying to build a proof bottom up, we utilize these rules inversed to try to arrive at what are called axioms or leafs, rules with no premises.
% $$
% \AxiomC{}
% \UnaryInfC{$\phi \vdash \phi$}
% \DisplayProof
% $$
% 
% Gentzen introduced the sequent calculus LK for classical logic, this had -- other that the usual rules -- three so called structural rules.
% These rules were used to manipulate the sequent itself, and are
% \begin{itemize}
% 	\item weakening: we can always ``weaken'' the sequent by adding a proposition without changing its truth,
% 		$$
% 		\AxiomC{$\Gamma \vdash \Delta$}
% 		\UnaryInfC{$\Gamma, \phi \vdash \Delta$}
% 		\DisplayProof
% 		$$
% 	\item contraction: we can always ``contract'' two copies of the same proposition into one without changing the truth of the sequent,
% 		$$
% 		\AxiomC{$\Gamma, \phi, \phi \vdash \Delta$}
% 		\UnaryInfC{$\Gamma, \phi \vdash \Delta$}
% 		\DisplayProof
% 		$$
% 	\item exchange: we can change position of two propositions in a sequent freely without changing its truth
% 		$$
% 		\AxiomC{$\Gamma, \phi, \psi \vdash \Delta$}
% 		\UnaryInfC{$\Gamma, \psi, \phi \vdash \Delta$}
% 		\DisplayProof
% 		$$
% \end{itemize}
% and their symmetric right rules.
% These structural rules will be important in the next section where we will introduce linear logic.

\section{Linear logic}
Linear logic is a logic proposed by Jean-Yves Girard in his seminal paper of 1987 \cite{LinearLogic}.
The distintive trait of this logic is that its formulae cannot be copied (called weakening) or discarded (called contraction), but instead they are consumed.
And a certain sequent it true if and only if all its formulae get consumed exactly once.
For this reason this logic is sometimes called a logic of resources, in the same way classical logic is a logic of truths and intuitionistic logic is a logic of proofs.
% Questo particolare utilizzo delle formule permette di avere una logica che mantiene la simmetria delle logica classica, e il costruttivismo delle logica intuizionista.

In linear logic each connective of classical logic is doubled.
To better see this let's analyze classic conjuction, this can be defined as 

$$
\begin{array}{cc}
\AxiomC{$\Delta \vdash \phi_2, \Gamma$}
\AxiomC{$\Delta \vdash \phi_1, \Gamma$}
\BinaryInfC{$\Delta \vdash \phi_1 \wedge \phi_2, \Gamma$}
\DisplayProof
	&
\AxiomC{$\Delta'' \vdash \phi_2, \Gamma''$}
\AxiomC{$\Delta' \vdash \phi_1, \Gamma'$}
\BinaryInfC{$\Delta', \Delta'' \vdash \phi_1 \wedge \phi_2, \Gamma', \Gamma''$}
\DisplayProof
\end{array}
$$

On the other hand, these two rules are not equivalent in linear logic, since the former implies some weakening and contraction.
This is exactly the reason why in linear logic all connectives have two versions: an additive one -- where the two branches keep the same context, and a multiplicative one -- where the context gets partioned between the two branches.
Obviously the constants $\top$ and $\bot$ also have two versions.
We have that
\begin{center}
	\begin{tblr}{ccc}
		\hline
		& Additive & Multiplicative \\
		\hline
		\hline
		$\wedge$ & $\llwith$ & $\llten$ \\
		$\vee$ & $\llplus$ & $\llpar$ \\
		$\top$ & $\top$ & $1$ \\
		$\bot$ & $0$ & $\bot$ \\
	\end{tblr}
\end{center}
It is the multiplicative side which brings the most complexity.
The action of partitioning the context -- called splitting -- implies an exponential number of attempts to find which subset of the multiset is right for a certain branch.

Linear logic defined as of right now, albeit having the added complexity of splitting, is nonetheless decidable: since formulae are finite and they cannot be copied, it is possible to explore all the possibilities.
To make linear logic as strong as classical logic two new connectives are added: $\llbang{\phi}$ and $\llwn{\phi}$ -- called respectively bang and why-not.
% Linear logic defined as of right now is decidable, since formulas cannot grow in size we can explore each possibility.
% To be as strong as classical logic, there are two so called exponentials: bang or $!\phi$ and why not or $?\phi$.
% These have the purpose of localizing the uses of weakening and contraction:
These are called exponentials and their purpose is to localize uses of contraction and weakening.
For example, formulas marked with $!$ can be used any number of times. %, so the intuistic implication $a \rightarrow b$ is translated as $!(a \lolli b)$, and transitions in a petri net are represented by $!(resources_1 \lolli resources_2)$.

% Linear logic can be used to ensure that objects are used exactly once, thus allowing the system to safely deallocate an object after its use.
% The Haskell's compiler GHC has an experimental extensions to permit signatures with linear types.

% \subsection{Linear logic in practice}
% utilizzi logica lineare
%   * pi-calcolo
%   * risorse
%   * linear-haskell?
% A good example of linear logic may be chemical reactions % https://www.cs.cmu.edu/~crary/317-f22/lectures/20-linear.pdf
% Here we can see a reaction as an implication, if we have the reagents we can consume them to obtain the products.

% Petri nets can be encoded in linear logic, for example, 
% https://johnwickerson.github.io/talks/linearlogic.pdf

\section{State of the art}
Most forward provers for classic linear logic use some combination of focusing and normalization to structure their proofs, with the notable exception of llprover not using normalization.	% credo
We confront our prover with two other provers: llprover (1997, %cit
) and APLL (circa 2019, %cit
).

Usually the splitting is handled in two ways: trying every partition possible, or using something called the method of input/output %cit
. The latter tries to do one branch of the proof of a multiplicative, and then feeds the remaining formulae in the sequent of the other branch.

We now give a deeper look at the provers we confront with.

\subsection{APLL}\label{sec:apll}
APLL is the underlying prover of click\&collect. % cit
It provides 4 different searches -- forward and backwards for classic and intuitionistic linear logic. 
We will focus on the backwards algorithm for classic linear logic.

Before diving into the analysis of this prover, we give the definition of \textit{why-not height}
\begin{define}[Why-not height]
	\label{def:why-not-height}
	Why-not height is the maximum number of nested ``why-not''s in a formlua, or
	$$ \text{wnh}(\phi) = 
	\begin{cases}	
		0 & \text{if }\phi \in \{\llbot, \lltop, \llone, \llzero\} \\
		\max(\text{wnh}(\phi_1), \text{wnh}(\phi_2)) & \text{if } \phi \in \{ \phi_1 \llten \phi_2, \phi_1 \llpar \phi_2, \phi_1 \llplus \phi_2, \phi_1 \llwith \phi_2 \} \\
		\text{wnh}(\phi_1) & \text{if }\phi \in \{ \llnot{\phi_1}, \llbang{\phi_1}\} \\
		1 + \text{wnh}(\phi_1) & \text{if }\phi \in \{ \llwn{\phi_1} \} 
	\end{cases}
	$$
\end{define}
This measure is used in their particular way of dealing with uncontrained formulae, but is also used as a way to decide which branch to prove first for any normal operator.

The program is written in OCaML and implements a standard focused proof search on normalized formulae as seen in \cite{LiangMiller}.
In this section we will illustrate two noteworthy charateristics of its implementation:
\begin{itemize}
	\item Sequent splitting when encountering a tensor is done by generating all the numbers up to $2^{|\Delta|}$ -- where $\Delta$ is the sequent -- and using the bit representation of those to create the two subsets.
		This can be seen in the function \texttt{split\_list}, which in turn calls \texttt{split\_list\_aux}
		\begin{lstlisting}[language=caml]
let rec split_list_aux (acc1, acc2) l k = match l with
  | [] -> acc1, acc2
  | hd :: tl -> 
      if k mod 2 = 0 
      then split_list_aux (acc1, hd :: acc2) tl (k / 2)
      else split_list_aux (hd :: acc1, acc2) tl (k / 2)
		\end{lstlisting}
		where the argument \texttt{k} is the number that determines the decomposition of the sequent.
		This function is called recursively when a tensor is encountered during proof search, starting at $ k = 2^{|\Delta|}$ and decreasing by one at each iteration
		\begin{lstlisting}[language=caml]
(* ... *)
| Tensor (g, h) ->
  let rec split_gamma k = 
    if k = -1 then None
    else
      let gamma1, gamma2 = split_list gamma k in
        try
	  (* ... *)
        with NoValue ->
          split_gamma (k - 1) 
  in
    let k = fast_exp_2 (List.length gamma) - 1 in
      (* ... *)
		\end{lstlisting}

		As we will see in \ref{sec:benchmarking} this implementation choice will result in a degradation of performance on formulae with a high number of multiplicatives.
	\item This prover does not use a simple limit to the number of applications of the contraction rule in a branch, instead an initially empty queue of unrestricted formulae (\texttt{select\_d2}) and a counter (\texttt{max\_d2}) are kept during the search.
		Two cases arise:
		\begin{itemize}
			\item if \texttt{select\_d2 = []} and \texttt{max\_d2 > 0} then the sequent of unrestricted formulae is taken, negative terms are filtered out and it is sorted based on why-not height.	% cambio persona
				\begin{lstlisting}[language=caml]
(* ... *)
if select_d2 = [] then begin
  (if max_d2 = 0 then (bl := true; raise NoValue));                    
  let select_d2' = 
    sort_whynot (List.filter (fun x -> not (is_neg x)) 
    (Set_formula.elements theta)) in
  if select_d2' = [] then None
  else
    apply_d2 select_d2' (max_d2 - 1) end
    (* ... *)
				\end{lstlisting}
				This new list of unrestricted formule becomes the new \texttt{select\_d2}.
				Otherwise if \texttt{select\_d2} is still empty after being refilled (line 7) or if \texttt{max\_d2} is 0 (line 3) the branch fails.
			\item if \texttt{select\_d2} is not empty, then the first formula in the queue is extracted and added to the working set.	% sistemo
				If the branch fails the formula gets discarded and the next one in the queue is tried.	% sistemo
		\end{itemize}
		The main purpose of this whole process is to avoid infinite loops that always contract on the same formula.
		Instead all the formulae are tried one by one, starting from the simplest (lower why-not height).

		The counter \texttt{max\_d2} is a local bound, since decreasing it in a branch does not affect other branches.
\end{itemize}

\subsection{\texttt{llprover}}
\texttt{llprover} is a prover by Naoyuki Tamura.
Where APLL had different provers for implicative and classical linear logic, this prover encodes all the rules as the same predicate \texttt{rule/6}, using the first argument as a selector for the system.
Using classical logic as the system uses all the rules, included the ones for implicative linear logic.
For this reason the prover does not implement normalization.

Another particular characteristic of llprover is that it uses a local bound with iterative deepening, so in the benchmarks for formulae which need a lot of contractions, it will perform slightly worse.

\section{Why Prolog}
Prolog as a language and as an environment has been historically tied to automated theorem proving for its ability to express these kind of algorithmns naturally.
% In particular we chose SWI-Prolog because it offers a comprehensive and mature free Prolog environment.
One particularly conventient characteristic of Prolog is its automatic managing of backtracking, in most other languages we would have had to use exceptions to walk down the stack, or a queue of unfinished computations, which would have made the code much less readable.

Most Prolog implementations also support CLP or constraint logic programming.
This allows to have constraints referencing some attributes of variables in the body of clauses, in our case we use CLP($\mathcal{B}$) for boolean constraints and an handy interface to a sat-solver.
The library exposes operators to compose boolean formulas made of prolog variables
\begin{lstlisting}[language=prolog, numbers=none]
X = (X =:= 1),
Y = (X * X)
\end{lstlisting}
and to check the satisfaiability of said formulas
\begin{lstlisting}[language=prolog, numbers=none]
?- sat(X * Y).
X = Y, Y = 1.
\end{lstlisting}

One other characteristic of Prolog which revealed to be very handy for our prover is unification.
Using this we didn't have to explicitly propagate the solutions of the SAT-solver, which instead were automatically propagated between branches.

\chapter{The focused calculus}
Before describing the calculus we must give some definitions

% Explaination on constraints why we use them ecc ecc ecc

\begin{define}[Annotated formula]
	\label{def:annotated}
	Given a formula $\phi$ defined as in Figure \ref{fig:ll-connectives} and a boolean expression
	$e$ defined as in Figure \ref{fig:var-name}, an \textit{annotated formula} is simply a term 
	$$ \text{af}(\phi, e) $$
	that associates the formula to the expression.
	We denote 
	$$ \text{exp}(\af{\phi}{e}) = e $$
	and then extend this notation to sequents, such that $ \text{exp}(\Delta) $ is the set of all boolean expressions of $\Delta$.
\end{define}
As seen in Figure \ref{fig:ll-connectives} and \ref{fig:var-name} we will usually use $\phi$ to refer to formulas, $x$ to refer to boolean variables, and $e$ to refer to boolean expressions (conjuctions of variables).

The purpose of putting formulae and variables together in the annotated formula is twofold:
\begin{itemize}
	\item the actions taken on the formula determine the constraints that will be generated, and these depend on the variables associated to said formula;
	\item after the constraints are solved we can query the assignement of the variables and find out if the associated formula is used or not in a certain branch of a proof.
\end{itemize}
\begin{figure}[H]
	\centering
	\begin{tblr}{ colspec = {cccccccccr}
		    , cells = { mode = math } 
		    % , vborder{1-4} = { leftspace = 0pt, rightspace = 0pt } 
		    }
		\phi & ::=  & 1             &\mid& \phi \llten \phi  &\mid& \bot &\mid& \phi \llpar \phi  & \text{(Multiplicatives and their constants)} \\
		     & \mid & 0             &\mid& \phi \llplus \phi &\mid& \top &\mid& \phi \llwith \phi & \text{(Additives and their constants)} \\
		     & \mid & \llbang{\phi} &\mid& \llwn{\phi}       &    &      &    &                   & \text{(Exponentials)} \\
		     & \mid & \llnot{\phi}  &\mid& \text{name} 
	\end{tblr}
	\caption{Linear logic connectives}
	\label{fig:ll-connectives}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{tblr}{ colspec = {cccccr}, cells = { mode = math } }
		x & ::=  & x_i &\mid& \overline{x_i} & \text{(Variable)}\\
		e & ::=  & x \wedge e    &\mid& x & \text{(Expression)} \\
	\end{tblr}
	\caption{Definition of a boolean variable and expression}
	\label{fig:var-name}
\end{figure}

\begin{define}[Consuming formulae]
	\label{def:used}
	Given a boolean expression $e$ as in Figure \ref{fig:var-name}, we use the following notation
	\begin{itemize}
		\item ``$\used{e}$'' to state that the formula associated to the boolean expression $e$ gets consumed in this branch of the proof,
			this corresponds to saying the constraint $e$ is true;
		\item ``$\notUsed{e}$'' to state that the formula associated to the boolean variable $e$ does not get consumed in this branch of the proof,
			this corresponds to saying the constraint $e$ is not true.
	\end{itemize}
	We then extends these predicates to sequents
	\begin{align*}
		\used{\Delta} &= \{ \used{e_2} \mid e_2 \in \text{exp}(\Delta) \} \\
		\notUsed{\Delta} &= \{ \notUsed{e_2} \mid e_2 \in \text{exp}(\Delta) \}
	\end{align*}
\end{define}

\begin{define}[Members of the sequent]
	Given any sequent this can be in either two forms:
	\begin{itemize}
		\item focused or in the synchronous phase, written:
			$$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\phi}$$
		\item in the asynchronous phase, written:
			$$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\Phi}$$
	\end{itemize}
	These two have more or less the same members, which are
	\begin{itemize}
		\item the set $\Psi$ of unrestricted formulae, or all formulae that can be freely discarded or duplicated;
		\item the multisets $\Delta$ and $\Phi$ of linear (annotated) formulae, these are respectively the formulas ``put to the side'' and the formulae which are being ``worked on'' during a certain moment of the asynchronous phase;
		\item the set $\Lambda$ of constraints, which is to be interpreted as the conjunction of its members, and the set $V$ which represents a propagated solution.
			Furthermore if $\Lambda$ is satisfiable we write $\sat{\Lambda}{V}$, so:
				$$ \sat{\Lambda}{V} \;\Longleftrightarrow\; \bigwedge_{e \in \Lambda} e \text{ is \textit{sat} by } V $$
			$V$ is in and of itself a boolean expression as in Figure \ref{fig:var-name}, in fact it can be seen as the conjunction of the variables assigned to true and the negation of the variables assigned to false.
			As such it can be used as a constraint, stating that a certain solution must be respected in a new one.
			
			This approach to constraints helps to make the flow of the variables and solutions through the proof tree more explicit and clear and leaves no ambiguity to where the constraints should be checked.

			The choice of letters is mainly a mnemonic or visual one, constraints $\Lambda$ ``go-up'' the proof tree and solutions $V$ ``come down'' from the leaves.
	\end{itemize}
\end{define}

\begin{define}[Splitting a sequent]
	\label{def:split}
	Given a sequent of annotated formulae $\Delta$ we define the operation of splitting it as a function
	$$ \text{split}(\Delta) \mapsto (\Delta^L, \Delta^R) $$
	where, given a set $X$ of new variable names $x_i$ for each formula $\phi_i \in \Delta$
	\begin{align*}
		\Delta^L &= \{ \af{\phi_i}{x_i \wedge e_i} \mid i \in \{1, \dots, n\}\} \\
		\Delta^R &= \{ \af{\phi_i}{\varNot{x_i} \wedge e_i} \mid i \in \{1, \dots, n\}\} \\
	\end{align*}
	with $n$ the cardinality of $\Delta$, and $\phi_i$ and $e_i$ respectively the formula and the variable of the $i$-eth annotated formula in $\Delta$ using an arbitrary order.

	It is worth noting that the variables $x_i \in X$ used for $\Delta^L$ and $\Delta^R$ must be the same, this condition is necessary for the mechanism ensuring that a formula used on the left side of a tensor proof is not used on the right side and viceversa.

	With a slight abuse of notation we will write $\text{split}(\Delta)_L$ and $\text{split}(\Delta)_R$ respectively as the left projection and the right projection of the pair $(\Delta_L, \Delta_R)$.
\end{define}
As a small example for clarity, given the sequent
$$ \Delta = \af{a \llten b}{x_1}, \af{\llnot{c}}{x_2} $$
this is split into
\begin{align*}
	\text{split}(\Delta)_L &\mapsto \af{a \llten b}{x_3 \varAnd x_1}, \af{\llnot{c}}{x_4 \varAnd x_2} \\
	\text{split}(\Delta)_R &\mapsto \af{a \llten b}{\varNot{x_3} \varAnd x_1}, \af{\llnot{c}}{\varNot{x_4} \varAnd x_2} 
\end{align*}
% example of splitting and variables

% One simple but important detail that will be useful later when explaining the Prolog implementation is noting that the variables in common between two branches with the same root are always introduced before the two branches diverge.
% Or -- put differently -- all new variables introduced in any point of a path from the root of the proof to a leaf may appear only in the subtrees.	% sistemo
\begin{define}
	Lastly given a formula $\phi$ we define the following predicates
	\begin{itemize}
		\item $\isAsy{\phi}$ is a predicate that's true only when $\phi$ is an asynchronous formula, which are
			$$ \phi ::= \phi \llpar \phi \mid \phi \llwith \phi \mid \llwn{\phi} \mid  \lltop \mid \llbot $$
		\item $\isNegLit{\phi}$ is a predicate that's true only when $\phi$ is a negative literal, in our implementation negative literals are atoms, and positive literals are negated atoms.
	\end{itemize}
\end{define}

We are now ready to present the full calculus.
\begin{figure}[H]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{ colspec = { cc }, rows = {abovesep=10pt, belowsep=10pt}}
			\SetCell[c=2]{c} {\small
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1}{e}, \af{\phi_2}{e}, \Phi}$}
			\LeftLabel{$[\llpar]$}
			\UnaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1 \llpar \phi_2}{e}, \Phi}$}
			\DisplayProof} \\
			{\small
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\Phi}$}
			\LeftLabel{$[\llbot]$}
			\UnaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\llbot}{e}, \Phi}$}
			\DisplayProof}
			&
			{\small
			\AxiomC{}
			\LeftLabel{$[\lltop]$}
			\UnaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\lltop}{-}, \Phi}$}
			\DisplayProof
			}
			\\
			\SetCell[c=2]{c} {\small
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{V'}}{\Psi}{\Delta}{\af{\phi_2}{e}, \Phi}$}
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{V''}}{\Psi}{\Delta}{\af{\phi_1}{e}, \Phi}$}
			\LeftLabel{$[\llwith]$}
			\BinaryInfC{$\async{\constr{\Lambda}{V', V''}}{\Psi}{\Delta}{\af{\phi_1 \llwith \phi_2}{e}, \Phi}$}	% capisco cosa fanno qui i constraint
			\DisplayProof}
			\\
			\SetCell[c=2]{c} {\small
			\AxiomC{$\async{\constr{\Lambda}{V}}{\Psi, \phi}{\Delta}{\Phi}$}
			\LeftLabel{$[\,?\,]$}
			\UnaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\llwn{\phi}}{-}, \Phi}$}
			\DisplayProof} 
			\\
			\SetCell[c=2]{c} {\small
			\AxiomC{$\isNotAsy{\phi}$}
			\AxiomC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta, \af{\phi}{e}}{\Phi}$}
			\LeftLabel{$[R\!\Uparrow]$}
			\BinaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}, \Phi}$}
			\DisplayProof
			}
		\end{tblr}
		\caption{Asynchronous rules}
	\end{subfigure}

	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{colspec = { cc }, rows = {abovesep=10pt, belowsep=10pt}}
			\SetCell[c=2]{c} {\small
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{V'}}{\Psi}{\text{split}(\Delta)^L}{\text{af}(\phi_1, e)}$}
			\AxiomC{$\focus{\constr{V'}{V''}}{\Psi}{\text{split}(\Delta)^R}{\text{af}(\phi_2, e)}$}
			\LeftLabel{$[\llten]$}
			\BinaryInfC{$\focus{\constr{\Lambda}{V''}}{\Psi}{\Delta}{\text{af}(\phi_1 \llten \phi_2, e)}$}	% capisco il movimento dei constraint
			\DisplayProof}
			\\ 
			{\small
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1}{e}} $}
			\LeftLabel{$[\llplus_L]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}}$}
			\DisplayProof}
			&
			{\small
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi_2}{e}}$}
			\LeftLabel{$[\llplus_R]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}}$}
			\DisplayProof}
			\\
			{\small
			\AxiomC{$\sat{ \used{e_1}, \notUsed{\Delta}, \Lambda}{V}$}
			\LeftLabel{$[1]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\llone}{e_1}}$}
			\DisplayProof} 
			&
			{\small
			\AxiomC{$\focus{\constr{\used{e_1}, \notUsed{\Delta}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e_1}}$}
			\LeftLabel{$[\,!\,]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\llbang{\phi}}{e_1}}$}
			\DisplayProof
			}
			\\
			\SetCell[c=2]{c} {\small
			\AxiomC{$\isAsy{\phi} \vee \isNegLit{\phi}$}
			\AxiomC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\LeftLabel{$[R\!\Downarrow]$}
			\BinaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\DisplayProof
			}
		\end{tblr}
		\caption{Synchronous rules}
	\end{subfigure}

	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{ colspec = { c }
			    , rows = {abovesep=10pt, belowsep=10pt}
			    , vborder{1-2} = { leftspace = -10pt, rightspace = -10pt } 
			    }
			{\small
			\AxiomC{$ \sat{\used{e_1}, \used{e_2}, \notUsed{\Delta}, \Lambda}{V}$}
			\LeftLabel{$[I_1]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta, \af{\phi}{e_2}}{\af{\llnot{\phi}}{e_1}}$}
			\DisplayProof}
			\\
			{\small
			\AxiomC{$ \sat{\used{e_1}, \notUsed{\Delta}, \Lambda}{V}$}
			\LeftLabel{$[I_2]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{V}}{\Psi, \phi}{\Delta}{\af{\llnot{\phi}}{e_1}}$}
			\DisplayProof}
		\end{tblr}
		\caption{Identity rules}
	\end{subfigure}

	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{ colspec = { c }
			    , rows = {abovesep=10pt, belowsep=10pt}
			    , vborder{1-2} = { leftspace = -10pt, rightspace = -10pt } 
			    }
			{\small
			\AxiomC{$\isNotNegLit{\phi}$}
			\AxiomC{$\focus{\constr{\Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\LeftLabel{$[D_1]$}
			\BinaryInfC{$\async{\constr{\Lambda}{V}}{\Psi}{\Delta, \af{\phi}{e}}{.}$}
			\DisplayProof}
			\\
			{\small
			\AxiomC{$\isNotNegLit{\phi}$}
			\AxiomC{$\new{e}$}
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{V}}{\Psi}{\Delta}{\af{\phi}{e}} $}
			\LeftLabel{$[D_2]$}
			\TrinaryInfC{$\async{\constr{\Lambda}{V}}{\Psi, \phi}{\Delta}{.}$}
			\DisplayProof}
		\end{tblr}
		\caption{Decide rules}
	\end{subfigure}
	\caption{The complete focused constraint calculus}
	\label{fig:calculus}
\end{figure}

\chapter{Implementation}

\section{Formula transformations}
Before beginning the proof a sequent passes through a number of transformations.
These transformations both preprocess the sequent to a more convenient form, and also add information about the subformulae.

As a first transformation the sequent gets normalized into a sequent in negated normal form (NNF).
NNF is the form where all negations are pushed down to atoms and all linear implications ($\lolli$) are expanded into pars ($\llpar$) using the following tautology
$$ a \lolli b \Leftrightarrow \llnot{a} \llpar b $$
Normalization is a common technique -- used in all the provers we compare with.
The process is composed of just two steps
\begin{enumerate}
	\item the left sequent is negated and appended to the right sequent, implemented by the predicate \texttt{negate\_premises/3};
	\item the predicate \texttt{nnf/2}, which encodes the DeMorgan rules, is mapped recursively over the new sequent
\end{enumerate}
This is possible since classic linear logic is symmetric and negation is involutive.

The purpose of this process is to cut away a great deal of possible rules applicable to the sequent, sacrificing some of the structure of the sequent.
In fact the number of rules we need to implement after normalization is more than halved, since we now need just the right rules, without the ones for negation and linear implication.

As a second transformation, to each formula we assign its why-not height, as defined in Definition \ref{def:why-not-height}.
Why-not height is used during proof search to decide which branch to do first and which exponential to decide first.
This is a technique borrowed from APLL and its obvious purpose is to try first the branches with less exponentials: in the case the first probably simpler branch fails, we do not have to try the other.
After this transformation formulae are attribute trees, with at each node the why not height of the subformula.

As a third and final transformation, each formula gets annotated.
This means we associate a variable to each formula in the sequent as in Definition \ref{def:annotated}.
Given a sequent $\Delta$ we obtain
$$ \hat{\Delta} = \{ \af{\phi}{x} \mid \new{x}, \phi \in \Delta \} $$
To be clear, a variable is only assigned to the ``top-level'' formula, and subformulae are left unchanged.

In the implementation the concept of variable is split in two: the name of the variable -- represented by a Prolog atom, and the value of the variable -- represented by a Prolog variable.
This is needed since, after checking the constraints, the SAT-solver unifies the variable to its value if it finds a satisfaiable solution, so the purpose of the atom is to associate the variable value to its name if the final proof.
The process of annotation is implemented by the predicate \texttt{annotate/3}
\begin{lstlisting}[language=prolog]
%! annotate(+[Formulae], -[AFs], -[Constraints]) is det.
annotate(Fs, Afs, Cns) :-
  maplist([ F
          , af(F, X, Var)
	  , v(Var) =:= 1
	  ]>>(gensym(x, X)), Fs, Afs, Cns).
\end{lstlisting}
which is a simple map over the sequent.
The constraints returned state that each formula must have its variable to one, that is to say each formulae must be used.
These are the constraints the proof search starts with.

\section{Constraint propagation}
Some care is to be given to explaining how the constraints propagate.
In fact, in constrast to Figure \ref{fig:calculus} the implementation does not have explicit propagation of the solution to the constraints.
This is because Prolog's unification implicitly propagates a solution from one branch to another.
% continuo?

\section{Helper predicates}\label{sec:helper}
We now define some helper predicates to work with the constraints.
What we defined as $\notUsed{\Delta}$ in Definition \ref{def:used} corresponds to the predicate \texttt{set\_to\_zero/2}
\begin{lstlisting}[language=prolog, numbers=none]
%! set_to_zero(+[AFs], -[Constr]) is det.
set_to_zero(Fs, Cns) :-
    maplist([af(_, _, E), v(E) =:= 0]>>true, Fs, Cns).
\end{lstlisting}

The other helper predicate implements the split function defined in Definition \ref{def:split}.
\begin{lstlisting}[language=prolog, numbers=none]
%! split_ctx(+[AFs], -[AFs], -[AFs], -[Cns], -[Cns]) is det.
split_ctx(Afs, Pos, Neg, PCns, NCns) :-
  maplist([ af(F, N, E)
          , af(F, VarPos, Y)
  	  , af(F, VarNeg, Z)
  	  , v(Y) =:= v(X) * v(E)
  	  , v(Z) =:= (~ v(X)) * v(E)
  	  ]>>(
  	    gensym(x, V),
  	    atomic_list_concat([N, V], '.', VarPos),
  	    atomic_list_concat([N, V], '.~', VarNeg)
  ), Afs, Pos, Neg, PCns, NCns).
\end{lstlisting}
It is again a map over the list of formulae, that generates the new formulae and the constraints accordingly.
Three new Prolog variables are introduced: \texttt{X}, \texttt{Y} and \texttt{Z}.
\texttt{X} is the new variable, the annotated formulae refer to the variable \texttt{Y} and \texttt{Z}, and constraints are added so that
\begin{align*}
	y &= x \varAnd e \\
	y &= \varNot{x} \varAnd e \\
\end{align*}
Compare this to the original definition of Definition \ref{def:split}, one can see that the two are basically identical other than the fact that here the name of the variable (the atom) and its value are treated separately.

\section{Focusing}
When explaining the code we will use some common names for variables, these are
\begin{itemize}
	\item \texttt{A} is a set of unrestricted atoms;
	\item \texttt{U} is a set of unrestriched formulae;
	\item \texttt{F}, \texttt{F1}, ..., are formulae, and \texttt{Fs} and \texttt{D} are a lists of them;
	\item \texttt{S} is the queue of currently usable unrestricted formulae;
	\item \texttt{In} is a list of constraints.
\end{itemize}
\subsection{Asynchronous and focusing phase}
During the asynchronous phase we have a list of formulae which are being worked on and a list of formulae which are put to the side.
With the former being called \texttt{Fss} and the latter \texttt{D}.
At each step we analyze the first element of the list \texttt{Fs}, and we keep scomposing the memebers of the list untill we can't anymore.
This process can be seen for example in the predicate for $\llwith$
\begin{lstlisting}[language=prolog]
async(A, U, D, [F|Fs], S, M, In, _) :-
  F = af(((F1-H1) & (F2-H2))-_), N, E), !,
  ( H2 > H1	
  -> async(A, U, D, [af((F1-H1), N, E)|Fs], S, M, [v(E) =:= 1|In], _), 
     async(A, U, D, [af((F2-H2), N, E)|Fs], S, M, [v(E) =:= 1|In], _) 
  ;  async(A, U, D, [af((F2-H2), N, E)|Fs], S, M, [v(E) =:= 1|In], _),
     async(A, U, D, [af((F1-H1), N, E)|Fs], S, M, [v(E) =:= 1|In], _)
  ).
\end{lstlisting}
Here we can see both the choice being made based on the why-not height of the two subformulae, and how the with is scomposed.
Compare this with the $\llwith$ rule in Figure \ref{fig:calculus}.
The cut at line 2 represents the main concept of the asynchronous phase: if an asynchronous connective is encountered the only thing we ought to do is to scompose it.

If a formula cannot be further be broken apart -- i.e. it is either an atom, a negated atom, or it has a toplevel synchronous connective -- then it is put to the side in \texttt{D}.
This can be seen in the rule \texttt{to\_delta} which implements the rule $R\!\Uparrow$
\begin{lstlisting}[language=prolog]
async(A, U, D, [F|Fs], S, M, In, _) :-
  F = af((F1-_), _, _),
  not(is_asy(F1)), !,
  async(A, U, [F|D], Fs, S, M, In, _).
\end{lstlisting}
This process goes as long as \texttt{Fs} has still formulae inside.

When \texttt{Fs} is empty the phase switches, and the focusing process begins: we choose a formula -- called \texttt{decide} -- from either \texttt{D} or \texttt{U} and we scompose it untill either an asynchronous connective is left or a negated atom.
This is represented by the rules $D_1$ and $D_2$ that will be discussed further ahead in Section \ref{sec:decide}.

\subsection{Identity rules}
This process of alternating asynchronous and synchronous phases in classic focusing goes on untill we have a only positive literal (in our case a negated atom) in \texttt{Fs} and the corresponding negative literal (in our case just an atom) in either \texttt{U} or \texttt{D}.
When this happens the axioms -- rules $I_1$ or $I_2$ -- are applied to close the branch.
In our case when we are focusing and we have a positive literal in \texttt{Fs}, we check if the corresponding negative literal exists in \texttt{D}.
If this is true, then the variables of all the other formulae in \texttt{D} are set to zero using the predicate \texttt{set\_to\_zero/2} defined in Section \ref{sec:helper}, and the constraints are checked.
This is encoded in the clause 
\begin{lstlisting}[language=prolog]
focus(A, U, D, F, _, _, In, node(id_1, A, U, D, F, [])) :-
  F = af(((~ T)-_), _, E1),
  is_term(T),
  select(af((T-_), _, E2), D, D1),
  set_to_zero(D1, Dz),
  append([v(E1) =:= 1, v(E2) =:= 1|Dz], In, Cns),
  check(Cns).
\end{lstlisting}
A slightly different process happens if instead a correspondence is found in \texttt{A} instead of \texttt{D}.
Here \texttt{A} is a special set containing just unrestricted atoms.
This is a slight modification to APLL approach based on the fact that once negative literals are put in a sequent they can never leave it, and is due to the fact that since \texttt{U} may be sorted many times, we try to keep the number of formulae in it small.

\subsection{Decide rules}\label{sec:decide}
For the decide rules, particularly for $D_2$, we use a slight modification to APLL algorithm defined in Section \ref{sec:apll}.
Like APLL we keep a queue of ordered unrestricted formulae which can be refilled only a certain number of times per-branch.
This can be seen in the definition of the rule \texttt{decide\_2}
\begin{lstlisting}
async(A, U, D, [], [H|T], M, In, _) :-
  \+ U = [],
  gensym(x, X),
  focus(A, U, D, af(H, X, E), T, M, [v(E) =:= 1|In], _).
async(A, U, D, [], [_|T], M, In, _) :-
  \+ U = [],
  async(A, U, D, [], T, M, In, _).
async(A, U, D, [], [], M, In, _) :-
  \+ U = [],
  refill(U, M, S, M1),
  early_stop(A, U, D, S, M1, In, _).
\end{lstlisting}
In particular, if the queue \texttt{S} is refilled, we do not directly call \texttt{async/8}, but instead call another predicate: \texttt{early\_stop/7} (line 11).
\begin{lstlisting}[language=prolog, numbers=none]
early_stop(_, _, _, [], _, _, _) :-
  false.
early_stop(A, U, D, [H|T], M, In, _) :-
  gensym(x, X),
  focus(A, U, D, af(H, X, E), T, M, [v(E) =:= 1|In], _).
early_stop(A, U, D, [_|T], M, In, _) :-
  early_stop(A, U, D, T, M, In, _).
\end{lstlisting}
This is due the simple fact that if the branch was not provable and we instead called directly \texttt{async/8} at line 11, we would try to refill the branch \texttt{M} times.
What early stop does is fail if the queue has just been refilled and it turns out the branch was not provable.

All the rules $D_1$, $I_1$ are put before the unrestricted counterparts, so that they are tried first.

\section{Building the tree}
In the listings above we omitted one parameter of the predicates, which purpose is to build the proof tree.
At each call of async and focus one node of the proof tree is built.
This contains the context of the call.
For example in
\begin{lstlisting}[language=prolog, numbers=none]
async(A, U, D, [F|Fs], S, M, In, node(par, A, U, D, [F|Fs], [Tree])) :- 
  F = af(((F1 / F2)-_), N, E), !,
  Fs1 = [af(F1, N, E), af(F2, N, E)|Fs],
  async(A, U, D, Fs1, S, M, [v(E) =:= 1|In], Tree).
\end{lstlisting}
we can see clearly the structure of the node: a label, the context and an -- optionally empty -- list of subtrees.
A leaf is just a node with an empty list of subtrees.

This tree can be used in the end to reconstruct the actual proof tree, by visiting it and -- for each formula -- querying whether its variable is set of one, and cancelling it otherwise.
A more sophisticated algorithm may even cancel out unwanted unrestricted formulae, that otherwise remain lingering in the sequent.

\chapter{Testing}
!!! VERY MUCH STILL WIP !!!
\section{Infrastructure}
\subsection{Reproducibility}
The prover's tests and benchmarks are made using a jupyter notebook.
To ensure reproducibility we use Nix, which is a build system based on reproducile and declarative recipes, called \textit{derivations}.
A docker image is also given, which in turn calls nix without the need to install it on the system.
Furthermore for the project infrastructure we used nix flakes, which are an experimental feature of nix that makes the process more hermetic and pure.

\subsection{Prefix format}
Since for benchmarking we will interface with a lot of different provers, each with its own syntax for expressions, the need for a common format which was easy to parse and translate arose.
For this purpose we define use a prefix format for linear logic formulae inspired by the format used by \cite{TarauPaiva} for implicational formulae
\begin{table}[H]
	\centering
	\begin{tblr}{ colspec = {cc} }
		\hline
			formula & symbol \\
		\hline
		\hline
			$\phi_A \llten \phi_B$  & \texttt{*AB} \\
			$\phi_A \llpar \phi_B$  & \texttt{|AB} \\
			$\phi_A \llplus \phi_B$ & \texttt{+AB} \\
			$\phi_A \llwith \phi_B$ & \texttt{\&AB} \\
			$\phi_A \lolli \phi_B$  & \texttt{@AB} \\
			$\llnot{\phi_A}$        & \texttt{\^{}A} \\
			$\llwn{\phi_A}$         & \texttt{?A} \\
			$\llbang{\phi_A}$       & \texttt{!A} \\
	\end{tblr}
\end{table}
Furthermore each single character not representing an operator is considered as a variable name.
Longer names can be specified by enclosing them in single apices as in \texttt{'varname'}.
As an example we give the translation of DeMorgan for the tensor:
$$ \text{trans}(\llnot{(a \llten b)} \lolli \llnot{a} \llpar \llnot{b}) = \text{\texttt{@\^{}ab|\^{}a\^{}b}} $$

% Since most one of the datasets we use is LLTP % cite
% , which is written in TPTP's format % cite
% , we define a parser for this using Haskell's parser generator happy. % cite

\subsection{File formats}\label{sec:file_format}
As a standard format to store the tests we use json because of its vast adoption by most programming languages.
A test suite is defined as a list of test cases
\begin{lstlisting}[numbers=none]
TestCase ::= {
  "id": <Number>,
  "premises": [ <PrefixFormula>+ ],
  "conclusions": [ <PrefixFormula> ]
}
\end{lstlisting}
where
\begin{description}
	\item[id] \hfill \\ is a number with the sole purpose of tracing back the test case from the output;
	\item[premises] \hfill \\ is a list of premises as prefix formulae
	\item[conclusions] \hfill \\ a list of conclusions as prefix formulae
\end{description}
Other arbitrary fields may be present, for example we will use the following optional fields:
\begin{description}
	\item[thm] \hfill \\ whether this test case is a tautology or not, may be null;
	\item[*, \&, +, ...] \hfill \\ the number of times a specific appears in the test case;
	\item[notes] \hfill \\ human readable text about the test case, for example its infix representation;
	\item[size] \hfill \\ an indicative number of the size of the formula;
	\item[atoms] \hfill \\ the upper bound on the number of atoms.
\end{description}

\subsection{Formula generation}\label{sec:formula_generator}
One of the sources of formulae we'll use in Section \ref{sec:benchmarking} is APLL's random formula generator.
The version we'll use is a slight modification of it where:
\begin{itemize}
	\item the output is in the json format described in Section \ref{sec:file_format}
	\item one can choose to generate normalized formulae or not;
	\item one can choose which connectives appear in the generated formula.
\end{itemize}
A noteworthy detail is how the parameters \texttt{size} and \texttt{atoms} mentioned in Section \ref{sec:file_format} are defined, since these are directly related to how the formulae are generated:
\begin{itemize}
	\item when one specifies a number of atoms \texttt{atoms}, the generator initializes an array containing \texttt{atoms} atoms, their negations, and the constants $\bot, \top, \dots$.
		During the generation of the formula this array is randomly accessed, extracting a random element when needed.
		This means that \texttt{atoms} represents an upper bound to the number of different atoms that may appear in the formula, not their exact number.
	\item when a formula is generated at each step, based on a threshold, it is chosen whether to generate a unary or binary connective:
		\begin{itemize}
			\item if a unary connective is chosen, the process continues with a size of $\text{\texttt{size}} - 1$;
			\item if a binary connective is chosen, the program chooses a random value between 0 and \texttt{size}, and it generates the two branches of the formula, with size respectively $k$ and $\text{\texttt{size}} - k$.
		\end{itemize}
\end{itemize}

\section{Benchmarking}\label{sec:benchmarking}
% Finding a comprehensive test suite for linear logic, moreover one which is not made up mostly of translations of other theorems to linear logic, is not easy.
% Most of the theorems we use for testing are llprover's %cite
We'll mainly use three sources for formulae:
\begin{itemize}
	\item llprover's tests
	\item LLTP, especially the translations of Kleene's intuistic formulae 
	\item randomly generated formulae made by the genrator described in \ref{sec:formula_generator}
\end{itemize}
llprovers tests are composed mainly by simple linear logic tautologies, e.g. the DeMorgan rules, for this reason these tests are used more as a simple and fast suite to catch the most obvious bugs between iterations of the prover.

We now show the results of running the provers on two datasets: KLE-cbn and KLE-cbv, respectively the call-by-name and call-by-value translations of Kleene's theorems.
These translations introduce a high number of exponentials, and this causes, other than timeouts because of the added complexity, some failures.
These failures are not due to bugs, but instead happen because the prover has reached its bound for that formula.
The benchmarks are done using a timeout of \qty{60}{\second} and a bound of 3.
\begin{table}[H]
	\centering
	\begin{tblr}{ 
		colspec = { l c c c c c c },
		vlines = {2,3,4,5,7,8,9,10}{solid},
		cells = { preto = {\small} },
		% cells{2,7} = { preto = {\small}, cmd = {\textbf} }
		}
		\SetCell[c=7]{c} Kleene CBN \\
		\hline
		prover & timeouts & failures & successes & success rate & avg. time (succ.) & avg. time (tot.) \\
		\hline
		\hline
		APLL & 0 & 16 & 72 & $\approx 0.80$ & \qty{0.037}{\second} & \qty{0.055}{\second} \\
		llprover & 20 & 6 & 62 & $\approx 0.70$ & \qty{1.709}{\second} & \qty{3.253}{\second} \\
		sat-ll & 4 & 18 & 66 & $\approx 0.75$ & \qty{0.130}{\second} & \qty{0.185}{\second} \\
		\hline
		\SetCell[c=7]{c} Kleene CBV \\
		\hline
		prover & timeouts & failures & successes & success rate & avg. time (succ.) & avg. time (tot.) \\
		\hline
		\hline
		APLL & 0 & 17 & 71 & $\approx 0.80$ & \qty{0.035}{\second} & \qty{0.326}{\second} \\
		llprover & 20 & 6 & 62 & $\approx 0.70$ & \qty{0.981}{\second} & \qty{2.179}{\second} \\
		sat-ll & 5 & 15 & 68 & $\approx 0.77$ & \qty{0.443}{\second} & \qty{0.496}{\second} \\
		\hline
	\end{tblr}
\end{table}

There are mainly two downsides with using random formulae:
\begin{itemize}
	\item most cases do not hold any structure, 
	\item ??? % non ricordo pi√π cosa volessi scrivere
\end{itemize}
Using random formulae we can clearly see that our prover outperforms APLL (and llprover) with formulae rich in multiplicatives.
\begin{figure}[H]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[scale=0.5]{./images/mll.pdf}
		\caption{Multiplicative case}
		\label{fig:mll_bars}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[scale=0.5]{./images/mall.pdf}
		\caption{Multiplicative and additive case}
		\label{fig:mall_bars}
	\end{subfigure}
	\caption{Percentage of number of timeouts out of a hundred formulae}
\end{figure}
We can see that in the multiplicative and additive case the difference begin to level.
The additive case is not that significant as the formulae remain manageable and no major differences can be seen.

As soon as exponentials come into play the differences level out.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{./images/cll-500-250.pdf}
	\caption{Full linear logic}
	\label{fig:cll_bars}
\end{figure}
In full linear logic our prover performs slightly worse than APLL.

All these test were done generating suits of 100 tests.
Figure \ref{fig:mll_bars}'s tests were normalized formulae of size 100, and atoms 50, with just the connectives $\llten$ and $\llpar$.
Figure \ref{fig:mall_bars}'s tests were normalized formulae formulae of size 500, and atoms 250, with the connectives $\llten, \llpar, \llwith$ and $\llplus$.
Finally \ref{fig:cll_bars}'s tests were normalized formulae of size 500, and atoms 250, with all the connectives.

When looking at the results of full linear logic, it is important to note that unlike the tests with multiplicatives and additives, some of the results may be early failures because of the bound.
Since llprover uses incremental search, its times are often of the slowest.
On the other hand our prover is consistently slightly slower that APLL, this difference is negligible and due to the fact that APLL is compiled, whereas our prover is interpreted.

% \chapter{Conclusions}
% We showed that -- given appropriate modifications -- Harland\&Pym's method is competitive to other provers, and in some cases (see multiplicatives) may be even faster.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
