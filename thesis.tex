\documentclass[a4paper, 12pt, tesi, english]{report}

\usepackage[dbg]{thesis}

\title{Titolo}

\begin{document}
\maketitle
\newpage
\tableofcontents

\chapter{Intro}
In 2001 Pym and Harland publish a paper \cite{HarlandPym} where they propose a new way to tackle the problem of splitting sequents during linear logic proof search using boolean constraints.
The paper proposes a new calculus for linear logic that associates to each formula a boolean variable, and enforces linearity by constraints on said variables.
This way the complexity shifts from choosing the right set of formulas to prove a certain branch, to solving for boolean assignment -- a problem for which there are much more sophisticated algorithmns.

We examine the efficiency of this method and we compare it to other provers for different substets of linear logic.

% \section{Sequent calculus}
% We will often talk about sequents, a seuquent of the form
% $$ \Delta_1, \dots, \Delta_n \vdash \Gamma_1, \dots, \Gamma_m $$
% is another way of writing
% $$ \Delta_1 \wedge \dots \wedge \Delta_n \Rightarrow \Gamma_1 \vee \dots \vee \Gamma_m $$
% In sequent calculus we define some rules to manipulate these sequents, these rules are for example 
% $$
% \AxiomC{$\Gamma, \phi \vdash \Delta$}
% \AxiomC{$\Gamma, \psi \vdash \Delta$}
% \BinaryInfC{$\Gamma, \phi \vee \psi \vdash \Delta$}
% \DisplayProof
% $$
% means that if $\Gamma, \phi \vdash \Delta$ and $\Gamma, \psi \vdash \Delta$ hold, then $\Gamma, \phi \vee \psi \vdash \Delta$ holds.
% This for example is the classic rule for $\vee$.
% 
% When trying to build a proof bottom up, we utilize these rules inversed to try to arrive at what are called axioms or leafs, rules with no premises.
% $$
% \AxiomC{}
% \UnaryInfC{$\phi \vdash \phi$}
% \DisplayProof
% $$
% 
% Gentzen introduced the sequent calculus LK for classical logic, this had -- other that the usual rules -- three so called structural rules.
% These rules were used to manipulate the sequent itself, and are
% \begin{itemize}
% 	\item weakening: we can always ``weaken'' the sequent by adding a proposition without changing its truth,
% 		$$
% 		\AxiomC{$\Gamma \vdash \Delta$}
% 		\UnaryInfC{$\Gamma, \phi \vdash \Delta$}
% 		\DisplayProof
% 		$$
% 	\item contraction: we can always ``contract'' two copies of the same proposition into one without changing the truth of the sequent,
% 		$$
% 		\AxiomC{$\Gamma, \phi, \phi \vdash \Delta$}
% 		\UnaryInfC{$\Gamma, \phi \vdash \Delta$}
% 		\DisplayProof
% 		$$
% 	\item exchange: we can change position of two propositions in a sequent freely without changing its truth
% 		$$
% 		\AxiomC{$\Gamma, \phi, \psi \vdash \Delta$}
% 		\UnaryInfC{$\Gamma, \psi, \phi \vdash \Delta$}
% 		\DisplayProof
% 		$$
% \end{itemize}
% and their symmetric right rules.
% These structural rules will be important in the next section where we will introduce linear logic.

\section{Linear logic}
Linear logic is a logic proposed by Jean-Yves Girard in his seminal paper of 1987 \cite{LinearLogic}.
The distintive trait of this logic is that its formulae cannot be copied (called weakening) or discarded (called contraction), but instead they are consumed.
And a certain sequent it true if and only if all its formulae get consumed exactly once.
For this reason this logic is sometimes called a logic of resources, in the same way classical logic is a logic of truths and intuitionistic logic is a logic of proofs.
% Questo particolare utilizzo delle formule permette di avere una logica che mantiene la simmetria delle logica classica, e il costruttivismo delle logica intuizionista.

In linear logic each connective of classical logic is doubled.
To better see this let's analyze classic conjuction, this can be defined as 

$$
\begin{array}{cc}
\AxiomC{$\Delta \vdash \phi_2, \Gamma$}
\AxiomC{$\Delta \vdash \phi_1, \Gamma$}
\BinaryInfC{$\Delta \vdash \phi_1 \wedge \phi_2, \Gamma$}
\DisplayProof
	&
\AxiomC{$\Delta'' \vdash \phi_2, \Gamma''$}
\AxiomC{$\Delta' \vdash \phi_1, \Gamma'$}
\BinaryInfC{$\Delta', \Delta'' \vdash \phi_1 \wedge \phi_2, \Gamma', \Gamma''$}
\DisplayProof
\end{array}
$$

On the other hand, these two rules are not equivalent in linear logic, since the former implies some weakening and contraction.
This is exactly the reason why in linear logic all connectives have two versions: an additive one -- where the two branches keep the same context, and a multiplicative one -- where the context gets partioned between the two branches.
Obviously the constants $\top$ and $\bot$ also have two versions.
We have that
\begin{center}
	\begin{tblr}{ccc}
		\hline
		& Add. & Mult. \\
		\hline
		\hline
		$\wedge$ & $\llwith$ & $\llten$ \\
		$\vee$ & $\llplus$ & $\llpar$ \\
		$\top$ & $\top$ & $1$ \\
		$\bot$ & $0$ & $\bot$ \\
	\end{tblr}
\end{center}
It is the multiplicative side which brings the most complexity.
The action of partitioning the context -- called splitting -- implies an exponential number of attempts to find which subset of the multiset is right for a certain branch.

Linear logic defined as of right now, albeit having the added complexity of splitting, is nonetheless decidable: since formulae are finite and they cannot be copied, it is possible to explore all the possibilities.
To make linear logic as strong as classical logic two new connectives are added: $\llbang{\phi}$ and $\llwn{\phi}$ -- called respectively bang and why-not.
% Linear logic defined as of right now is decidable, since formulas cannot grow in size we can explore each possibility.
% To be as strong as classical logic, there are two so called exponentials: bang or $!\phi$ and why not or $?\phi$.
% These have the purpose of localizing the uses of weakening and contraction:
These are called exponentials and their purpose is to localize uses of contraction and weakening.
For example, formulas marked with $!$ can be used any number of times. %, so the intuistic implication $a \rightarrow b$ is translated as $!(a \lolli b)$, and transitions in a petri net are represented by $!(resources_1 \lolli resources_2)$.

% Linear logic can be used to ensure that objects are used exactly once, thus allowing the system to safely deallocate an object after its use.
% The Haskell's compiler GHC has an experimental extensions to permit signatures with linear types.

% \subsection{Linear logic in practice}
% utilizzi logica lineare
%   * pi-calcolo
%   * risorse
%   * linear-haskell?
% A good example of linear logic may be chemical reactions % https://www.cs.cmu.edu/~crary/317-f22/lectures/20-linear.pdf
% Here we can see a reaction as an implication, if we have the reagents we can consume them to obtain the products.

% Petri nets can be encoded in linear logic, for example, 
% https://johnwickerson.github.io/talks/linearlogic.pdf


\section{Why Prolog}
Prolog as a language and as an environment has been historically tied to automated theorem proving for its ability to express these kind of algorithmns naturally.
% In particular we chose SWI-Prolog because it offers a comprehensive and mature free Prolog environment.
One particularly conventient characteristic of Prolog is its automatic managing of backtracking, in most other languages we would have had to use exceptions to walk down the stack, or a queue of unfinished computations, which would have made the code much less readable.

Most Prolog implementations also support CLP or constraint logic programming.
This allows to have constraints referencing some attributes of variables in the body of clauses, in our case we use CLP($\mathcal{B}$) for boolean constraints and an handy interface to a sat-solver.
The library exposes operators to compose boolean formulas made of prolog variables.
We will exclusively use conjunction, negation and equality, respectively 
\begin{minted}{prolog}
X * Y.
~ X.
X =:= Y.
\end{minted}
and to check the satisfaiability of said formulas
\begin{minted}{prolog}
?- sat(X * Y).
X = Y, Y = 1.
\end{minted}

One other characteristic of Prolog which revealed to be very handy for our prover is unification.
Using this we didn't have to explicitly propagate the solutions of the SAT-solver, which instead were automatically propagated between branches.

\chapter{The focused calculus}
Before describing the calculus we must give some preliminary definitions

% Explaination on constraints why we use them ecc ecc ecc

\section{Normalization}
Since in linear logic negation is symmetric and involutive, it is usual to work only with formulae in negated normal form.
\begin{define}[Negated Normal Form -- NNF]
	A farmula is in NNF if all its linear implications ($\lolli$) are expanded to pars ($\llpar$) and negation is found only at atoms.
	By extension, a sequent is in NNF if all its formulae are in NNF.
\end{define}
A generic formula is then normalized applying recursively the DeMorgan rules for linear logic, untill NNF is reached.
The process of normalization takes a two-sided judgement, of the form
$$ \Delta \vdash \Gamma $$
and transforms it into a one-sided judgement
$$ \vdash \Delta' $$
where the right side is composed of the normalization of $\Gamma$ and $\llnot{\Delta}$.

This choice has some implementation-wise advantages, but for now we will only care about the fact that it shrinks the size of the complete calulus by roughly half since we only have to deal with the right rules of the connectives.
\begin{figure}[H]
	\centering
	\begin{tblr}{ colspec = {cccccccccr}
		    , cells = { mode = math } 
		    % , vborder{1-4} = { leftspace = 0pt, rightspace = 0pt } 
		    }
		\phi & ::=  & 1              &\mid& \phi \llten \phi  &\mid& \bot &\mid& \phi \llpar \phi  & \text{(Multiplicatives and their constants)} \\
		     & \mid & 0              &\mid& \phi \llplus \phi &\mid& \top &\mid& \phi \llwith \phi & \text{(Additives and their constants)} \\
		     & \mid & \llbang{\phi}  &\mid& \llwn{\phi}       &    &      &    &                   & \text{(Exponentials)} \\
		     & \mid & \llnot{\alpha} &\mid& \alpha	      &    &      &    &                   & \text{(Where $\alpha$ is a term)}
	\end{tblr}
	\caption{Normalized linear logic formulae}
	\label{fig:ll-connectives}
\end{figure}
As seen in Figure \ref{fig:ll-connectives} we will use $\phi$ for formulae and $\alpha$ for terms.

\section{Focusing}
Focusing is a technique described by Andreoli in his seminal paper \cite{Focusing}.
There he recognizes two alternating phases in a proof: a deterministic phase, where the order of rule application to the sequent does not matter; and a non deterministic phase, where several choices may be tried.
These two phases are respectively called asynchronous and synchronous phase.
\begin{define}
	Given a formula $\phi$ we define the following predicates
	``$\isAsy{\phi}$'' indicates that the rule for the toplevel connective of the formula $\phi$ is asynchronous on the right, these connectives are 
	$$ \llpar\!, \llwith\!, \llwn{}, \lltop, \llbot $$
	conversely we define ``$\isSync{\phi}$'' to indicate that the rule for the toplevel connective of $\phi$ is synchronous on the right, these are
	$$ \llten, \llplus, \llbang{}, \llone $$
\end{define}
Furthermore in focusing to everything is assigned a polarity of positive or negative.
Connectives are negative if are asynchronous on the right, or positive if are asynchronous on the left.
Terms also have a polarity, which may be assigned with some complex mechanisms.
We will follow \cite{LiangMiller} and simply assing atoms with a negative polarity and negated atoms with a positive one.
\begin{define}
	``$\isNegLit{\alpha}$'' is a predicate that is true only when $\alpha$ is a negative literal (i.e. an atom).
	Conversely ``$\isPosLit{\alpha}$'' is a predicate that is only when $\alpha$ is a positive literal (i.e. a negated atom).
\end{define}

\section{Constraints}
Our calculus uses constraints to manage the resources.
\begin{define}[Variables, expressions]
	\label{def:bool-expr}
	A boolean variable is simply a symbol to which one can associate a value of true or false.
	A boolean expression, in our case, is just a conjunction of possibly negated boolean variables.
\end{define}
\begin{define}[New variables]
	\label{def:new}
	Sometimes we will write 
	$$ \new{x}, \new{X} $$
	to mean respectively:
	\begin{itemize}
		\item the variable name $x$ has not yet occurred in any expression in the proof;
		\item each variable name $x_i \in X$ has not yet occurred in the proof and 
			$$ \forall i, j \mid i \neq j \Rightarrow x_i \neq x_j $$
			each variable in $X$ is distinct.
	\end{itemize}
\end{define}
As seen in Figure \ref{fig:var-name} we will call $e$ such a conjuction and $x$ the single boolean variables.
\begin{figure}[h!]
	\centering
	\begin{tblr}{ colspec = {cccccr}, cells = { mode = math } }
		x & ::=  & x_i &\mid& \overline{x_i} & \text{(Variable)}\\
		e & ::=  & x \wedge e    &\mid& x & \text{(Expression)} \\
	\end{tblr}
	\caption{Definition of a boolean variable and expression}
	\label{fig:var-name}
\end{figure}

\begin{define}[Annotated formula]
	\label{def:annotated}
	Given a formula $\phi$ defined as in Figure \ref{fig:ll-connectives} and a boolean expression $e$ defined as in Definition \ref{def:bool-expr}, an \textit{annotated formula} is simply a term 
	$$ \text{af}(\phi, e) $$
	that associates the formula to the expression.
	We denote 
	$$ \text{exp}(\af{\phi}{e}) = e $$
	the operation of extracting the boolean expression associated to a given formula, and then extend this notation to sequents such that $ \text{exp}(\Delta) $ is the set of all boolean expressions of $\Delta$.
\end{define}

We now define the constraints on these expressions:
\begin{define}[Constraints]
	\label{def:constraints}
	Given an annotated formula $\af{\phi}{e}$ as in Definition \ref{def:annotated}, a constraint may be of two kinds
	\begin{itemize}
		\item ``$\used{e}$'' states that the formula $\phi$ gets consumed in this branch of the proof, this corresponds to saying the constraint $e$ is true or
			$$ x_i \wedge \dots \wedge x_j = \top $$
		\item ``$\avail{e}$'' states that the formula $\phi$ does not get consumed in this branch of the proof, this corresponds to saying the constraint $e$ is not true or
			$$ x_i \wedge \dots \wedge x_j = \bot $$
	\end{itemize}
	We then extends these predicates to sequents
	\begin{align*}
		\used{\Delta} &= \{ \used{e_2} \mid e_2 \in \text{exp}(\Delta) \} \\
		\avail{\Delta} &= \{ \avail{e_2} \mid e_2 \in \text{exp}(\Delta) \}
	\end{align*}
\end{define}
The purpose of putting formulae and expressions together in the annotated formula is twofold:
\begin{itemize}
	\item the actions taken on the formula determine the constraints that will be generated, and these depend on the variables associated to said formula;
	\item after the constraints are solved we can query the assignement of the variables and find out if the associated formula is used or not in a certain branch of a proof.
\end{itemize}

\begin{define}[Satisfaiability of constraints]
	\label{def:sat}
	Given a set of constraints $\Lambda$ and a function V
	$$ \text{V} : \{ x_1, x_2, \dots, x_n \} \rightarrow \{ \top, \bot \} $$
	mapping variables to their value, such that
	\begin{align*}
		\Lambda &= \{ \dots, \used{e_i}, \avail{e_j}, \dots \} \\
		\text{V}&= \{ \dots, (x_i, \top), (x_j, \bot), \dots \}
	\end{align*}
	we write $\sat{\Lambda}{\text{V}}$ if 
	$$ \bigwedge_{e \in \Lambda} e[x_1 / \text{V}(x_1), x_2 / \text{V}(x_2), \dots] = \top $$
	Here we are using the translation given in Definition \ref{def:constraints} such that
	\begin{align*}
		\used{e} &\Leftrightarrow e = \top \\
		\avail{e} &\Leftrightarrow e = \bot
	\end{align*}
	It is worth noting that simply by rewriting the function V to
	$$ \text{V} = \{ \dots, x_i = \top, x_j = \bot, \dots \} $$
	we get back a set of constraints.
\end{define}

We now expand the definition of triadic sequent of \cite{Focusing}, by adding constraints
\begin{define}[Members of the sequent]
	Given any sequent this can be in either two forms:
	\begin{itemize}
		\item focused or in the synchronous phase, written:
			$$\focus{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\phi}$$
		\item in the asynchronous phase, written:
			$$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\Phi}$$
	\end{itemize}
	These two have more or less the same members, which are
	\begin{itemize}
		\item the set $\Psi$ of unrestricted formulae, or all formulae that can be freely discarded or duplicated;
		\item the multisets $\Delta$ and $\Phi$ of linear (annotated) formulae, these are respectively the formulas ``put to the side'' and the formulae which are being ``worked on'' during a certain moment of the asynchronous phase;
		\item the sets $\Lambda$ and V as defined in Definition \ref{def:sat}.
			This approach to constraints helps to make the flow of the variables and solutions through the proof tree more explicit and clear and leaves no ambiguity to where the constraints should be checked.
			The choice of letters is mainly a mnemonic or visual one, constraints $\Lambda$ ``go-up'' the proof tree and solutions V ``come down'' from the leaves.
	\end{itemize}
\end{define}
As explained in Definition \ref{def:sat}, V may be used as a set of constraint itself; this states that a certain solution must be respected in a new one.

\begin{define}[Splitting]
	\label{def:split}
	Given a sequent of annotated formulae $\Delta$ and a set of variables $X$ such that $|\Delta| = |X|$ we define the operation of splitting it as a function
	$$ \text{split}(\Delta, X) \mapsto (\Delta^L, \Delta^R) $$
	where
	\begin{align*}
		\Delta^L &= \{ \af{\phi_i}{x_i \wedge e_i} \mid i \in \{1, \dots, n\}\} \\
		\Delta^R &= \{ \af{\phi_i}{\varNot{x_i} \wedge e_i} \mid i \in \{1, \dots, n\}\}
	\end{align*}
	with $n$ the cardinality of $\Delta$, and $\phi_i$ (resp. $e_i$) the formula (resp. the expression) of the $i$-eth annotated formula in $\Delta$ using an arbitrary order.
	The same holds for $x_i$ and $X$.

	With a slight abuse of notation we will write $\text{split}(\Delta, X)_L$ and $\text{split}(\Delta, X)_R$ respectively as the left projection and the right projection of the pair $(\Delta_L, \Delta_R)$.
\end{define}
As a small example for clarity, given the sequent
\begin{align*}
	\Delta &= \af{a \llten b}{x_1}, \af{\llnot{c}}{x_2} \\
	X      &= \{ x_3, x_4 \} 
\end{align*}
this is split into
\begin{align*}
	\text{split}(\Delta, X)^L &\mapsto \af{a \llten b}{x_3 \varAnd x_1}, \af{\llnot{c}}{x_4 \varAnd x_2} \\
	\text{split}(\Delta, X)^R &\mapsto \af{a \llten b}{\varNot{x_3} \varAnd x_1}, \af{\llnot{c}}{\varNot{x_4} \varAnd x_2} 
\end{align*}
% example of splitting and variables

% One simple but important detail that will be useful later when explaining the Prolog implementation is noting that the variables in common between two branches with the same root are always introduced before the two branches diverge.
% Or -- put differently -- all new variables introduced in any point of a path from the root of the proof to a leaf may appear only in the subtrees.	% sistemo

We are now ready to present the full calculus.
\begin{figure}[H]
	\begin{subfigure}{\textwidth}
		\centering
			\begin{tblr}{ colspec = { cc }
				    , rows = {abovesep=5pt, belowsep=5pt}
				    }
			\SetCell[c=2]{c} {\scriptsize
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi_1}{e}, \af{\phi_2}{e}, \Phi}$}
			\LeftLabel{$[\llpar]$}
			\UnaryInfC{$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi_1 \llpar \phi_2}{e}, \Phi}$}
			\DisplayProof} \\
			{\scriptsize
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{\text{V}}}{\Psi}{\Delta}{\Phi}$}
			\LeftLabel{$[\llbot]$}
			\UnaryInfC{$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\llbot}{e}, \Phi}$}
			\DisplayProof}
			&
			{\scriptsize
			\AxiomC{}
			\LeftLabel{$[\lltop]$}
			\UnaryInfC{$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\lltop}{-}, \Phi}$}
			\DisplayProof
			}
			\\
			\SetCell[c=2]{c} {\scriptsize
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{\text{V}'}}{\Psi}{\Delta}{\af{\phi_2}{e}, \Phi}$}
			\AxiomC{$\async{\constr{\used{e}, \Lambda}{\text{V}''}}{\Psi}{\Delta}{\af{\phi_1}{e}, \Phi}$}
			\LeftLabel{$[\llwith]$}
			\BinaryInfC{$\async{\constr{\Lambda}{\text{V}', \text{V}''}}{\Psi}{\Delta}{\af{\phi_1 \llwith \phi_2}{e}, \Phi}$}	% capisco cosa fanno qui i constraint
			\DisplayProof}
			\\
			\SetCell[c=2]{c} {\scriptsize
			\AxiomC{$\async{\constr{\Lambda}{\text{V}}}{\Psi, \phi}{\Delta}{\Phi}$}
			\LeftLabel{$[\,?\,]$}
			\UnaryInfC{$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\llwn{\phi}}{-}, \Phi}$}
			\DisplayProof} 
			\\
			\SetCell[c=2]{c} {\scriptsize
			\AxiomC{$\neg\isAsy{\phi}$}
			\AxiomC{$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta, \af{\phi}{e}}{\Phi}$}
			\LeftLabel{$[R\!\Uparrow]$}
			\BinaryInfC{$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi}{e}, \Phi}$}
			\DisplayProof
			}
		\end{tblr}
		\caption{Asynchronous rules}
	\end{subfigure}
\end{figure}
\begin{figure}[H]
	\ContinuedFloat
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{ colspec = { cc } 
			    , rows = {abovesep=5pt, belowsep=5pt}
			    }
			\SetCell[c=2]{c} {\scriptsize
			\AxiomC{$\new{X}$}
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{\text{V}'}}{\Psi}{\text{split}(\Delta, X)^L}{\text{af}(\phi_1, e)}$}
			\AxiomC{$\focus{\constr{\text{V}'}{\text{V}''}}{\Psi}{\text{split}(\Delta, X)^R}{\text{af}(\phi_2, e)}$}
			\LeftLabel{$[\llten]$}
			\TrinaryInfC{$\focus{\constr{\Lambda}{\text{V}''}}{\Psi}{\Delta}{\text{af}(\phi_1 \llten \phi_2, e)}$}	% capisco il movimento dei constraint
			\DisplayProof}
			\\ 
			{\scriptsize
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi_1}{e}} $}
			\LeftLabel{$[\llplus_L]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}}$}
			\DisplayProof}
			&
			{\scriptsize
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi_2}{e}}$}
			\LeftLabel{$[\llplus_R]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi_1 \llplus \phi_2}{e}}$}
			\DisplayProof}
			\\
			{\scriptsize
			\AxiomC{$\sat{ \used{e_1}, \avail{\Delta}, \Lambda}{\text{V}}$}
			\LeftLabel{$[1]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\llone}{e_1}}$}
			\DisplayProof} 
			&
			{\scriptsize
			\AxiomC{$\focus{\constr{\used{e_1}, \avail{\Delta}, \Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi}{e_1}}$}
			\LeftLabel{$[\,!\,]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\llbang{\phi}}{e_1}}$}
			\DisplayProof
			}
			\\
			\SetCell[c=2]{c} {\scriptsize
			\AxiomC{$\isAsy{\phi} \vee \isNegLit{\phi}$}
			\AxiomC{$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\LeftLabel{$[R\!\Downarrow]$}
			\BinaryInfC{$\focus{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\DisplayProof
			}
		\end{tblr}
		\caption{Synchronous rules}
	\end{subfigure}
\end{figure}
\begin{figure}[H]
	\ContinuedFloat
	\begin{subfigure}{\textwidth}
		\centering
		\begin{tblr}{ colspec = { cc }
			    , rows = {abovesep=5pt, belowsep=5pt}
			    , vborder{1-2} = { leftspace = -5pt, rightspace = -5pt } 
			    }
			{\scriptsize
			\AxiomC{$ \sat{\used{e_1}, \used{e_2}, \avail{\Delta}, \Lambda}{\text{V}}$}
			\LeftLabel{$[I_1]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta, \af{\phi}{e_2}}{\af{\llnot{\phi}}{e_1}}$}
			\DisplayProof}
			&
			{\scriptsize
			\AxiomC{$\isPosLit{\phi}$}
			\AxiomC{$\focus{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi}{e}}$}
			\LeftLabel{$[D_1]$}
			\BinaryInfC{$\async{\constr{\Lambda}{\text{V}}}{\Psi}{\Delta, \af{\phi}{e}}{.}$}
			\DisplayProof}
			\\
			{\scriptsize
			\AxiomC{$ \sat{\used{e_1}, \avail{\Delta}, \Lambda}{\text{V}}$}
			\LeftLabel{$[I_2]$}
			\UnaryInfC{$\focus{\constr{\Lambda}{\text{V}}}{\Psi, \phi}{\Delta}{\af{\llnot{\phi}}{e_1}}$}
			\DisplayProof}
			&
			{\scriptsize
			\AxiomC{$\isPosLit{\phi}$}
			\AxiomC{$\new{e}$}
			\AxiomC{$\focus{\constr{\used{e}, \Lambda}{\text{V}}}{\Psi}{\Delta}{\af{\phi}{e}} $}
			\LeftLabel{$[D_2]$}
			\TrinaryInfC{$\async{\constr{\Lambda}{\text{V}}}{\Psi, \phi}{\Delta}{.}$}
			\DisplayProof}
		\end{tblr}
		\caption{Identity and decide rules}
	\end{subfigure}
	\caption{Focused constraint calculus for Linear Logic}
	\label{fig:calculus}
\end{figure}

\chapter{Implementation}
We now describe the main implementation details of the prover.
Our mechanism to deal with exponentials borrows a great deal from APLL's one, which we will talk about briefly in \ref{sec:apll}.
APLL uses the measure of why-not height throught all its implementation.
\begin{define}[Why-not height]
	\label{def:why-not-height}
	Why-not height is the maximum number of nested ``why-not''s in a formula, or
	$$ \text{wnh}(\phi) = 
	\begin{cases}	
		0 & \text{if }\phi \in \{\llbot, \lltop, \llone, \llzero\} \\
		\max(\text{wnh}(\phi_1), \text{wnh}(\phi_2)) & \text{if } \phi \in \{ \phi_1 \llten \phi_2, \phi_1 \llpar \phi_2, \phi_1 \llplus \phi_2, \phi_1 \llwith \phi_2 \} \\
		\text{wnh}(\phi_1) & \text{if }\phi \in \{ \llnot{\phi_1}, \llbang{\phi_1}\} \\
		1 + \text{wnh}(\phi_1) & \text{if }\phi \in \{ \llwn{\phi_1} \} 
	\end{cases}
	$$
\end{define}
This attribute is used both to deal with unconstrained formulae, and to decide which branch to prove first for any normal operator.

\section{Formula transformations}
Before beginning the proof a sequent passes through a number of transformations.
These transformations both preprocess the sequent to a more convenient form, and also add information about the subformulae.

As a first transformation the sequent gets normalized into a sequent in negated normal form (NNF).
NNF is the form where all negations are pushed down to atoms and all linear implications ($\lolli$) are expanded into pars ($\llpar$) using the following tautology
$$ a \lolli b \Leftrightarrow \llnot{a} \llpar b $$
Normalization is a common technique -- used in all the provers we compare with.
The process is composed of just two steps
\begin{enumerate}
	\item the left sequent is negated and appended to the right sequent, implemented by the predicate \texttt{negate\_premises/3};
	\item the predicate \texttt{nnf/2}, which encodes the DeMorgan rules, is mapped recursively over the new sequent
\end{enumerate}
This is possible since classic linear logic is symmetric and negation is involutive.

The purpose of this process is to cut away a great deal of possible rules applicable to the sequent, sacrificing some of the structure of the sequent.
In fact the number of rules we need to implement after normalization is more than halved, since we now need just the right rules, without the ones for negation and linear implication.

As a second transformation, to each formula we assign its why-not height, as defined in Definition \ref{def:why-not-height}.
Why-not height is used during proof search to decide which branch to do first and which exponential to decide first.
This is a technique borrowed from APLL and its obvious purpose is to try first the branches with less exponentials: in the case the first probably simpler branch fails, we do not have to try the other.
After this transformation formulae are attribute trees, with at each node the why not height of the subformula.

As a third and final transformation, each formula gets annotated.
This means we associate a variable to each formula in the sequent as in Definition \ref{def:annotated}.
Given a sequent $\Delta$ we obtain
$$ \hat{\Delta} = \{ \af{\phi}{x} \mid \new{x}, \phi \in \Delta \} $$
To be clear, a variable is only assigned to the ``top-level'' formula, and subformulae are left unchanged.

In the implementation the concept of variable is split in two: the name of the variable -- represented by a Prolog atom, and the value of the variable -- represented by a Prolog variable.
This is needed since, after checking the constraints, the SAT-solver unifies the variable to its value if it finds a satisfaiable solution, so the purpose of the atom is to associate the variable value to its name if the final proof.
The process of annotation is implemented by the predicate \texttt{annotate/3}
\begin{minted}{prolog}
%! annotate(+[Formulae], -[AFs], -[Constraints]) is det.
annotate(Fs, Afs, Cns) :-
  maplist([ F
          , af(F, X, Var)
	  , v(Var) =:= 1
	  ]>>(gensym(x, X)), Fs, Afs, Cns).
\end{minted}
which is a simple map over the sequent.
The constraints returned state that each formula must have its variable to one, that is to say each formulae must be used.
These are the constraints the proof search starts with.

\section{Constraint propagation}
Some care is to be given to explaining how the constraints propagate.
In fact, in constrast to Figure \ref{fig:calculus} the implementation does not have explicit propagation of the solution to the constraints.
This is because Prolog's unification implicitly propagates a solution from one branch to another.
% continuo?

\section{Helper predicates}\label{sec:helper}
We now define some helper predicates to work with the constraints.
What we defined as $\avail{\Delta}$ in Definition \ref{def:constraints} corresponds to the predicate \texttt{set\_to\_zero/2}.

The other helper predicate implements the split function defined in Definition \ref{def:split}.
\begin{minted}{prolog}
%! split_ctx(+[AFs], -[AFs], -[AFs], -[Cns], -[Cns]) is det.
split_ctx(Afs, Pos, Neg, PCns, NCns) :-
  maplist([ af(F, N, E)
          , af(F, VarPos, Y)
  	  , af(F, VarNeg, Z)
  	  , v(Y) =:= v(X) * v(E)
  	  , v(Z) =:= (~ v(X)) * v(E)
  	  ]>>(
  	    gensym(x, V),
  	    atomic_list_concat([N, V], '.', VarPos),
  	    atomic_list_concat([N, V], '.~', VarNeg)
  ), Afs, Pos, Neg, PCns, NCns).
\end{minted}
It is again a map over the list of formulae, that generates the new formulae and the constraints accordingly.
Three new Prolog variables are introduced: \texttt{X}, \texttt{Y} and \texttt{Z}.
\texttt{X} is the new variable, the annotated formulae refer to the variable \texttt{Y} and \texttt{Z}, and constraints are added so that
\begin{align*}
	y &= x \varAnd e \\
	y &= \varNot{x} \varAnd e \\
\end{align*}
Compare this to the original definition of Definition \ref{def:split}, one can see that the two are basically identical other than the fact that here the name of the variable (the atom) and its value are treated separately.

\section{Focusing}
When explaining the code we will use some common names for variables, these are
\begin{itemize}
	\item \texttt{A} is a set of unrestricted atoms;
	\item \texttt{U} is a set of unrestriched formulae;
	\item \texttt{F}, \texttt{F1}, ..., are formulae, and \texttt{Fs} and \texttt{D} are a lists of them;
	\item \texttt{S} is the queue of currently usable unrestricted formulae;
	\item \texttt{In} is a list of constraints.
\end{itemize}
\subsection{Asynchronous and focusing phase}
During the asynchronous phase we have a list of formulae which are being worked on and a list of formulae which are put to the side.
With the former being called \texttt{Fss} and the latter \texttt{D}.
At each step we analyze the first element of the list \texttt{Fs}, and we keep scomposing the memebers of the list untill we cannot anymore.
This process can be seen for example in the predicate for $\llwith$
\begin{minted}{prolog}
async(A, U, D, [F|Fs], S, M, In, _) :-
  F = af(((F1-H1) & (F2-H2))-_), N, E), !,
  ( H2 > H1	
  -> async(A, U, D, [af((F1-H1), N, E)|Fs], S, M, [v(E) =:= 1|In], _), 
     async(A, U, D, [af((F2-H2), N, E)|Fs], S, M, [v(E) =:= 1|In], _) 
  ;  async(A, U, D, [af((F2-H2), N, E)|Fs], S, M, [v(E) =:= 1|In], _),
     async(A, U, D, [af((F1-H1), N, E)|Fs], S, M, [v(E) =:= 1|In], _)
  ).
\end{minted}
Here we can see both the choice being made based on the why-not height of the two subformulae, and how the with is scomposed.
Compare this with the $\llwith$ rule in Figure \ref{fig:calculus}.
The cut at line 2 represents the main concept of the asynchronous phase: if an asynchronous connective is encountered the only thing we ought to do is to scompose it.

If a formula cannot be further be broken apart -- i.e. it is either an atom, a negated atom, or it has a toplevel synchronous connective -- then it is put to the side in \texttt{D}.
This can be seen in the rule \texttt{to\_delta} which implements the rule $R\!\Uparrow$
\begin{minted}{prolog}
async(A, U, D, [F|Fs], S, M, In, _) :-
  F = af((F1-_), _, _),
  not(is_asy(F1)), !,
  async(A, U, [F|D], Fs, S, M, In, _).
\end{minted}
This process goes as long as \texttt{Fs} has still formulae inside.

When \texttt{Fs} is empty the phase switches, and the focusing process begins: we choose a formula -- called \texttt{decide} -- from either \texttt{D} or \texttt{U} and we scompose it untill either an asynchronous connective is left or a negated atom.
This is represented by the rules $D_1$ and $D_2$ that will be discussed further ahead in Section \ref{sec:decide}.

\subsection{Identity rules}
This process of alternating asynchronous and synchronous phases in classic focusing goes on untill we have a only positive literal (in our case a negated atom) in \texttt{Fs} and the corresponding negative literal (in our case just an atom) in either \texttt{U} or \texttt{D}.
When this happens the axioms -- rules $I_1$ or $I_2$ -- are applied to close the branch.
In our case when we are focusing and we have a positive literal in \texttt{Fs}, we check if the corresponding negative literal exists in \texttt{D}.
If this is true, then the variables of all the other formulae in \texttt{D} are set to zero using the predicate \texttt{set\_to\_zero/2} defined in Section \ref{sec:helper}, and the constraints are checked.
This is encoded in the clause 
\begin{minted}{prolog}
focus(A, U, D, F, _, _, In, node(id_1, A, U, D, F, [])) :-
  F = af(((~ T)-_), _, E1),
  is_term(T),
  select(af((T-_), _, E2), D, D1),
  set_to_zero(D1, Dz),
  append([v(E1) =:= 1, v(E2) =:= 1|Dz], In, Cns),
  check(Cns).
\end{minted}
A slightly different process happens if instead a correspondence is found in \texttt{A} instead of \texttt{D}.
Here \texttt{A} is a special set containing just unrestricted atoms.
This is a small modification to APLL's approach based on the fact that once negative literals are put in a sequent they can never leave it, and is due to the fact that since \texttt{U} may be sorted many times, we try to keep the number of formulae in it small.

\subsection{Decide rules}\label{sec:decide}
For the decide rules, particularly for $D_2$, we use a modified version of APLL's algorithm defined in Section \ref{sec:apll}.
The method consists of not using directly the set $\Psi$ in the $D_2$ rule, but instead we keep a queue of ordered unrestricted formulae which can be refilled only a certain number of times per-branch.
This can be seen in the definition of the rule \texttt{decide\_2}
\begin{minted}{prolog}
async(A, U, D, [], [H|T], M, In) :-
  \+ U = [],
  gensym(x, X),
  focus(A, U, D, af(H, X, E), T, M, [v(E) =:= 1|In]).
async(A, U, D, [], [_|T], M, In) :-
  \+ U = [],
  async(A, U, D, [], T, M, In).
async(A, U, D, [], [], M, In) :-
  \+ U = [],
  refill(U, M, S, M1),
  early_stop(A, U, D, S, M1, In).
\end{minted}
Here the fifth argument is the queue and the sixth is the bound.
Two cases arise:
\begin{itemize}
	\item if \texttt{S = []} and \texttt{M > 0} then the sequent of unrestricted formulae is taken and it is sorted based on why-not height.	% cambio persona
		This can be seen in the predicate \texttt{refill/4}
		\begin{minted}{prolog}
refill(U, M, S, M1) :-
	\+ M = 0,
	\+ U = [], 
	sort(2, @=<, U, S), 
	M1 is M - 1.
		\end{minted}
		Here line 4 is to be interpreted as a sort in increasing order on the second attribute (the why-not height) keeping duplicates.
		This new list of unrestricted formule becomes the new \texttt{S} and \texttt{M} is decreased.
		Otherwise if if \texttt{M} is 0 (line 2) the branch fails.
	\item if \texttt{S} is not empty, then the first formula in the queue is extracted and added to the working set.	% sistemo
		If the branch fails the formula gets discarded and the next one in the queue is tried.	% sistemo
\end{itemize}
In particular, if the queue \texttt{S} is refilled, we do not directly call \texttt{async/8}, but instead call another predicate: \texttt{early\_stop/7} (line 11).
\begin{minted}{prolog}
early_stop(_, _, _, [], _, _) :-
  false.
early_stop(A, U, D, [H|T], M, In) :-
  gensym(x, X),
  focus(A, U, D, af(H, X, E), T, M, [v(E) =:= 1|In]).
early_stop(A, U, D, [_|T], M, In) :-
  early_stop(A, U, D, T, M, In).
\end{minted}
This is due the simple fact that if the branch was not provable and we instead called directly \texttt{async/8} at line 11, we would try to refill the branch \texttt{M} times.
What \texttt{early\_stop/7} does is fail if the queue has just been refilled and it turns out the branch was not provable.

All the rules $D_1$, $I_1$ are defined before the unrestricted counterparts, so that they are tried first.


\section{Building the tree}
In the listings above we omitted one parameter of the predicates, which purpose is to build the proof tree.
At each call of async and focus one node of the proof tree is built.
This contains the context of the call.
For example in
\begin{minted}{prolog}
async(A, U, D, [F|Fs], S, M, In, node(par, A, U, D, [F|Fs], [Tree])) :- 
  F = af(((F1 / F2)-_), N, E), !,
  Fs1 = [af(F1, N, E), af(F2, N, E)|Fs],
  async(A, U, D, Fs1, S, M, [v(E) =:= 1|In], Tree).
\end{minted}
we can see clearly the structure of the node: a label, the context and an -- optionally empty -- list of subtrees.
A leaf is just a node with an empty list of subtrees.

This tree can be used in the end to reconstruct the actual proof tree, by visiting it and -- for each formula -- querying whether its variable is set to one, and deleting it otherwise.
A more sophisticated algorithm may even cancel out unwanted unrestricted formulae, that otherwise remain lingering in the sequent.

\chapter{State of the art}
Most forward provers for classic linear logic use some combination of focusing and normalization to structure their proofs, with the notable exception of llprover not using normalization.	% credo
We confront our prover with two other provers: llprover (1997, %cit
) and APLL (circa 2019, %cit
).

Usually the splitting is handled in two ways: trying every partition possible, or using something called the method of input/output %cit
. The latter tries to do one branch of the proof of a multiplicative, and then feeds the remaining formulae in the sequent of the other branch.

We now give a deeper look at the provers we confront with.

\section{APLL}\label{sec:apll}
APLL is the underlying prover of click\&collect. % cit
It provides 4 different searches -- forward and backwards for classic and intuitionistic linear logic. 
We will focus on the backwards algorithm for classic linear logic.


The program is written in OCaML and implements a standard focused proof search on normalized formulae as seen in \cite{LiangMiller}.
In this section we will illustrate two noteworthy charateristics of its implementation:
\begin{itemize}
	\item Sequent splitting when encountering a tensor is done by generating all the numbers up to $2^{|\Delta|}$ -- where $\Delta$ is the sequent -- and using the bit representation of those to create the two subsets.
		This can be seen in the function \texttt{split\_list}, which in turn calls \texttt{split\_list\_aux}
		\begin{minted}{ocaml}
let rec split_list_aux (acc1, acc2) l k = match l with
  | [] -> acc1, acc2
  | hd :: tl -> 
      if k mod 2 = 0 
      then split_list_aux (acc1, hd :: acc2) tl (k / 2)
      else split_list_aux (hd :: acc1, acc2) tl (k / 2)
		\end{minted}
		where the argument \texttt{k} is the number that determines the decomposition of the sequent.
		This function is called recursively when a tensor is encountered during proof search, starting at $ k = 2^{|\Delta|}$ and decreasing by one at each iteration
		\begin{minted}{ocaml}
(* ... *)
| Tensor (g, h) ->
  let rec split_gamma k = 
    if k = -1 then None
    else
      let gamma1, gamma2 = split_list gamma k in
        try
	  (* ... *)
        with NoValue ->
          split_gamma (k - 1) 
  in
    let k = fast_exp_2 (List.length gamma) - 1 in
      (* ... *)
		\end{minted}

		As we will see in \ref{sec:benchmarking} this implementation choice will result in a degradation of performance on formulae with a high number of multiplicatives.
\end{itemize}

\section{\texttt{llprover}}
\texttt{llprover} is a prover by Naoyuki Tamura.
Where APLL had different provers for implicative and classical linear logic, this prover encodes all the rules as the same predicate \texttt{rule/6}, using the first argument as a selector for the system.
Using classical logic as the system uses all the rules, included the ones for implicative linear logic.
For this reason the prover does not implement normalization.

Another particular characteristic of llprover is that it uses a local bound with iterative deepening, so in the benchmarks for formulae which need a lot of contractions, it will perform slightly worse.

\chapter{Testing}
!!! VERY MUCH STILL WIP !!!
\section{Infrastructure}
The infrastructure for the project is divided in two parts:
\begin{itemize}
	\item the nix ``glue''
	\item python utilities to call and confront provers
\end{itemize}
In nix everything must be packaged as a derivation, so derivations are written for all the programs we interact with:
\begin{itemize}
	\item the provers, such as APLL or llprover
	\item the formula generator adapted from APLL described in Section \ref{sec:formula_generator}
	\item the TPTP parser
\end{itemize}
All these derivations, together with a derivation for the python environment, are then used to define a single development environment to run the jupyter notebook with all necessary dependencies.
Furthermore having used ``flakes'' all dependencies are locked to a certain commit, thus ensuring better reproducibility.

The python module \texttt{testprover} on the other hand defines some helper utilities run.
Here for simplicity, and to avoid parsing of standard output, we assume that a prover returns a code of 0 if it found a solution to a sequent, or any other number otherwise.

Defining the call to our prover using this library is as simple as writing
\begin{minted}{python}
pc = Registered()

@pc.register('sat-ll')
@call_prover(PrefixTree.SAT_LL_DICT)
def call_sat_prover(premises, conclusions):
        return [ 'sat-ll'
               , '-b', '3'
               , f'{premises} |- {conclusions}'
               ]
\end{minted}
The innermost decorator (line 3) accepts a dictionary, and is needed to auomatically call the prover with a test entry (as in Section \ref{sec:file_format}), time it and eventually terminate the process it if the prover timeouts.
The outermost decorator (line 4) instead simply add an entry \texttt{sat-ll} associated to the prover call in the register \texttt{pc}.
This register is then passed to other functions, which will automatically call all the provers in it.
Since often the prover take a considerable amount of time to finish, calls are usually done using futures.

The library then provides two functions
\begin{itemize}
	\item the function \texttt{testall} accepts a single prover and a test suite and checks if the output of the prover corresponds to the expected value for the test;
	\item the function \texttt{benchmark} accepts a register and a test suite and returns the times and outcomes of each prover.
\end{itemize}
All the outputs of the functions above pandas are \texttt{DataFrame}s, which means that they can be easily queried, dumped to csv, aggregated, or visualized using most plotting libraries.

\subsection{Reproducibility}
The prover's tests and benchmarks are organized in a jupyter notebook.
To ensure reproducibility we use Nix which is a build system based on declarative recipes, called \textit{derivations}.
A docker image is also given, which in turn calls nix without the need to install it on the system.
% Furthermore for the project infrastructure we used nix flakes, which are an experimental feature of nix that makes the process more hermetic and pure.

\subsection{Prefix format}
Since for benchmarking we will interface with a lot of different provers, each with its own syntax for expressions, the need for a common format which was easy to parse and translate arose.
For this purpose we define a prefix format for linear logic formulae inspired by the format used by \cite{TarauPaiva} for implicational formulae
\begin{table}[H]
	\centering
	\begin{tblr}{ colspec = {cc} }
		\hline
			formula & symbol \\
		\hline
		\hline
			$\phi_A \llten \phi_B$  & \texttt{*AB} \\
			$\phi_A \llpar \phi_B$  & \texttt{|AB} \\
			$\phi_A \llplus \phi_B$ & \texttt{+AB} \\
			$\phi_A \llwith \phi_B$ & \texttt{\&AB} \\
			$\phi_A \lolli \phi_B$  & \texttt{@AB} \\
			$\llnot{\phi_A}$        & \texttt{\^{}A} \\
			$\llwn{\phi_A}$         & \texttt{?A} \\
			$\llbang{\phi_A}$       & \texttt{!A} \\
	\end{tblr}
\end{table}
Furthermore each single character not representing an operator is considered as a variable name.
Longer names can be specified by enclosing them in single apices as in \texttt{'varname'}.
As an example we give the translation of DeMorgan for the tensor:
$$ \text{trans}(\llnot{(a \llten b)} \lolli \llnot{a} \llpar \llnot{b}) = \text{\texttt{@\^{}ab|\^{}a\^{}b}} $$

% Since most one of the datasets we use is LLTP % cite
% , which is written in TPTP's format % cite
% , we define a parser for this using Haskell's parser generator happy. % cite

\subsection{File formats}\label{sec:file_format}
We use json as a standard format to store the tests because of its vast adoption by most programming languages.
A test suite is thus defined as a list of test cases
\begin{verbatim}
TestCase ::= {
  "id": <Number>,
  "premises": [ <PrefixFormula>* ],
  "conclusions": [ <PrefixFormula>* ]
}
\end{verbatim}
where
\begin{description}
	\item[id] \hfill \\ is a number with the sole purpose of tracing back the test case from the output;
	\item[premises] \hfill \\ is a list of premises as prefix formulae;
	\item[conclusions] \hfill \\ a list of conclusions as prefix formulae.
\end{description}
Other arbitrary fields may be present, for example we will use the following optional fields:
\begin{description}
	\item[thm] \hfill \\ whether this test case is a tautology or not, may be null;
	\item[*, \&, +, ...] \hfill \\ the number of times a specific connective appears in the test case;
	\item[notes] \hfill \\ human readable text about the test case, for example its infix representation;
	\item[size] \hfill \\ an indicative number of the size of the formula;
	\item[atoms] \hfill \\ the upper bound on the number of atoms.
\end{description}
% As an example the test case for the DeMorgan rule of $\llten$
% \begin{minted}{json}
% {
% 	"id": 1,
% 	"thm": true,
% 	"premises": [ "^*ab" ],
% 	"conclusions": [ "|^a^b" ],
% 	"notes": "tensor DeMorgan"
% }
% \end{minted}

\subsection{Formula generation}\label{sec:formula_generator}
One of the sources of formulae we'll use in Section \ref{sec:benchmarking} is APLL's random formula generator.
The version we'll use is a slight modification of it where:
\begin{itemize}
	\item the output is in the json format described in Section \ref{sec:file_format}
	\item one can choose to generate normalized formulae or not;
	\item one can choose which connectives appear in the generated formula.
\end{itemize}
A noteworthy detail is how the parameters \texttt{size} and \texttt{atoms} mentioned in Section \ref{sec:file_format} are defined, since these are directly related to how the formulae are generated:
\begin{itemize}
	\item when one specifies a number of atoms \texttt{atoms}, the generator initializes an array containing \texttt{atoms} atoms, their negations, and the constants $\bot, \top, \dots$.
		During the generation of the formula this array is randomly accessed, choosing an element when needed.
		This means that \texttt{atoms} represents an upper bound to the number of different atoms that may appear in the formula, not their exact number.
	\item when a formula is generated, at each step it is chosen whether to generate a unary or binary connective based on a threshold:
		\begin{itemize}
			\item if a unary connective is chosen, the process continues with a size of $\text{\texttt{size}} - 1$;
			\item if a binary connective is chosen, the program chooses a random value between 0 and \texttt{size}, and it generates the two branches of the formula, with size respectively $k$ and $\text{\texttt{size}} - k$.
		\end{itemize}
\end{itemize}

\section{Benchmarking}\label{sec:benchmarking}
% Finding a comprehensive test suite for linear logic, moreover one which is not made up mostly of translations of other theorems to linear logic, is not easy.
% Most of the theorems we use for testing are llprover's %cite
We'll mainly use three sources for formulae:
\begin{itemize}
	\item llprover's tests
	\item LLTP, especially the translations of Kleene's intuitionistic formulae 
	\item randomly generated formulae made by the genrator described in \ref{sec:formula_generator}
\end{itemize}
llprovers tests are composed mainly by simple linear logic tautologies, e.g. the DeMorgan rules, for this reason these tests are used more as a simple and fast suite to catch the most obvious bugs between iterations of the prover.

We now show the results of running the provers on two datasets: KLE-cbn and KLE-cbv, respectively the call-by-name and call-by-value translations of Kleene's theorems.
These translations introduce a high number of exponentials, and this causes -- other than timeouts because of the added complexity -- some failures.
These failures are not due to bugs, but instead happen because the prover has reached its bound for that formula.
The benchmarks are done using a timeout of \qty{60}{\second} and a bound of 3.
\begin{table}[H]
	\centering
	\begin{tblr}{ 
		colspec = { l c c c c c c },
		vlines = {2,3,4,5,7,8,9,10}{solid},
		cells = { preto = {\small} },
		% cells{2,7} = { preto = {\small}, cmd = {\textbf} }
		}
		\SetCell[c=7]{c} Kleene CBN \\
		\hline
		prover & timeouts & failures & successes & success rate & avg. time (succ.) & avg. time (tot.) \\
		\hline
		\hline
		APLL & 0 & 16 & 72 & $\approx 0.80$ & \qty{0.037}{\second} & \qty{0.055}{\second} \\
		llprover & 20 & 6 & 62 & $\approx 0.70$ & \qty{1.709}{\second} & \qty{3.253}{\second} \\
		sat-ll & 4 & 18 & 66 & $\approx 0.75$ & \qty{0.130}{\second} & \qty{0.185}{\second} \\
		\hline
		\SetCell[c=7]{c} Kleene CBV \\
		\hline
		prover & timeouts & failures & successes & success rate & avg. time (succ.) & avg. time (tot.) \\
		\hline
		\hline
		APLL & 0 & 17 & 71 & $\approx 0.80$ & \qty{0.035}{\second} & \qty{0.326}{\second} \\
		llprover & 20 & 6 & 62 & $\approx 0.70$ & \qty{0.981}{\second} & \qty{2.179}{\second} \\
		sat-ll & 5 & 15 & 68 & $\approx 0.77$ & \qty{0.443}{\second} & \qty{0.496}{\second} \\
		\hline
	\end{tblr}
\end{table}

There are mainly two downsides with using random formulae:
\begin{itemize}
	\item most cases do not hold any structure, 
	\item ??? % non ricordo più cosa volessi scrivere
\end{itemize}
Using random formulae we can clearly see that our prover outperforms APLL (and llprover) when dealing with formulae rich in multiplicatives.
\begin{figure}[H]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[scale=0.5]{./images/mll.pdf}
		\caption{Multiplicative case}
		\label{fig:mll_bars}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[scale=0.5]{./images/mall.pdf}
		\caption{Multiplicative and additive case}
		\label{fig:mall_bars}
	\end{subfigure}
	\caption{Percentage of number of timeouts out of a hundred formulae}
\end{figure}
We can see that in the multiplicative and additive case the difference begin to level.
The additive case is not that significant as the formulae remain manageable and no major differences can be seen.

As soon as exponentials come into play the differences level out.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{./images/cll-500-250.pdf}
	\caption{Full linear logic}
	\label{fig:cll_bars}
\end{figure}
It can be seen in \ref{fig:cll_bars} that in full linear logic our prover performs slightly worse than APLL.

All these test were done generating suits of 100 tests.
Figure \ref{fig:mll_bars}'s tests were composed of normalized formulae of \texttt{size} 100, and \texttt{atoms} 50, with just the connectives $\llten$ and $\llpar$.
Figure \ref{fig:mall_bars}'s tests were composed of normalized formulae formulae of \texttt{size} 500, and \texttt{atoms} 250, with the connectives $\llten, \llpar, \llwith$ and $\llplus$.
Finally \ref{fig:cll_bars}'s tests were composed of normalized formulae of \texttt{size} 500, and \texttt{atoms} 250, with all the connectives.

When looking at the results of full linear logic, it is important to note that unlike the tests with multiplicatives and additives, some of the results may be early failures because of the bound.
Since llprover uses incremental search, its times are often the slowest.
Similarly our prover is consistently slightly slower that APLL, this difference is negligible and due to the fact that APLL is compiled, whereas our prover is interpreted.

% \chapter{Conclusions}
% We showed that -- given appropriate modifications -- Harland\&Pym's method is competitive to other provers, and in some cases (see multiplicatives) may be even faster.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
